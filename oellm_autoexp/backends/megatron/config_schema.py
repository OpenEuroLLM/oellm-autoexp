"""Megatron-LM configuration schema (auto-generated)."""

from dataclasses import dataclass, field
from typing import Any, Literal
from compoconf import ConfigInterface


@dataclass
class MegatronConfig(ConfigInterface):
    """Typed projection of Megatron-LM CLI arguments."""

    # Generated by scripts/generate_megatron_dataclass.py
    # Number of transformer layers.
    num_layers: int | None = None

    # Number of encoder transformer layers.
    encoder_num_layers: int | None = None

    # Number of decoder transformer layers.
    decoder_num_layers: int | None = None

    # Tansformer hidden size.
    hidden_size: int | None = None

    # Transformer Feed-Forward Network hidden size. This is set to 4*hidden-size if not
    # provided
    ffn_hidden_size: int | None = None

    # Number of transformer attention heads.
    num_attention_heads: int | None = None

    # Attention backend to use (flash,fused,unfused,local,auto). Defaults to auto
    attention_backend: str = "auto"

    # Projection weights dimension in multi-head attention. This is set to    args.hidden_size
    # // args.num_attention_heads if not provided.
    kv_channels: int | None = None

    # Use group-query attention.
    group_query_attention: bool = False

    num_query_groups: int = 1

    # Maximum number of position embeddings to use. This is the size of position embedding.
    max_position_embeddings: int | None = None

    # Position embedding type.
    position_embedding_type: Literal["learned_absolute", "rope", "mrope", "relative", "none"] = (
        "learned_absolute"
    )

    # Number of buckets for relative position embeddings.
    relative_attention_num_buckets: int = 32

    # Maximum distance for relative position embeddings calculation.
    relative_attention_max_distance: int = 128

    # Use rotary positional embeddings or not. Deprecated: use --position-embedding-type
    use_rotary_position_embeddings: bool = False

    # Base to use for rotary positional embeddings, default 10000
    rotary_base: int = 10000

    # Percent of rotary dimension to use, default 100%%
    rotary_percent: float = 1.0

    # Use interleaved rotary embedding.
    rotary_interleaved: bool = False

    # Sequence length interpolation factor for rotary embeddings.
    rotary_seq_len_interpolation_factor: int | None = None

    # Apply rope scaling as used in llama3.x
    use_rope_scaling: bool = False

    # Rope scaling factor in llama3.x models
    rope_scaling_factor: float = 8.0

    # Controls which layers to skip performing Rotary Position Embedding. Accepts either: - An
    # integer N: Represents a 1:N ratio, meaning RoPE is skipped every N-1 layers. - A string
    # containing a Python list expression that defines a custom pattern, e.g.:
    # "([0]*3+[1]*1)*3" evaluates to [0,0,0,1,0,0,0,1,0,0,0,1] where 1 indicates no-rope
    # layer. This patten is equivalent to --no-rope-freq=4.By default this is disabled and set
    # to None, indicating RoPE will be performedon every layer.
    no_rope_freq: Any | None = None

    # Disable position embedding. Deprecated: use --position-embedding-type
    add_position_embedding: bool = True

    # Multimodal rope section is for channel dimension, empty by default.
    mrope_section: list[int] | None = None

    # Pad the vocab size to be divisible by this value.This is added for computational
    # efficieny reasons.
    make_vocab_size_divisible_by: int = 128

    # Which normalization technique to use.
    normalization: Literal["LayerNorm", "RMSNorm"] = "LayerNorm"

    # Epsilon for layer norm and RMS norm.
    norm_epsilon: float = 1e-05

    # Adjust LayerNorm weights such that they are centered around zero. This improves
    # numerical stability.
    apply_layernorm_1p: bool = False

    # If set, use original BERT residula connection ordering.
    apply_residual_connection_post_layernorm: bool = False

    # Use OpenAIs GeLU implementation. This optionshould not be used unless for backward
    # compatibilityreasons.
    openai_gelu: bool = False

    # Use squared relu activation instead of default gelu
    squared_relu: bool = False

    # Use gated linear units and SiLU activation instead of default gelu
    swiglu: bool = False

    # Use workarounds for known problems with Torch ONNX exporter
    onnx_safe: bool | None = None

    # Disable BERT binary head.
    bert_binary_head: bool = True

    # Untie embeddings and output weights.
    untie_embeddings_and_output_weights: bool = False

    # Use multi-latent attention for model.
    multi_latent_attention: bool = False

    # Number of Multi-Token Prediction (MTP) Layers.MTP extends the prediction scope to
    # multiple future tokens at each position.This MTP implementation sequentially predict
    # additional tokens by using D sequential modules to predict D additional tokens.
    mtp_num_layers: int | None = None

    # Scaling factor of Multi-Token Prediction (MTP) loss. We compute the average of the MTP
    # losses across all depths, and multiply it the scaling factor to obtain the overall MTP
    # loss, which serves as an additional training objective.
    mtp_loss_scaling_factor: float = 0.1

    # Post attention dropout probability.
    attention_dropout: float = 0.1

    # Dropout probability for hidden state transformer.
    hidden_dropout: float = 0.1

    # Weight decay coefficient for L2 regularization.
    weight_decay: float = 0.01

    # Initial weight decay coefficient for L2 regularization.
    start_weight_decay: float | None = None

    # End of run weight decay coefficient for L2 regularization.
    end_weight_decay: float | None = None

    # Weight decay increment function.
    weight_decay_incr_style: Literal["constant", "linear", "cosine"] = "constant"

    # Gradient clipping based on global L2 norm.
    clip_grad: float = 1.0

    # First coefficient for computing running averages of gradient and its square
    adam_beta1: float = 0.9

    # Second coefficient for computing running averages of gradient and its square
    adam_beta2: float = 0.999

    # Term added to the denominator to improvenumerical stability
    adam_eps: float = 1e-08

    # Momentum factor for sgd
    sgd_momentum: float = 0.9

    # Batch size per model instance (local batch size). Global batch size is local batch size
    # times data parallel size times number of micro batches.
    micro_batch_size: int | None = None

    # Old batch size parameter, do not use. Use --micro-batch-size instead
    batch_size: int | None = None

    # Training batch size. If set, it should be a multiple of micro-batch-size times data-
    # parallel-size. If this value is None, then use micro-batch-size * data-parallel-size as
    # the global batch size. This choice will result in 1 for number of micro-batches.
    global_batch_size: int | None = None

    # Batch size ramp up with the following values:  --rampup-batch-size <start batch size>
    # <batch size incerement>                       <ramp-up samples> For example:   --rampup-
    # batch-size 16 8 300000 \    --global-batch-size 1024will start with global batch size 16
    # and over  (1024 - 16) / 8 = 126 intervals will increasethe batch size linearly to 1024.
    # In each intervalwe will use approximately 300000 / 126 = 2380 samples.
    rampup_batch_size: list[str] | None = None

    # If set, decrease batch size if microbatch_size * dp_sizedoes not divide batch_size.
    # Useful for KSO (Keep Soldiering On)to continue making progress if number of healthy GPUs
    # (andcorresponding dp_size) does not support current batch_size.Old batch_size will be
    # restored if training is re-started withdp_size that divides batch_size //
    # microbatch_size.
    decrease_batch_size_if_needed: bool = False

    # recompute activation to allow for training with larger models, sequences, and batch
    # sizes.
    recompute_activations: bool = False

    # Checkpoint activations to allow for training with larger models, sequences, and batch
    # sizes. It is supported at two granularities 1) full: whole transformer layer is
    # recomputed, 2) selective: submodules set in --recompute-modules are recomputed, default
    # is core_attn.
    recompute_granularity: Literal["full", "selective"] | None = None

    # Check for NaNs in loss and grad
    check_for_nan_in_loss_and_grad: bool = True

    # Check for spiky loss
    check_for_spiky_loss: bool = False

    # Check for unexpectedly large grads
    check_for_large_grads: bool = False

    # If set, distribute recomputed activations across model parallel group.
    distribute_saved_activations: bool = False

    # 1) uniform: uniformly divide the total number of Transformer layers and recompute the
    # input activation of each divided chunk at specified granularity, 2) recompute the input
    # activations of only a set number of individual Transformer layers per pipeline stage and
    # do the rest without any recomputing at specified granularitydefault) do not apply
    # activations recompute to any layers
    recompute_method: Literal["uniform", "block"] | None = None

    # 1) uniform: the number of Transformer layers in each uniformly divided recompute unit,
    # 2) block: the number of individual Transformer layers to recompute within each pipeline
    # stage.
    recompute_num_layers: int | None = None

    # The submodules to recompute. choices: "core_attn", "moe_act", "layernorm",
    # "mla_up_proj",          "mlp", "moe", "shared_experts". default:
    # ["core_attn"]."core_attn": recompute the core attention part of the transformer layer.
    # "moe_act": recompute the MoE MLP activation function. "layernorm": recompute the
    # input_layernorm and pre_mlp_layernorm. "mla_up_proj": recompute the MLA up projection
    # and RoPE applying parts."mlp": recompute the dense MLP layer."moe": recompute the MoE
    # layer."shared_experts": recompute the shared experts in the MoE layer."moe_act",
    # "layernorm", and "mla_up_proj" use output-discarding checkpointing, "core_attn", "mlp",
    # "moe", and "shared_experts" use normal checkpointing.
    recompute_modules: list[str] | None = None

    # If not set, clone the output of the scatter in embedding layer to GC original tensor.
    clone_scatter_output_in_embedding: bool = True

    # Enable nsys profiling. When using this option, nsys options should be specified in
    # commandline. An example nsys commandline is `nsys profile -s none -t nvtx,cuda -o
    # <path/to/output_file> --force-overwrite true --capture-range=cudaProfilerApi --capture-
    # range-end=stop`.
    profile: bool = False

    # Global step to start profiling.
    profile_step_start: int = 10

    # Global step to stop profiling.
    profile_step_end: int = 12

    # List of iterations to skip, empty by default.
    iterations_to_skip: list[int] = field(default_factory=lambda: [])

    # Optional name of file tracking `result_rejected` events.
    result_rejected_tracker_filename: str | None = None

    # Disables creation and usage of Gloo process groups.
    enable_gloo_process_groups: bool = True

    # Use the built-in pytorch profiler. Useful if you wish to view profiles in tensorboard.
    use_pytorch_profiler: bool = False

    # Global ranks to profile.
    profile_ranks: list[int] = field(default_factory=lambda: [0])

    # Record memory history in last rank.
    record_memory_history: bool = False

    # Specifies where to dump the memory history pickle.
    memory_snapshot_path: str = "snapshot.pickle"

    # Enables the  overlap of Tensor parallel communication and GEMM kernels.
    tp_comm_overlap: bool = False

    # Config file when tp_comm_overlap is enabled.
    tp_comm_overlap_cfg: str | None = None

    # Disables the All-Gather overlap with GEMM by pipelining the GEMM and All-Gather.
    tp_comm_overlap_ag: bool = True

    # Disables the Reduce-Scatter overlap with GEMM by pipelining the GEMM and Reduce-Scatter.
    tp_comm_overlap_rs: bool = True

    # Enables the Reduce-Scatter overlap with dgrad GEMM.
    tp_comm_overlap_rs_dgrad: bool = False

    # Disables the All-Gather overlap with bprop activation gradient GEMM.
    tp_comm_bulk_dgrad: bool = True

    # Disables the Reduce-Scatter overlap with bprop weight gradient GEMM.
    tp_comm_bulk_wgrad: bool = True

    # Set the bootstrapping backend of Tensor parallel communications.
    tp_comm_bootstrap_backend: Literal["nccl", "mpi", "gloo"] = "nccl"

    # If set, initialize weights on the CPU. This eliminates init differences based on tensor
    # parallelism.
    use_cpu_initialization: bool | None = None

    # Call torch.cuda.empty_cache() each iteration (training and eval), to reduce
    # fragmentation.0=off, 1=moderate, 2=aggressive.
    empty_unused_memory_level: Literal[0, 1, 2] = 0

    # Choose code that has deterministic execution. This usually means slower execution, but
    # is good for debugging and testing.
    deterministic_mode: bool = False

    # Interval to check weight hashes are same across DP replicas. If not specified, weight
    # hashes not checked.
    check_weight_hash_across_dp_replicas_interval: int | None = None

    # Scale cross entropy loss by the number of non-padded tokens in the global batch, versus
    # the default behavior of assuming all tokens are non-padded.
    calculate_per_token_loss: bool = False

    # Training CPU-GPU synchronization interval, to ensure that CPU is not running too far
    # ahead of GPU.
    train_sync_interval: int | None = None

    # Checkpoint activation to allow for training with larger models, sequences, and batch
    # sizes.
    checkpoint_activations: bool = False

    # Total number of iterations to train over all training runs. Note that either train-iters
    # or train-samples should be provided.
    train_iters: int | None = None

    # Total number of samples to train over all training runs. Note that either train-iters or
    # train-samples should be provided.
    train_samples: int | None = None

    # Report loss and timing interval.
    log_interval: int = 100

    # Exit the program after the iteration is divisible by this value.
    exit_interval: int | None = None

    # Exit the program after this many minutes.
    exit_duration_in_mins: int | None = None

    # Dynamically save the checkpoint and shutdown the training if SIGTERM is received
    exit_signal_handler: bool = False

    # Write TensorBoard logs to this directory.
    tensorboard_dir: str | None = None

    # Disable fusion of query_key_value scaling, masking, and softmax.
    masked_softmax_fusion: bool = True

    # Disable bias and gelu fusion.
    bias_gelu_fusion: bool = True

    # Disable bias and swiglu fusion, the fusion is available only when using megatron-core.
    bias_swiglu_fusion: bool = True

    # Use fused weighted squared relu when using MoE.
    use_fused_weighted_squared_relu: bool = False

    # Disable bias and dropout fusion.
    bias_dropout_fusion: bool = True

    # Disable rope fusion, the fusion is available only when using megatron-core.
    apply_rope_fusion: bool = True

    # Type of rope to use. Note that MLA takes yarn by default, and common attention takes
    # rope by default.
    rope_type: Literal["rope", "yarn"] | None = None

    # Enabled fusion of cross entropy loss calculation.
    cross_entropy_loss_fusion: bool = False

    # Implementation of cross entropy loss calculation.
    cross_entropy_fusion_impl: Literal["native", "te"] = "native"

    # use FlashAttention implementation of attention. https://arxiv.org/abs/2205.14135
    use_flash_attn: bool = False

    # Disable bias in the linear layers
    add_bias_linear: bool = True

    # Enable bias only in the QKV linear layers
    add_qkv_bias: bool = False

    # Optimizer function
    optimizer: Literal["adam", "sgd"] = "adam"

    # Offload optimizer state to CPU
    optimizer_cpu_offload: bool = False

    # Ratio of optimizer state to offload to CPU
    optimizer_offload_fraction: float = 1.0

    # Use torch.optim.Optimizer instead of Megatron's optimizer in optimizer cpu offload mode.
    use_torch_optimizer_for_cpu_offload: bool = False

    # Overlap CPU optimizer step, gradients D2H and updated parameters H2D.
    overlap_cpu_optimizer_d2h_h2d: bool = False

    # Disable pinning of CPU memory for gradients.
    pin_cpu_grads: bool = True

    # Disable pinning of CPU memory for parameters.
    pin_cpu_params: bool = True

    # Single pass vs multiple pass data loader
    dataloader_type: Literal["single", "cyclic", "external"] | None = None

    # DEPRECATED. This flag is ignored.
    async_tensor_model_parallel_allreduce: bool = True

    # Disable using persistent fused layer norm kernel. This kernel supports only a set of
    # hidden sizes. Please check persist_ln_hidden_sizes if your hidden size is supported.
    no_persist_layer_norm: bool = False

    # Enable sequence parallel optimization.
    sequence_parallel: bool = False

    # Disable fusing gradient accumulation to weight gradient computation of linear layers
    gradient_accumulation_fusion: bool = True

    # DEPRECATED. Use the implementation from megatron core.Now ignored and mcore models are
    # the default, use --use-legacy-models to not use core models.
    deprecated_use_mcore_models: bool = False

    # Use the legacy Megatron models, not Megatron-Core models.
    use_legacy_models: bool = False

    # Disable the threshold-based default garbage collector and trigger the garbage collection
    # manually. Manual garbage collection helps to align the timing of the collection across
    # ranks which mitigates the impact of CPU-associated jitters. When the manual gc is
    # enabled, garbage collection is performed only at the start and the end of the validation
    # routine by default.
    manual_gc: bool = False

    # Training step interval to trigger manual garbage collection. When the value is set to 0,
    # garbage collection is not triggered between training steps.
    manual_gc_interval: int = 0

    # When using manual garbage collection, disable garbage collection at the start and the
    # end of each evaluation run.
    manual_gc_eval: bool = True

    # Disables the All-Gather overlap with fprop GEMM.
    tp_comm_split_ag: bool = True

    # Disables the Reduce-Scatter overlap with fprop GEMM.
    tp_comm_split_rs: bool = True

    # Select a communicator backend for pipeline parallel communication. If None, the default
    # backend will be used.
    pipeline_model_parallel_comm_backend: Literal["nccl", "ucc"] | None = None

    # The communicator group names to use high priority streams.
    high_priority_stream_groups: list[str] = field(default_factory=lambda: [])

    # Random seed used for python, numpy, pytorch, and cuda.
    seed: int = 1234

    # Enable random initialization of params across data parallel ranks
    data_parallel_random_init: bool = False

    # Standard deviation of the zero mean normal distribution used for weight initialization.
    init_method_std: float = 0.02

    # Standard deviation of the zero mean normal distribution used for embedding weight
    # initialization. If unset, embeddings will be initialized the same way as other weights.
    # Setting this to a value around 1.0 may avoid loss spikes in training. Setting this to
    # any value will also skip applying weight decay on embedding weights to avoid shrinkage
    # towards zero. See https://arxiv.org/abs/2312.16903 for more details.
    embedding_init_method_std: float | None = None

    # Enable Xavier uniform parameter initialization
    init_method_xavier_uniform: bool = False

    # Initial learning rate. Depending on decay style and initial warmup, the learning rate at
    # each iteration would be different.
    lr: float | None = None

    # Learning rate decay function.
    lr_decay_style: Literal["constant", "linear", "cosine", "inverse-square-root", "WSD"] = "linear"

    # Decay style for the annealing phase of WSD
    lr_wsd_decay_style: Literal["exponential", "linear", "cosine", "minus_sqrt"] = "exponential"

    # number of iterations to decay learning rate over, If None defaults to `--train-iters`
    lr_decay_iters: int | None = None

    # number of samples to decay learning rate over, If None defaults to `--train-samples`
    lr_decay_samples: int | None = None

    # number of samples for the annealing phase in the wsd schedule
    lr_wsd_decay_samples: int | None = None

    # number of iterations for the annealing phase in the wsd schedule
    lr_wsd_decay_iters: int | None = None

    # fraction of lr-warmup-(iters/samples) to use for warmup (as a float)
    lr_warmup_fraction: float | None = None

    # number of iterations to linearly warmup learning rate over.
    lr_warmup_iters: int = 0

    # number of samples to linearly warmup learning rate over.
    lr_warmup_samples: int = 0

    # Initial value for learning rate warmup. The scheduler starts warmup from this value.
    lr_warmup_init: float = 0.0

    # Old lr warmup argument, do not use. Use one of the--lr-warmup-* arguments above
    warmup: int | None = None

    # Minimum value for learning rate. The schedulerclip values below this threshold.
    min_lr: float = 0.0

    # Reset the values of the scheduler (learning rate,warmup iterations, minimum learning
    # rate, maximum number of iterations, and decay style from input arguments and ignore
    # values from checkpoints. Notethat all the above values will be reset.
    override_opt_param_scheduler: bool = False

    # Use checkpoint to set the values of the scheduler (learning rate, warmup iterations,
    # minimum learning rate, maximum number of iterations, and decay style from checkpoint and
    # ignore input arguments.
    use_checkpoint_opt_param_scheduler: bool = False

    # Separate learning rate for the input and output layer
    decoupled_lr: float | None = None

    # Minimum value for learning rate for the input and output layer. The schedulerclip values
    # below this threshold
    decoupled_min_lr: float | None = None

    # Output directory to save checkpoints to.
    save: str | None = None

    # Number of iterations between persistent checkpoint saves.
    save_interval: int | None = None

    # Number of iterations between retained checkpoints (othercheckpoints _except the last
    # checkpoint_ are automatically deleted).
    save_retain_interval: int | None = None

    # Do not save current optimizer.
    no_save_optim: bool | None = None

    # Do not save current rng state.
    no_save_rng: bool | None = None

    # Directory containing a model checkpoint.
    load: str | None = None

    # Do not load optimizer when loading checkpoint.
    no_load_optim: bool | None = None

    # Load main parameters from checkpoint directly.
    load_main_params_from_ckpt: bool | None = None

    # Do not load rng state when loading checkpoint.
    no_load_rng: bool | None = None

    # Do not strict loading for fsdp_dtensor checkpoint format.
    strict_fsdp_dtensor_load: bool = True

    # Number of iterations between non-persistent saves.
    non_persistent_save_interval: int | None = None

    # Type of non-persistent model checkpoints. "global" - Saved as a standard checkpoint
    # (e.g., on Lustre) with old checkpoints being removed. "local" - Each rank saves a
    # portion of the checkpoint locally (e.g., on SSD/ramdisk). None - No non-persistent
    # checkpointing (default option).
    non_persistent_ckpt_type: Literal["global", "local", "in_memory", None] = None

    # Directory containing global non-persistent model checkpoints.
    non_persistent_global_ckpt_dir: str | None = None

    # Directory containing local non-persistent model checkpoints.
    non_persistent_local_ckpt_dir: str | None = None

    # Algorithm for local non-persistent checkpointing.
    non_persistent_local_ckpt_algo: Literal["fully_parallel", "atomic"] = "fully_parallel"

    # Load model for finetuning. Do not load optimizer or rng state from checkpoint and set
    # iteration to 0. Assumed when loading a release checkpoint.
    finetune: bool = False

    # Directory containing a pretrained model checkpoint for finetuning.
    pretrained_checkpoint: str | None = None

    # Checkpoint step to load model from.
    ckpt_step: int | None = None

    # Do not perform initialization when building model, can reduce startup time when
    # definitely loading from a checkpoint
    perform_initialization: bool = True

    # Override model-related command-line arguments with arguments from checkpoint
    use_checkpoint_args: bool = False

    # Copy model parallelism command-line arguments from checkpoint
    use_mp_args_from_checkpoint_args: bool = False

    # If set, do not use tokenizer model path from checkpoint
    use_tokenizer_model_from_checkpoint_args: bool = True

    # If '--load' is set, but checkpoint is not found (e.g., path typo), then exit instead of
    # random initialization.
    exit_on_missing_checkpoint: bool = False

    # Deprecated: see --ckpt-format.
    use_dist_ckpt_deprecated: bool = False

    # Enables a persitent checkpoint worker for async save
    use_persistent_ckpt_worker: bool = False

    # Determine if the checkpoint format is in legacy or distributed format. If False, expects
    # distributed checkpoint iff args.ckpt_format != "torch". Might slow down loading a bit
    # (double rank0 ckpt load).
    auto_detect_ckpt_format: bool = False

    # Deprecated: see --ckpt-format.
    dist_ckpt_format_deprecated: Any | None = None

    # Checkpoint format to use. torch is the format used by torch.save/load. torch_dist is a
    # megatron built-in distributed checkpointing format. torch_dcp is the
    # torch.distributed.checkpoint format. fsdp_dtensor is a torch DCP native, Megatron FSDP
    # training-specific checkpoint format.
    ckpt_format: Literal["torch", "torch_dist", "zarr", "torch_dcp", "fsdp_dtensor"] = "torch_dist"

    # Checkpoint format for conversion.
    ckpt_convert_format: Literal["torch", "torch_dist", "zarr"] | None = None

    # Save directory for converted checkpoint.
    ckpt_convert_save: Any | None = None

    # When loading a checkpoint, update the legacy format for the distributed optimizer, which
    # previously used a merged param/grad buffer and a different bucket mapping. The legacy
    # format was deprecated on Feb 13, 2024.
    ckpt_convert_update_legacy_dist_opt_format: bool = False

    # Deprecated: see --no-ckpt-fully-parallel-save.
    ckpt_fully_parallel_save_deprecated: bool = False

    # Disable applying full save parallelization across DP for distributed checkpoints.
    # Depending on ckpt format might decrease the number of files in the checkpoint. Makes
    # DistributedOptimizer checkpoint non-reshardable.
    ckpt_fully_parallel_save: bool = True

    # Apply async checkpointing save. Currently works only with`torch_dist` distributed
    # checkpoint format.
    async_save: bool | None = None

    # Apply full load parallelization across DP for distributed checkpoints.
    ckpt_fully_parallel_load: bool = False

    # If the model and optimizer state dict structure isconstant throughout a *single training
    # job*, it allows fordifferent checkpointing performance optimizations.
    ckpt_assume_constant_structure: bool = False

    # Determine handling of key mismatch during checkpoint load. Check StrictHandling docs for
    # flags meaning. NOTE: This flag controls only distributed checkpoint load from storage,
    # not loading state dict into the model.
    dist_ckpt_strictness: Literal[
        "assume_ok_unexpected",
        "log_unexpected",
        "log_all",
        "raise_unexpected",
        "raise_all",
        "return_unexpected",
        "return_all",
        "ignore_all",
    ] = "assume_ok_unexpected"

    # Load a checkpoint for TensorRT model optimizer (nvidia-modelopt).This function can also
    # be used to load NeMo .nemo sharded checkpoints.
    load_model_opt_format: bool = False

    # Run model in fp16 mode.
    fp16: bool = False

    # Run model in bfloat16 mode.
    bf16: bool = False

    # Reduce gradients in bfloat16.
    grad_reduce_in_bf16: bool = False

    # Static loss scaling, positive power of 2 values can improve fp16 convergence. If None,
    # dynamicloss scaling is used.
    loss_scale: float | None = None

    # Initial loss-scale for dynamic loss scaling.
    initial_loss_scale: float = 4294967296

    # Minimum loss scale for dynamic loss scaling.
    min_loss_scale: float = 1.0

    # Window over which to raise/lower dynamic scale.
    loss_scale_window: float = 1000

    # hysteresis for dynamic loss scaling
    hysteresis: int = 2

    # Move residual connections to fp32.
    fp32_residual_connection: bool = False

    # Scale Q * K^T by 1 / layer-number. Useful for fp16 training. Also sets
    # `attention_softmax_in_fp32` to True.
    apply_query_key_layer_scaling: bool = False

    # Run attention masking and softmax in fp32.
    attention_softmax_in_fp32: bool = False

    # Gradient accumulation and all-reduce in fp32.
    accumulate_allreduce_grads_in_fp32: bool = False

    # Move the cross entropy unreduced loss calculationfor lm head to fp16.
    fp16_lm_cross_entropy: bool = False

    # If True, sets torch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction=False to
    # prevent matmul from using reduced precision accumulation when using BF16.
    disable_bf16_reduced_precision_matmul: bool = False

    # If True, reuse the grad buffer for MXFP8 parameter all-gather.
    reuse_grad_buf_for_mxfp8_param_ag: bool = False

    # Degree of tensor model parallelism.
    tensor_model_parallel_size: int = 1

    # Degree of pipeline model parallelism.
    pipeline_model_parallel_size: int = 1

    # The number of transformer layers on the first pipeline stage of the decoder. Default
    # None is even split of transformer layers across all pipeline stages
    decoder_first_pipeline_num_layers: int | None = None

    # The number of transformer layers on the last pipeline stage of the decoder. Default None
    # is even split of transformer layers across all pipeline stages
    decoder_last_pipeline_num_layers: int | None = None

    # A string that describes a custom pipeline model parallel layout. e.g.,
    # "E|(t|)*3,m|m||L". E, L, t, m denotes embedding, loss, transformer decoder layer, and
    # mtp layer, respectively. Stages are split by "|". Replicated stages or layers can be
    # described with multiplication. Commas can be used cosmetically. Default None is not
    # using this argument to set the layout.
    pipeline_model_parallel_layout: str | None = None

    # Old model parallel argument, do not use. Use --tensor-model-parallel-size instead.
    model_parallel_size: int | None = None

    # Number of layers per virtual pipeline stage
    num_layers_per_virtual_pipeline_stage: int | None = None

    # Number of virtual pipeline stages per pipeline parallelism rank
    num_virtual_stages_per_pipeline_rank: int | None = None

    # Number of contiguous microbatches per virtual pipeline stage
    microbatch_group_size_per_vp_stage: int | None = None

    # overlap pipeline parallel communication with forward and backward chunks in 1F1B
    overlap_p2p_comm: bool = True

    # if set, overlap pipeline parallel communication in warmup and flush
    overlap_p2p_comm_warmup_flush: bool = False

    # Which backend to use for distributed training.
    distributed_backend: Literal["nccl", "gloo"] = "nccl"

    # Timeout minutes for torch.distributed.
    distributed_timeout_minutes: int = 10

    # If set, overlap DDP grad reduce.
    overlap_grad_reduce: bool = False

    # If set, defers the vocabulary projection linear layer weightgradient compute to pipeline
    # flush.
    defer_embedding_wgrad_compute: bool = False

    # Number of micro-batches for whichweight gradient computation of vocabulary projection is
    # deferred, defaults to 0 whichmeans all the micro-batches are deferred. Invalid if
    # `defer-embedding-wgrad-compute`is not set
    wgrad_deferral_limit: int = 0

    # If not set, all PP stages will launch gradient reduces simultaneously. Otherwise, each
    # PP stage will independently launch as needed.
    align_grad_reduce: bool = True

    # Number of buckets for data-parallel communication
    ddp_num_buckets: int | None = None

    # Bucket size for data-parallel communication
    ddp_bucket_size: int | None = None

    # If set, make sure the bucket size is divisible by a large power of 2 (2^16) to ensure
    # NCCL collectives have high bus bandwidth at large DP counts, since NCCL message size
    # (which for ring algorithms is bucket_size / dp_size) apparently needs to be divisible by
    # a power of 2 for high busbw.
    ddp_pad_buckets_for_high_nccl_busbw: bool = False

    # If set, average directly in data-parallel communication collective.
    ddp_average_in_collective: bool = False

    # If set, overlap param all-gather in distributed optimizer.
    overlap_param_gather: bool = False

    # If set, overlap param all-gather of first bucket with optimizer step.
    overlap_param_gather_with_optimizer_step: bool = False

    # If not set, all PP stages will launch param all-gathers simultaneously. Otherwise, each
    # PP stage will independently launch as needed.
    align_param_gather: bool = True

    # If not set, use scatter/gather to optimize communication of tensors in pipeline.
    scatter_gather_tensors_in_pipeline: bool = True

    # If set, use custom-built ring exchange for p2p communications. Note that this option
    # will require a custom built image that support ring-exchange p2p.
    use_ring_exchange_p2p: bool = False

    # local rank passed from distributed launcher.
    local_rank: int = 0

    # If set to True, initialize_megatron() skips DDP initialization and returns function to
    # complete it instead. Also turns on --use-cpu-initialization flag. This is for external
    # DDP manager.
    lazy_mpu_init: bool | None = None

    # If set, *input* embedding layer will be treated as a standard transformerlayer in the
    # context of partition and placement for pipeline parallelism.
    account_for_embedding_in_pipeline_split: bool = False

    # If set, loss layer will be treated as a standard transformerlayer in the context of
    # partition and placement for pipeline parallelism.
    account_for_loss_in_pipeline_split: bool = False

    # Use distributed optimizer.
    use_distributed_optimizer: bool = False

    # Use the userbuffer registration for DP/FSDP communication buffers.This option will
    # reduce GPU SM usage for the DP/FSDP communication,which is improving the performance of
    # the overlapped computation.
    nccl_ub: bool = False

    # Required to enable SHARP communication.
    use_sharp: bool = False

    # IB SHARP can be enabled from only one communication group. By default, it is enabled
    # from dp group. Available options: [dp, dp_replica]
    sharp_enabled_group: Literal["dp", "dp_replica"] | None = None

    # Use the Megatron FSDP code path in DDP.
    use_megatron_fsdp: bool = False

    init_model_with_meta_device: bool = False

    # Sharding strategy of data parallelism.
    data_parallel_sharding_strategy: Literal[
        "no_shard", "optim", "optim_grads", "optim_grads_params"
    ] = "no_shard"

    # If not set, fuse the division in gradient reduce.
    gradient_reduce_div_fusion: bool = True

    # Enable double buffering for temporary memory needed for Megatron FSDP communications.
    # Double-buffering the communication memory improves memory management efficiency by
    # reusing previously allocated buffers, rather than creating new buffers for each FSDP
    # communication. This is required for user buffer registration and is enabled by default
    # when using NCCL user buffers.
    fsdp_double_buffer: bool = False

    # Specifies the number of elements to communicate at once during FSDP (Fully Sharded Data
    # Parallel) operations. This flag also affects FSDP all-gather prefetch behavior. Setting
    # a larger value increases the communication buffer size, while a smaller value disables
    # prefetching and may degrade performance. Adjust this value based on your system's memory
    # and performance requirements.
    suggested_communication_unit_size: int | None = None

    # If set, keep the fp8 transpose cache when using Megatron FSDP.
    keep_fp8_transpose_cache: bool = False

    # If set, enable full sharding in megatron-fsdp Hybrid Sharded Data Parallel (HSDP) mode.
    enable_full_sharding_in_hsdp: bool = False

    # Number of Distributed Optimizer copies across Data Parallel domain.
    num_distributed_optimizer_instances: int = 1

    # Use the torch FSDP2 implementation. FSDP2 has not been tested with pipeline parallelism,
    # and may contain bugs.
    use_torch_fsdp2: bool = False

    # Whether to reshard weights after forward pass when using PyTorch FSDP2. Set to enable
    # FSDP ZeRO-2.
    torch_fsdp2_reshard_after_forward: bool = True

    # Degree of context parallelism.
    context_parallel_size: int = 1

    # Inter-gpu communication type for context parallelism: p2p, a2a, allgather or a2a+p2p. If
    # a single string is provided, all layers will share the same communication type. Users
    # can also specify separated types for each layer like --cp-comm-type p2p p2p a2a a2a
    # a2a+p2p a2a+p2p
    cp_comm_type: list[str] = field(default_factory=lambda: ["p2p"])

    # Degrees of the hierarchical context parallelism. Users should provide a list to specify
    # the sizes for different levels. --hierarchical-context-parallel-sizes 2 4 indicates
    # every two adjacent gpus forms the first level of cp groups and the cp ranks with the
    # same odevity forms the second level of cp groups.
    hierarchical_context_parallel_sizes: list[int] | None = None

    # Path to the yaml file with NCCL communicator configurations. The number of min/max
    # thread groups and thread group cluster size of each communicator can be configured by
    # setting `min_ctas`, `max_ctas`, and `cga_cluster_size`.
    nccl_communicator_config_path: str | None = None

    # If set, distributed ranks initialize order is changed from tp-cp-ep-dp-pp to tp-cp-ep-
    # pp-dp.
    use_tp_pp_dp_mapping: bool = False

    # If set, replication of local checkpoints is enabled. Needs to be enabled on all ranks.
    replication: bool = False

    # Specifies `J`, the spacing between ranks storing replicas of a given rank's data.
    # Replicas for rank `n` may be on ranks `n+J`, `n+2J`, ..., or `n-J`, `n-2J`, etc. This
    # flag has an effect only if --replication is used. and must be consistent across all
    # ranks.
    replication_jump: int | None = None

    # Number of machines storing the replica of a given rank's data.
    replication_factor: int = 2

    # If set, each time validation occurs it uses the full validation dataset(s). This
    # currently only works for GPT datasets!
    full_validation: bool = False

    # If set, multiple datasets listed in the validation split are evaluated independently
    # with a separate loss for each dataset in the list. This argument requires that no
    # weights are included in the list
    multiple_validation_sets: bool = False

    # Number of iterations to run for evaluationvalidation/test for.
    eval_iters: int = 100

    # Interval between running evaluation on validation set.
    eval_interval: int = 1000

    # Run all real-time test alongside the experiment.
    test_mode: bool = False

    # If set, bypass the training loop, optionally do evaluation for validation/test, and
    # exit.
    skip_train: bool = False

    # The weight and prefix list for a set of train, validation, and testdatasets which split
    # according to --split. The accepted formats are: (1) a single prefix, (2) a list of
    # weight prefix pairs e.g. weight1 prefix1 weight2 prefix2, (3) a list of prefixes e.g.
    # prefix1 prefix2. For (3), weights are inferred from the lengths of the contributing
    # datasets. This argument is exclusive to the other independent --*-data-path arguments.
    data_path: list[str] | None = None

    # Comma-separated list of proportions for training, validation, and test split. For
    # example the split `90,5,5` will use 90%% of data for training, 5%% for validation and
    # 5%% for test.
    split: str | None = None

    # The weight and prefix list for an independent train dataset. Follows the same pattern
    # rules as --data-path.
    train_data_path: list[str] | None = None

    # The weight and prefix list for an independent validation dataset. Follows the same
    # pattern rules as --data-path.
    valid_data_path: list[str] | None = None

    # The weight and prefix list for an independent test dataset. Follows the same pattern
    # rules as --data-path.
    test_data_path: list[str] | None = None

    # Path to data-args. Instead of feeding `--data-path` with weighted dataset, we pass in a
    # file path from which we read that argument. This is useful when the list of data is too
    # big.
    data_args_path: str | None = None

    # Path to per-split-data-args. Instead of feeding `--(train|valid|test)-data-path` with
    # weighted dataset, we pass in a file path from which we read those arguments. This is
    # useful when the list of data is too big. Format is a json file with `train`, `valid,
    # `test` keys
    per_split_data_args_path: str | None = None

    # Path to a directory to hold cached index files.
    data_cache_path: Any | None = None

    # Disable mmap-ing of .bin files.
    mmap_bin_files: bool = True

    # Skip data loading and validation and opt for artificial generation of mock data when an
    # implementation is available.
    mock_data: bool = False

    # Maximum sequence length to process.
    seq_length: int | None = None

    # Maximum encoder sequence length to process.This should be exclusive of --seq-length
    encoder_seq_length: int | None = None

    # Maximum decoder sequence length to process.
    decoder_seq_length: int | None = None

    # Maximum sequence length for the biencoder model for retriever
    retriever_seq_length: int = 256

    # sample rate for training data. Supposed to be 0  < sample_rate < 1
    sample_rate: float = 1.0

    # Probability of replacing a token with mask.
    mask_prob: float = 0.15

    # Probability of producing a short sequence.
    short_seq_prob: float = 0.1

    # Dataloader number of workers.
    num_workers: int = 2

    # Reset posistion ids after end-of-document token.
    reset_position_ids: bool = False

    # Reset self attention maske after end-of-document token.
    reset_attention_mask: bool = False

    # Mask loss for the end of document tokens.
    eod_mask_loss: bool = False

    # If set, do not create attention_masks in dataloader.
    create_attention_mask_in_dataloader: bool = True

    # Number of parallel threads per rank for dataset builder
    num_dataset_builder_threads: int = 1

    # Path to cache index files when using s3 or msc dataloader
    object_storage_cache_path: str | None = None

    # The sample surplus to build for the mid-level datasets(s)
    mid_level_dataset_surplus: float = 0.005

    # Size of vocab before EOD or padding.
    vocab_size: int | None = None

    # Vocabulary size of the model (padded to be divisible by tensor model parallel size). If
    # not provided, it will be automatically calculated from vocab-size.
    padded_vocab_size: int | None = None

    # Path to the vocab file.
    vocab_file: str | None = None

    # Path to the BPE merge file.
    merge_file: str | None = None

    # Number of additional vocabulary tokens. They are used for span masking in the T5 model
    vocab_extra_ids: int = 0

    # What type of tokenizer to use.
    tokenizer_type: (
        Literal[
            "BertWordPieceLowerCase",
            "BertWordPieceCase",
            "GPT2BPETokenizer",
            "SentencePieceTokenizer",
            "GPTSentencePieceTokenizer",
            "HuggingFaceTokenizer",
            "Llama2Tokenizer",
            "TikTokenizer",
            "MultimodalTokenizer",
            "NullTokenizer",
            "NullMultimodalTokenizer",
            "SFTTokenizer",
        ]
        | None
    ) = None

    # Sentencepiece tokenizer model.
    tokenizer_model: str | None = None

    # Use Megatron-LM legacy tokenizer system (works offline without HuggingFace).
    legacy_tokenizer: bool = False

    # Which tiktoken pattern to use. Options: [v1, v2]
    tiktoken_pattern: str | None = None

    # Number of special tokens in tiktoken tokenizer
    tiktoken_num_special_tokens: int = 1000

    # List of tiktoken special tokens, needs to have ["<unk>", "<s>", "</s>"]
    tiktoken_special_tokens: list[str] | None = None

    # Enable autoresume on adlr cluster.
    adlr_autoresume: bool = False

    # Intervals over which check for autoresumetermination signal
    adlr_autoresume_interval: int = 1000

    # Size of block embeddings to be used in ICT and REALM (paper default: 128)
    ict_head_size: int | None = None

    # Size of projection head used in biencoder (paper default: 128)
    biencoder_projection_dim: int = 0

    # Whether to share the parameters of the query and context models or not
    biencoder_shared_query_context_model: bool = False

    # Directory containing an ICTBertModel checkpoint
    ict_load: str | None = None

    # Directory containing an BertModel checkpoint (needed to start ICT and REALM)
    bert_load: str | None = None

    # Path to titles dataset used for ICT
    titles_data_path: str | None = None

    # Probability of keeping query in block for ICT dataset
    query_in_block_prob: float = 0.1

    # Whether to use one sentence documents in ICT
    use_one_sent_docs: bool = False

    # Path to Wikipedia Evidence frm DPR paper
    evidence_data_path: str | None = None

    # Which top-k accuracies to report (e.g. '1 5 20')
    retriever_report_topk_accuracies: list[int] = field(default_factory=lambda: [])

    # Whether to scale retriever scores by inverse square root of hidden size
    retriever_score_scaling: bool = False

    # Where to save/load BlockData to/from
    block_data_path: str | None = None

    # Where to save/load Open-Retrieval Embedding data to/from
    embedding_path: str | None = None

    # How large of batches to use when doing indexing jobs
    indexer_batch_size: int = 128

    # After how many batches should the indexer report progress
    indexer_log_interval: int = 1000

    # num of classes in vision classificaiton task
    num_classes: int = 1000

    # Image height for vision classification task
    img_h: int = 224

    # Image height for vision classification task
    img_w: int = 224

    # Number of channels in input image data
    num_channels: int = 3

    # patch dimension
    patch_dim: int = 16

    # training with fraction of classes.
    classes_fraction: float = 1.0

    # training with fraction of data per class.
    data_per_class_fraction: float = 1.0

    # Disable data sharding.
    data_sharding: bool = True

    # learning rate multiplier for head during finetuning
    head_lr_mult: float = 1.0

    # flag to indicate vision pretraining
    vision_pretraining: bool = False

    # pretraining objectives
    vision_pretraining_type: Literal["classify", "inpaint", "dino"] = "classify"

    # backbone types types
    vision_backbone_type: Literal["vit", "mit", "swin"] = "vit"

    # pretraining objectives
    swin_backbone_type: Literal["tiny", "base", "h3"] = "tiny"

    # mask types
    mask_type: Literal["random", "row"] = "random"

    # mask size scaling parameter
    mask_factor: float = 1.0

    # iterations per epoch
    iter_per_epoch: int = 1250

    # Image size for vision classification task
    dino_local_img_size: int = 96

    # Number of local crops
    dino_local_crops_number: int = 10

    # Hidden dimension size in dino head
    dino_head_hidden_size: int = 2048

    # Bottle neck dimension in dino head
    dino_bottleneck_size: int = 256

    # Freezing last layer weights
    dino_freeze_last_layer: float = 1

    # Disable Norm in last layer.
    dino_norm_last_layer: bool = False

    # warump teacher temperature
    dino_warmup_teacher_temp: float = 0.04

    # teacher temperature
    dino_teacher_temp: float = 0.07

    # warmup teacher temperaure epochs
    dino_warmup_teacher_temp_epochs: int = 30

    # Whether to layer normalize the q and k attention embeddings.
    qk_layernorm: bool = False

    # Use llama 4 qk l2 norm
    qk_l2_norm: bool = False

    # Degree of expert model parallelism.
    expert_model_parallel_size: int = 1

    # Degree of expert model parallelism. Default is None, which will be set to the value of
    # --tensor-model-paralle-size.
    expert_tensor_parallel_size: int | None = None

    # Number of Experts in MoE (None means no MoE)
    num_experts: int | None = None

    # Frequency between MoE layers and Dense layers. Accepts either: - An integer N:
    # Represents a 1:N ratio, meaning one expert layer for every N-1 dense layers - A string
    # containing a Python list expression that defines a custom pattern, e.g.:
    # "([1]*3+[0]*1)*3" evaluates to [1,1,1,0,1,1,1,0,1,1,1,0] where 1 indicates an expert
    # layer and 0 indicates a dense layer. Examples: "([0]+[1]*23)": 1 dense layer followed by
    # 23 experts layers, "([1]*3+[0]*2)*2": Three expert layers followed by two dense layers,
    # repeated twice.
    moe_layer_freq: Any = 1

    # The hidden size of each expert's feed-forward network (ffn). If not specified, defaults
    # to the ffn_hidden_size.
    moe_ffn_hidden_size: int | None = None

    # Shared expert total ffn hidden size. It should be equal to "num_shared_experts *
    # ffn_size_of_each_shared_expert" if there are multiple shared experts. None means no
    # shared expert.
    moe_shared_expert_intermediate_size: int | None = None

    # Enable overlapping between shared expert computations and dispatcher communications.
    # Without this, the shared epxerts execute after the routed experts. Only effective when
    # moe-shared-expert-intermediate-size is set.
    moe_shared_expert_overlap: bool = False

    # When there are multiple experts per rank, launch multiple local GEMM kernels in multiple
    # streams to improve the utilization and performance with GroupedLinear in
    # TransformerEngine.
    moe_grouped_gemm: bool = False

    # Use legacy GroupedMLP rather than TEGroupedMLP. Note: The legacy one will be deprecated
    # soon.
    moe_use_legacy_grouped_gemm: bool = False

    # Enable checkpointing for moe_layer, should be used when memory is not sufficient.
    # Deprecated. Use "--recompute-granularity selective --recompute-modules moe" instead.
    moe_layer_recompute: bool = False

    # Deprecated. Use --expert-tensor-parallel-size instead.
    moe_extended_tp: bool = False

    # Load a checkpoint of a dense model, convert it into an MoE model, and save the converted
    # model to the path specified by --save. Upcycling is implemented on the top of
    # distributed checkpointing, so it supports parallel modes different from the dense model.
    moe_use_upcycling: bool = False

    # Determines the load balancing strategy for the router. "aux_loss" corresponds to the
    # load balancing loss used in GShard and SwitchTransformer; "seq_aux_loss" corresponds to
    # the load balancing loss used in DeepSeekV2, which computes the loss for each individual
    # sample; "sinkhorn" corresponds to the balancing algorithm used in S-BASE, and "none"
    # implies no load balancing. The default is "aux_loss".
    moe_router_load_balancing_type: Literal["aux_loss", "seq_aux_loss", "sinkhorn", "none"] = (
        "aux_loss"
    )

    # Data type for routing computation and expert output weighted averaging. Fp32/fp64
    # enhances numerical stability, especially with numerous experts. The perf impact should
    # be negligible when used with permute fusion. None means no changes for dtype.
    moe_router_dtype: Literal["fp32", "fp64"] | None = None

    # Enable fusion for MoE TopK routing and aux-loss computation. This is only supported in
    # TransformerEngine 2.7.0 and above.
    moe_router_fusion: bool = False

    # Score function for MoE TopK routing. Can be "softmax" or "sigmoid".
    moe_router_score_function: Literal["softmax", "sigmoid"] = "softmax"

    # Number of experts to route to for each token. The default is 2.
    moe_router_topk: int = 2

    # Enable pre-softmax routing for MoE, which means softmax is before the top-k selection.
    # By default, softmax is done after top-k.
    moe_router_pre_softmax: bool = False

    # Number of groups to divide experts into for group-limited routing. When using group-
    # limited routing: 1) Experts are divided into equal-sized groups, 2) For each token, a
    # subset of groups are selected based on routing scores (sum of top-2 expert scores within
    # each group), 3) From these selected groups, moe_router_topk experts are chosen.Two
    # common use cases: 1) Device-limited routing: Set equal to expert parallel size (EP) to
    # limit each token to experts on a subset of devices (See DeepSeek-V2:
    # https://arxiv.org/pdf/2405.04434) 2) Node-limited routing: Set equal to number of nodes
    # in EP group to limit each token to experts on a subset of nodes (See DeepSeek-V3:
    # https://arxiv.org/pdf/2412.19437)
    moe_router_num_groups: int | None = None

    # Number of selected groups for group-limited routing.
    moe_router_group_topk: int | None = None

    # Scaling factor for routing score in top-k selection, only works when --moe-router-pre-
    # softmax enabled. Defaults to None, which means no scaling.
    moe_router_topk_scaling_factor: float | None = None

    # TopK routing with dynamic expert bias in the aux-loss-free load balancing strategy. The
    # routing decision is based on the sum of the routing scores and the expert bias. See
    # https://arxiv.org/abs/2408.15664 for details.
    moe_router_enable_expert_bias: bool = False

    # Expert bias update rate in the aux-loss-free load balancing strategy. The expert bias is
    # updated based on the number of assigned tokens to each expert in a global batch, where
    # the bias is increased for the experts with less assigned tokens and decreased for the
    # experts with more assigned tokens. The default value 1e-3 is same as that used in
    # DeepSeekV3.
    moe_router_bias_update_rate: float = 0.001

    # [Experimental] Force override routing to balance token distribution using random logits
    # for MoE routers, supporting naive top-k and group-limited top-k. This experimental
    # feature is for benchmarking purposes only!
    moe_router_force_load_balancing: bool = False

    # Pad the routing_map to make sure the number of tokens each expert received is a multiple
    # of 16/32 for FP8 precision. It is suggested to enable this for dropless training with
    # FP8 precision when num_local_experts > 1. This is a more efficient way to pad for FP8
    # which eliminates the explicit padding in the GroupedMLP layer.
    moe_router_padding_for_fp8: bool = False

    # Scaling coefficient for the aux loss: a starting value of 1e-2 is recommended.
    moe_aux_loss_coeff: list[float] | float = 0.0

    # Scaling coefficient for the z-loss: a starting value of 1e-3 is recommended.
    moe_z_loss_coeff: float | None = None

    # Add noise to the input tensor by applying jitter with a specified epsilon value.
    moe_input_jitter_eps: float | None = None

    # Enable per-layer logging for MoE, currently supports auxiliary loss and z loss.
    moe_per_layer_logging: bool = False

    # The type of token dispatcher to use. The default is 'allgather'. Options are
    # 'allgather', 'alltoall'. We recommend using 'alltoall' when applying expert parallelism.
    # For more information, please refer to the documentation in core/moe/README.
    moe_token_dispatcher_type: Literal["allgather", "alltoall", "flex"] = "allgather"

    # [Experimental] Enable DeepSeek/DeepEP for efficient token dispatching and combine in MoE
    # models. Only works with flex token dispatcher by setting --moe-token-dispatcher-
    # type=flex.
    moe_enable_deepep: bool = False

    # Number of SMs to use for DeepEP.
    moe_deepep_num_sms: int = 20

    # Fuse token rearrangement ops during token dispatching.
    moe_permute_fusion: bool = False

    # The capacity factor for each expert, None means no token will be dropped.
    moe_expert_capacity_factor: float | None = None

    # Pads the input for each expert to match the expert capacity length, effective only after
    # the --moe-expert-capacity-factor is set.
    moe_pad_expert_input_to_capacity: bool = False

    # The policy to drop tokens. Can be either "probs" or "position". If "probs", the tokens
    # with the lowest probabilities will be dropped. If "position", tokens at the end of each
    # batch will be dropped.
    moe_token_drop_policy: Literal["probs", "position"] = "probs"

    # Apply probs before mlp activation for moe routing.
    moe_apply_probs_on_input: bool = False

    # Overlap the EP A2A communication by batch-level overlapping in 1f1b stage.
    overlap_moe_expert_parallel_comm: bool = False

    # Delay the wgrad compute for batch-level overlapping
    delay_wgrad_compute: bool = False

    # This param sepecifics how many times smaller is the expert hidden size compared with the
    # original dense FFN hidden size. For using granular upcycling strategy, please set this
    # param as a positive integer. If this param is set to 1, it means using the default
    # upcycling strategy.
    moe_upcycling_granularity: int = 1

    # Rank of Query tensor's low rank representation.
    q_lora_rank: int | None = None

    # Rank of Key and Value tensors' low rank representation.
    kv_lora_rank: int = 32

    # Dimension of the head in the QK projection. q_head_dim = qk_head_dim +
    # qk_pos_emb_head_dim
    qk_head_dim: int = 128

    # Dimension of the position embedding in the QK projection.
    qk_pos_emb_head_dim: int = 64

    # Dimension of the head in the V projection.
    v_head_dim: int = 128

    # Rotary scaling factor for the rotary embeddings.
    rotary_scaling_factor: float = 1.0

    # Mscale for YaRN RoPE in multi-latent attention.
    mscale: float = 1.0

    # Mscale all dimensions for YaRN RoPE in multi-latent attention.
    mscale_all_dim: float = 0.0

    # If set caches the mla down projected latents with mla flash decode.
    cache_mla_latents: bool = False

    # Path to json file containing heterogeneous model configuration. Use the format of the
    # HuggingFace config files in llama nemotron models, e.g.
    # https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1/resolve/main/config.json.
    heterogeneous_layers_config_path: str | None = None

    # This is encoded json string of the heterogeneous model configuration. Used to keep the
    # content of the heterogeneous model specification in args when the model is loaded from a
    # checkpoint. Use the format of the HuggingFace config files in llama nemotron models,
    # e.g. https://huggingface.co/nvidia/Llama-3_3-Nemotron-
    # Super-49B-v1/resolve/main/config.json.
    heterogeneous_layers_config_encoded_json: str | None = None

    # If set, calculate and log parameters norm.
    log_params_norm: bool = False

    # If set, calculate and log the number of zeros in gradient.
    log_num_zeros_in_grad: bool = False

    # If set, calculate and log throughput per GPU.
    log_throughput: bool = False

    # If set, log progress (in terms of number of processed tokens and number of floating-
    # point operations) to progress.txt file in checkpoint directory.
    log_progress: bool = False

    # Granularity level to measure and report timing.    0: report only iteration time and
    # make sure timing       does not introduce extra overhead.   1: report timing for
    # operations that are executed       very limited times (basically once) during       each
    # iteration (such as gradient all-reduce)    2: report timing for operations that migh be
    # executed numerous times during each iteration. Note that setting the level to 1 or 2
    # might cause increase in iteration time.
    timing_log_level: Literal[0, 1, 2] = 0

    # If set, log energy consumption (in Joules)
    log_energy: bool = False

    # If not set, use barrier with level 1 time measurements. Note that this is up to the user
    # to make sure calling barrier with their timers will not result in hangs. This can happen
    # if for example the user adds a level 1 timer that is not called by all ranks.
    barrier_with_L1_time: bool = True

    # Options for logging timing:  max: report the max timing across all ranks  minmax: report
    # min and max timings across all ranks  all: report timings of all ranks.
    timing_log_option: Literal["max", "minmax", "all"] = "minmax"

    # Report to tensorboard interval.
    tensorboard_log_interval: int = 1

    # Size of the tensorboard queue for pending events and summaries before one of the "add"
    # calls forces a flush to disk.
    tensorboard_queue_size: int = 1000

    # If set, write timers to tensorboard.
    log_timers_to_tensorboard: bool = False

    # Disable loss-scale logging to tensorboard.
    log_loss_scale_to_tensorboard: bool = True

    # If set, write validation perplexity to tensorboard.
    log_validation_ppl_to_tensorboard: bool = False

    # Enable memory logging to tensorboard.
    log_memory_to_tensorboard: bool = False

    # Enable world size logging to tensorboard.
    log_world_size_to_tensorboard: bool = False

    # The wandb project name. Ignore wandb by default.
    wandb_project: str = ""

    # The wandb experiment name.
    wandb_exp_name: str = ""

    # Path to save the wandb results locally.
    wandb_save_dir: str = ""

    # The wandb entity name.
    wandb_entity: str = ""

    # Set default logging level
    logging_level: int | None = None

    # If set, tracks and logs straggler per GPU.
    log_straggler: bool = False

    # If set, StragglerDetector is disabled on startup.
    disable_straggler_on_startup: bool = False

    # Port number to toggle StragglerDetector on/off at runtime
    straggler_ctrlr_port: int = 65535

    # Number of ranks to report with high/low estimated throughput
    straggler_minmax_count: int = 1

    # If set, enables workload inspector server for on-demand profiling.
    run_workload_inspector_server: bool = False

    # If (batch-size * sequence-length) is smaller than this thresholdthen batches will not be
    # split up for pipelining.Requires setting --pipeline-model-parallel-size > 1.Setting this
    # to -1 indicates that batch pipelining is not used.
    inference_batch_times_seqlen_threshold: int = -1

    # Maximum number of tokens during inferencetokens here is # in prompt + # to
    # generateAllows us to throw an error before OOM crashes server
    max_tokens_to_oom: int = 12000

    # Output Bert embeddings (via mean pooling) from model, rather than its binary head output
    # or entire hidden batch.
    output_bert_embeddings: bool = False

    # Select either Megatron or Huggingface as the Bert embedder.
    bert_embedder_type: Literal["megatron", "huggingface"] = "megatron"

    # Whether to use the flash decoding kernel.
    flash_decode: bool = False

    # Use CUDA graph capture and replay. --cuda-graph-scope="full_iteration" enables whole
    # iteration CUDA graph.
    enable_cuda_graph: bool = False

    # Number of CUDA graph warmup steps
    cuda_graph_warmup_steps: int = 3

    # Use CUDA graph capture and replay. The CUDA graphs aremanually captured in the training
    # script.
    external_cuda_graph: bool = False

    # Determines the CUDA graphs capturing scope. Valid values are "full", "attn" and
    # "full_iteration". "Full" scope captures a whole Transformer layer. "Attn" scope only
    # captures operations in TransformerLayer._forward_attention(). "ful_iteration" scope
    # captures a whole iteration.
    cuda_graph_scope: Literal["full", "attn", "full_iteration"] = "full"

    # Maximum number of requests for inference.
    inference_max_batch_size: int = 8

    # Maximum sequence length expected for inference (prefill + decode).
    inference_max_seq_length: int = 2560

    # Enable dynamic batching mode.
    inference_dynamic_batching: bool = False

    # Total buffer size (GB) allocated for the chunked KV memory.
    inference_dynamic_batching_buffer_size_gb: float = 40.0

    # KV cache chunk size. It should be a multiple of 256
    inference_dynamic_batching_chunk_size: int = 256

    # Space is reserved within the inference context memory buffer to guarantee that a minimum
    # number of active requests will always be able to run to completion. This is to avoid the
    # context being blocked by paused requests.
    inference_dynamic_batching_buffer_guaranteed_fraction: float = 0.2

    # Scaling factor over the memory buffer size for auto computing `max_requests` and
    # `max_tokens`. This scaling factor is used for fitting more requests and tokens in the
    # memory buffer than it can safely hold, which in turn increases throughput.
    inference_dynamic_batching_buffer_overflow_factor: float | None = None

    # If set, this overrides the max requests as computed from `--inference-dynamic-batching-
    # buffer-overflow-factor`.
    inference_dynamic_batching_max_requests_override: int | None = None

    # If set, this overrides the max tokens as computed from `--inference-dynamic-batching-
    # buffer-overflow-factor`.
    inference_dynamic_batching_max_tokens_override: int | None = None

    # Maximum number of cuda graphs to capture, where the cuda graph batch sizes range from 1
    # to `max_requests`. (See `dynamic_context.py` for details on how `max_requests` is
    # computed). Due to rounding, the actual number of cuda graphs may not equal this
    # argument.
    inference_dynamic_batching_num_cuda_graphs: int = 16

    # What type of symmetric all reduce to use. The default is none which is no use of
    # symetric memory
    symmetric_ar_type: Literal["two_shot", "one_shot", "multimem_all_reduce", None] = None

    # When using symmeric all reduce kernels this will use regular nccl kernels for prefill.
    # This can be more effecient when prefill is large as the nccl kernels can be more
    # bandwith optimized
    nccl_all_reduce_for_prefill: bool = False

    # Number of chunks along sequence dimension for MLP computation during prefill
    mlp_chunks_for_prefill: int = 1

    # Which fp8 format scheme to use for FP8 tensors in the forward and backward pass
    fp8: Literal["e4m3", "hybrid"] | None = None

    # Which fp8 recipe to use for FP8 tensors in the forward and backward pass
    fp8_recipe: Literal["tensorwise", "delayed", "mxfp8", "blockwise"] = "delayed"

    # Scaling margin for fp8
    fp8_margin: int = 0

    # DEPRECATED. This flag is ignored. Scaling update interval for fp8
    fp8_interval: int = 1

    # Number of steps for which amax history is recorded per tensor
    fp8_amax_history_len: int = 1

    # Algorithm for computing amax from history
    fp8_amax_compute_algo: Literal["most_recent", "max"] = "most_recent"

    # Execute wgrad in higher precision even for FP8 runs
    fp8_wgrad: bool = True

    # Which Transformer implementation to use.
    transformer_impl: Literal["local", "transformer_engine"] = "transformer_engine"

    # Keep the compute param in fp8 (do not use any other intermediate dtype) and perform the
    # param all-gather in fp8.
    fp8_param_gather: bool = False

    # Construct first and last layers in bf16 when doing FP8 training.
    first_last_layers_bf16: bool = False

    # Number of layers at start to construct in bf16 when --first-last-layers-bf16 is enabled.
    num_layers_at_start_in_bf16: int = 1

    # Number of layers at end to construct in bf16 when --first-last-layers-bf16 is enabled.
    num_layers_at_end_in_bf16: int = 1

    # Use the Transformer Engine version of the random number generator. Required for CUDA
    # graphs support.
    te_rng_tracker: bool = False

    # Use a random number generator configured for inference.
    inference_rng_tracker: bool = False

    # Retro project directory, which contains the preprocessed data for pretraining. This
    # directory is built during preprocessing (see tools/retro/README.md), and contains
    # subdirectories for the chunk database and pretraining neighbors.
    retro_project_dir: Any | None = None

    # Add a retriever to the transformer, for use in pretraining a Retro model.
    retro_add_retriever: bool = False

    # Set number of training iterations for cyclic Retro training.
    retro_cyclic_train_iters: int | None = None

    # Number of layers to use for the retrieval encoder.
    retro_encoder_layers: int = 2

    # Hidden dropout for retrieval encoder.
    retro_encoder_hidden_dropout: float = 0.1

    # Attention dropout for retrieval encoder.
    retro_encoder_attention_dropout: float = 0.1

    # Number of neighbors to retrieve during pretraining.
    retro_num_neighbors: int = 2

    # Number of chunks to retrieve from the retrieval database.
    retro_num_retrieved_chunks: int = 2

    # Gated cross attention.
    retro_attention_gate: float = 1

    # Skip verifying that len(GPT dataset) == len(saved neighbors).
    retro_verify_neighbor_count: bool = True

    # Enable experimental features.
    enable_experimental: bool = False

    # Specify the <module_location function_name> pair that returns a spec to customize a
    # model, transformer block, or transformer layer, depending on the use case.To use local
    # spec specify local as the argument.For more details, see the model class,
    # `transformer_block.py`, or `transformer_layer.py`
    spec: list[str] | None = None

    # Ratio of attention layers to total layers, in the range [0.0, 1.0].
    hybrid_attention_ratio: float = 0.0

    # Ratio of mlp layers to total layers, in the range [0.0, 1.0].
    hybrid_mlp_ratio: float = 0.0

    # Force a specific hybrid layer pattern. The valueshould be a string of characters chosen
    # fromcore.ssm.mamba_hybrid_layer_allocation.Symbols.If a value greater than 0.0 is
    # supplied to any of the hybrid ratio arguments, then the number of each typeof layer in
    # the override pattern must match number inthe overidden pattern
    hybrid_override_pattern: str | None = None

    # State dimension for Mamba layers.
    mamba_state_dim: int = 128

    # Head dimension for Mamba layers.
    mamba_head_dim: int = 64

    # Number of groups for Mamba layers.
    mamba_num_groups: int = 8

    # Number of heads for Mamba layers.If not set, then the number of heads will be --hidden-
    # size * expand // --mamba-head-dim
    mamba_num_heads: int | None = None

    # Indicates whether the model is a hybrid model.
    is_hybrid_model: bool = False

    # Disable Mamba efficient path.
    disable_mamba_mem_eff_path: bool = False

    # Config file to add additional arguments
    yaml_cfg: str | None = None

    # Use the precision-aware optimizer in TransformerEngine, which allows setting the main
    # params and optimizer states to lower precision, such as fp16, bf16 and fp8.
    use_precision_aware_optimizer: bool = False

    # Dtype of main grads when enabling precision-aware-optimizer
    main_grads_dtype: Literal["fp32", "bf16"] = "fp32"

    # Dtype of main params when enabling precision-aware-optimizer
    main_params_dtype: Literal["fp32", "fp16"] = "fp32"

    # Dtype of exp_avg (1st moment in adam optimizer) when enabling precision-aware-optimizer.
    # This dtype is used for storing the optimizer state in memory during training but does
    # not affect the precision in the kernel computation.
    exp_avg_dtype: Literal["fp32", "fp16", "bf16", "fp8"] = "fp32"

    # Dtype of exp_avg_sq (2nd moment in adam optimizer) when enabling precision-aware-
    # optimizer. This dtype is used for storing the optimizer state in memory during training
    # but does not affect the precision in the kernel computation.
    exp_avg_sq_dtype: Literal["fp32", "fp16", "bf16", "fp8"] = "fp32"

    # If set, disable using one_logger to track E2E metricsNote that one_logger is an internal
    # tool and not available externally. For installation, please go to
    # https://confluence.nvidia.com/display/MLWFO/Package+Repositoriesfor more details
    enable_one_logger: bool = True

    # The one-logger project name. Will ignore if --no-one-logger is set
    one_logger_project: str = "megatron-lm"

    # The one-logger run name displayed. Will ignore if --no-one-logger is set
    one_logger_run_name: str | None = None

    # If set, forces one_logger to use async mode.
    one_logger_async: bool = False

    # Jobs belonging to same training run, suppose to have the same name. It will be used to
    # track progress of a training done over multiple different jobs
    app_tag_run_name: str | None = None

    # The version of the training of which current job is part of. It will be used to track
    # the changes in the application side which might change the performance baseline
    app_tag_run_version: str = "0.0.0"

    # Enables in-process restart.
    inprocess_restart: bool = False

    # Maximum number of in-process restart iterations.
    inprocess_max_iterations: int | None = None

    # Monitoring interval (in seconds) for the monitoring thread.
    inprocess_monitor_thread_interval: float = 1.0

    # Monitoring interval (in seconds) for the monitoring process.
    inprocess_monitor_process_interval: float = 1.0

    # Interval (in seconds) for automatic progress watchdog timestamp updates.
    inprocess_progress_watchdog_interval: float = 1.0

    # Monitoring interval (in seconds) for detecting unresponsive ranks.
    inprocess_heartbeat_interval: float = 30

    # Soft progress timeout (in seconds).
    inprocess_soft_timeout: float = 60

    # Hard progress timeout (in seconds).
    inprocess_hard_timeout: float = 90

    # Timeout (in seconds) for a missing rank detection heartbeat.
    inprocess_heartbeat_timeout: float = 60

    # Timeout (in seconds) for internal distributed barrier
    inprocess_barrier_timeout: float = 120

    # Timeout (in seconds) for barrier on completion on all ranks
    inprocess_completion_timeout: float = 120

    # Time interval (in seconds) for other ranks to report concurrent terminal failures.
    inprocess_last_call_wait: float = 1

    # Interval (in seconds) between SIGTERM and SIGKILL issued on hard timeout
    inprocess_termination_grace_time: float = 1

    # Granularity for in-process restart.
    inprocess_granularity: Literal["node", "rank"] = "node"

    # The number of ranks initially executing the workload. The remaining ranks from the
    # allocation are set aside as warm reserve.
    inprocess_active_world_size: int = 1

    # Release all unoccupied cached GPU memory on every in-process restart.
    inprocess_empty_cuda_cache: bool = False

    # If set, Fault Tolerance package is enabled. Note: This feature is for Nvidia internal
    # use only.
    enable_ft_package: bool = False

    # If set, FT package will try to automatically compute the timeouts. Note: This feature is
    # for Nvidia internal use only.
    calc_ft_timeouts: bool = False

    # If set, will dump all configs to --config-logger-dir
    config_logger_dir: str = ""

    # Rate at which to inject unexpected results, e.g. 1000 means once every 1000 result
    # validations
    error_injection_rate: int = 0

    # Type of error to inject.
    error_injection_type: Literal["correct_result", "transient_error", "persistent_error"] = (
        "transient_error"
    )

    # Use re-run engine to validate results (default) or to emit stats on variability of
    # computations due to non-deterministic algorithms.
    rerun_mode: Literal["disabled", "validate_results", "report_stats"] = "validate_results"

    # Disable the usage of Multi-Storage Client (MSC) in Megatron Core.
    enable_msc: bool = True

    # Use the config .yaml file at the specified location to configure kitchen quantization.
    kitchen_config_file: str | None = None

    # Use a default kitchen recipe for all layers as defined by QAT_PARAMS index
    kitchen_recipe_number: int | None = None

    # Megatron SFT training
    sft: bool = False

    # SFT prompt format.
    sft_tokenizer_prompt_format: str = "nemotron-h-aligned"

    aux: dict[str, Any] = field(default_factory=dict)


__all__ = ["MegatronConfig"]
