"""Megatron-LM configuration schema (auto-generated)."""

from __future__ import annotations

from dataclasses import dataclass, field
from typing import Any, Literal
from compoconf import ConfigInterface


@dataclass
class MegatronConfig(ConfigInterface):
    """Typed projection of Megatron-LM CLI arguments."""

    # Generated by scripts/generate_megatron_dataclass.py
    # Number of transformer layers.
    num_layers: int | None = None

    # Number of encoder transformer layers.
    encoder_num_layers: int | None = None

    # Number of decoder transformer layers.
    decoder_num_layers: int | None = None

    # Tansformer hidden size.
    hidden_size: int | None = None

    # Transformer Feed-Forward Network hidden size. This is set to 4*hidden-size if not
    # provided
    ffn_hidden_size: int | None = None

    # Number of transformer attention heads.
    num_attention_heads: int | None = None

    # Attention backend to use (flash,fused,unfused,local,auto). Defaults to auto
    attention_backend: str = "auto"

    # Projection weights dimension in multi-head attention. This is set to    args.hidden_size
    # // args.num_attention_heads if not provided.
    kv_channels: int | None = None

    # Use group-query attention.
    group_query_attention: bool = False

    num_query_groups: int = 1

    # Maximum number of position embeddings to use. This is the size of position embedding.
    max_position_embeddings: int | None = None

    # Position embedding type.
    position_embedding_type: Literal["learned_absolute", "rope", "none"] = "learned_absolute"

    # Use rotary positional embeddings or not. Deprecated: use --position-embedding-type
    use_rotary_position_embeddings: bool = False

    # Base to use for rotary positional embeddings, default 10000
    rotary_base: int = 10000

    # Percent of rotary dimension to use, default 100%%
    rotary_percent: float = 1.0

    # Use interleaved rotary embedding.
    rotary_interleaved: bool = False

    # Sequence length interpolation factor for rotary embeddings.
    rotary_seq_len_interpolation_factor: int | None = None

    # Apply rope scaling as used in llama3.1
    use_rope_scaling: bool = False

    # Disable position embedding. Deprecated: use --position-embedding-type
    add_position_embedding: bool = True

    # Disable fused rope from transformer-engine: use --disable_te_fused_rope
    disable_te_fused_rope: bool = False

    # Pad the vocab size to be divisible by this value.This is added for computational
    # efficieny reasons.
    make_vocab_size_divisible_by: int = 128

    # Which normalization technique to use.
    normalization: Literal["LayerNorm", "RMSNorm"] = "LayerNorm"

    # Epsilon for layer norm and RMS norm.
    norm_epsilon: float = 1e-05

    # Adjust LayerNorm weights such that they are centered around zero. This improves
    # numerical stability.
    apply_layernorm_1p: bool = False

    # If set, use original BERT residula connection ordering.
    apply_residual_connection_post_layernorm: bool = False

    # Use OpenAIs GeLU implementation. This optionshould not be used unless for backward
    # compatibilityreasons.
    openai_gelu: bool = False

    # Use squared relu activation instead of default gelu
    squared_relu: bool = False

    # Use gated linear units and SiLU activation instead of default gelu
    swiglu: bool = False

    # Use workarounds for known problems with Torch ONNX exporter
    onnx_safe: bool | None = None

    # Disable BERT binary head.
    bert_binary_head: bool = True

    # Untie embeddings and output weights.
    untie_embeddings_and_output_weights: bool = False

    # Use multi-latent attention for model.
    multi_latent_attention: bool = False

    # Pad value head dim to the size of qk head dim, and run ck fused attention.
    fused_padded_mla_attention: bool = False

    # k attention sink tokens.
    attention_sink_k: int = 0

    # sliding window size
    window_size: int | None = None

    # Post attention dropout probability.
    attention_dropout: float = 0.1

    # Dropout probability for hidden state transformer.
    hidden_dropout: float = 0.1

    # Weight decay coefficient for L2 regularization.
    weight_decay: float = 0.01

    # Initial weight decay coefficient for L2 regularization.
    start_weight_decay: float | None = None

    # End of run weight decay coefficient for L2 regularization.
    end_weight_decay: float | None = None

    # Weight decay increment function.
    weight_decay_incr_style: Literal["constant", "linear", "cosine"] = "constant"

    # Gradient clipping based on global L2 norm.
    clip_grad: float = 1.0

    # First coefficient for computing running averages of gradient and its square
    adam_beta1: float = 0.9

    # Second coefficient for computing running averages of gradient and its square
    adam_beta2: float = 0.999

    # Term added to the denominator to improvenumerical stability
    adam_eps: float = 1e-08

    # Momentum factor for sgd
    sgd_momentum: float = 0.9

    # Batch size per model instance (local batch size). Global batch size is local batch size
    # times data parallel size times number of micro batches.
    micro_batch_size: int | None = None

    # Old batch size parameter, do not use. Use --micro-batch-size instead
    batch_size: int | None = None

    # Training batch size. If set, it should be a multiple of micro-batch-size times data-
    # parallel-size. If this value is None, then use micro-batch-size * data-parallel-size as
    # the global batch size. This choice will result in 1 for number of micro-batches.
    global_batch_size: int | None = None

    # Batch size ramp up with the following values:  --rampup-batch-size <start batch size>
    # <batch size incerement>                       <ramp-up samples> For example:   --rampup-
    # batch-size 16 8 300000 \    --global-batch-size 1024will start with global batch size 16
    # and over  (1024 - 16) / 8 = 126 intervals will increasethe batch size linearly to 1024.
    # In each intervalwe will use approximately 300000 / 126 = 2380 samples.
    rampup_batch_size: list[str] | None = None

    # If set, decrease batch size if microbatch_size * dp_sizedoes not divide batch_size.
    # Useful for KSO (Keep Soldiering On)to continue making progress if number of healthy GPUs
    # (andcorresponding dp_size) does not support current batch_size.Old batch_size will be
    # restored if training is re-started withdp_size that divides batch_size //
    # microbatch_size.
    decrease_batch_size_if_needed: bool = False

    # recompute activation to allow for training with larger models, sequences, and batch
    # sizes.
    recompute_activations: bool = False

    # Checkpoint activations to allow for training with larger models, sequences, and batch
    # sizes. It is supported at two granularities 1) full: whole transformer layer is
    # recomputed, 2) selective: core attention part of the transformer layer is recomputed.
    recompute_granularity: Literal["full", "selective"] | None = None

    # Check for NaNs in loss and grad
    check_for_nan_in_loss_and_grad: bool = True

    # Check for spiky loss
    check_for_spiky_loss: bool = False

    # If set, distribute recomputed activations across model parallel group.
    distribute_saved_activations: bool = False

    # 1) uniform: uniformly divide the total number of Transformer layers and recompute the
    # input activation of each divided chunk at specified granularity, 2) recompute the input
    # activations of only a set number of individual Transformer layers per pipeline stage and
    # do the rest without any recomputing at specified granularitydefault) do not apply
    # activations recompute to any layers
    recompute_method: Literal["uniform", "block"] | None = None

    # 1) uniform: the number of Transformer layers in each uniformly divided recompute unit,
    # 2) block: the number of individual Transformer layers to recompute within each pipeline
    # stage.
    recompute_num_layers: int | None = None

    # If not set, clone the output of the scatter in embedding layer to GC original tensor.
    clone_scatter_output_in_embedding: bool = True

    # Enable nsys profiling. When using this option, nsys options should be specified in
    # commandline. An example nsys commandline is `nsys profile -s none -t nvtx,cuda -o
    # <path/to/output_file> --force-overwrite true --capture-range=cudaProfilerApi --capture-
    # range-end=stop`.
    profile: bool = False

    # Global step to start profiling.
    profile_step_start: int = 10

    # Global step to stop profiling.
    profile_step_end: int = 12

    # Use the built-in pytorch profiler. Useful if you wish to view profiles in tensorboard.
    use_pytorch_profiler: bool = False

    # Global ranks to profile.
    profile_ranks: list[int] = field(default_factory=lambda: [0])

    # Record memory history in last rank.
    record_memory_history: bool = False

    # Specifies where to dump the memory history pickle.
    memory_snapshot_path: str = "snapshot.pickle"

    # Enables the  overlap of Tensor parallel communication and GEMM kernels.
    tp_comm_overlap: bool = False

    # Config file when tp_comm_overlap is enabled.
    tp_comm_overlap_cfg: str | None = None

    # Disables the All-Gather overlap with GEMM by pipelining the GEMM and All-Gather.
    tp_comm_overlap_ag: bool = True

    # Disables the Reduce-Scatter overlap with GEMM by pipelining the GEMM and Reduce-Scatter.
    tp_comm_overlap_rs: bool = True

    # Enables the Reduce-Scatter overlap with dgrad GEMM.
    tp_comm_overlap_rs_dgrad: bool = False

    # Disables the All-Gather overlap with bprop activation gradient GEMM.
    tp_comm_bulk_dgrad: bool = True

    # Disables the Reduce-Scatter overlap with bprop weight gradient GEMM.
    tp_comm_bulk_wgrad: bool = True

    # Set the bootstrapping backend of Tensor parallel communications.
    tp_comm_bootstrap_backend: Literal["nccl", "mpi", "gloo"] = "nccl"

    # If set, initialize weights on the CPU. This eliminates init differences based on tensor
    # parallelism.
    use_cpu_initialization: bool | None = None

    # Call torch.cuda.empty_cache() each iteration (training and eval), to reduce
    # fragmentation.0=off, 1=moderate, 2=aggressive.
    empty_unused_memory_level: Literal[0, 1, 2] = 0

    # Choose code that has deterministic execution. This usually means slower execution, but
    # is good for debugging and testing.
    deterministic_mode: bool = False

    # Interval to check weight hashes are same across DP replicas. If not specified, weight
    # hashes not checked.
    check_weight_hash_across_dp_replicas_interval: int | None = None

    # Scale cross entropy loss by the number of non-padded tokens in the global batch, versus
    # the default behavior of assuming all tokens are non-padded.
    calculate_per_token_loss: bool = False

    # Training CPU-GPU synchronization interval, to ensure that CPU is not running too far
    # ahead of GPU.
    train_sync_interval: int | None = None

    # Checkpoint activation to allow for training with larger models, sequences, and batch
    # sizes.
    checkpoint_activations: bool = False

    # Total number of iterations to train over all training runs. Note that either train-iters
    # or train-samples should be provided.
    train_iters: int | None = None

    # Total number of samples to train over all training runs. Note that either train-iters or
    # train-samples should be provided.
    train_samples: int | None = None

    # Report loss and timing interval.
    log_interval: int = 100

    # Exit the program after the iteration is divisible by this value.
    exit_interval: int | None = None

    # Exit the program after this many minutes.
    exit_duration_in_mins: int | None = None

    # Dynamically save the checkpoint and shutdown the training if SIGTERM is received
    exit_signal_handler: bool = False

    # Write TensorBoard logs to this directory.
    tensorboard_dir: str | None = None

    # Disable fusion of query_key_value scaling, masking, and softmax.
    masked_softmax_fusion: bool = True

    # Disable bias and gelu fusion.
    bias_gelu_fusion: bool = True

    # Disable bias and swiglu fusion, the fusion is available only when using megatron-core.
    bias_swiglu_fusion: bool = True

    # Disable bias and dropout fusion.
    bias_dropout_fusion: bool = True

    # Disable rope fusion, the fusion is available only when using megatron-core.
    apply_rope_fusion: bool = True

    # Enabled fusion of cross entropy loss calculation.
    cross_entropy_loss_fusion: bool = False

    # use FlashAttention implementation of attention. https://arxiv.org/abs/2205.14135
    use_flash_attn: bool = False

    # Disable bias in the linear layers
    add_bias_linear: bool = True

    # Enable bias only in the QKV linear layers
    add_qkv_bias: bool = False

    # Optimizer function
    optimizer: Literal["adam", "sgd"] = "adam"

    # Single pass vs multiple pass data loader
    dataloader_type: Literal["single", "cyclic", "external"] | None = None

    # DEPRECATED. This flag is ignored.
    async_tensor_model_parallel_allreduce: bool = True

    # Disable using persistent fused layer norm kernel. This kernel supports only a set of
    # hidden sizes. Please check persist_ln_hidden_sizes if your hidden size is supported.
    no_persist_layer_norm: bool = False

    # Enable sequence parallel optimization.
    sequence_parallel: bool = False

    # Disable fusing gradient accumulation to weight gradient computation of linear layers
    gradient_accumulation_fusion: bool = True

    # DEPRECATED. Use the implementation from megatron core.Now ignored and mcore models are
    # the default, use --use-legacy-models to not use core models.
    deprecated_use_mcore_models: bool = False

    # Use the legacy Megatron models, not Megatron-Core models.
    use_legacy_models: bool = False

    # Disable the threshold-based default garbage collector and trigger the garbage collection
    # manually. Manual garbage collection helps to align the timing of the collection across
    # ranks which mitigates the impact of CPU-associated jitters. When the manual gc is
    # enabled, garbage collection is performed only at the start and the end of the validation
    # routine by default.
    manual_gc: bool = False

    # Training step interval to trigger manual garbage collection. When the value is set to 0,
    # garbage collection is not triggered between training steps.
    manual_gc_interval: int = 0

    # When using manual garbage collection, disable garbage collection at the start and the
    # end of each evaluation run.
    manual_gc_eval: bool = True

    # Disables the All-Gather overlap with fprop GEMM.
    tp_comm_split_ag: bool = True

    # Disables the Reduce-Scatter overlap with fprop GEMM.
    tp_comm_split_rs: bool = True

    # use z-loss for supplement loss (Google PaLM method)
    use_z_loss: bool = False

    # Random seed used for python, numpy, pytorch, and cuda.
    seed: int = 1234

    # Enable random initialization of params across data parallel ranks
    data_parallel_random_init: bool = False

    # Standard deviation of the zero mean normal distribution used for weight initialization.
    init_method_std: float = 0.02

    # Enable Xavier uniform parameter initialization
    init_method_xavier_uniform: bool = False

    # Initial learning rate. Depending on decay style and initial warmup, the learning rate at
    # each iteration would be different.
    lr: float | None = None

    # Learning rate decay function.
    lr_decay_style: Literal["constant", "linear", "cosine", "inverse-square-root", "WSD"] = "linear"

    # Decay style for the annealing phase of WSD
    lr_wsd_decay_style: Literal["exponential", "linear", "cosine"] = "exponential"

    # number of iterations to decay learning rate over, If None defaults to `--train-iters`
    lr_decay_iters: int | None = None

    # number of samples to decay learning rate over, If None defaults to `--train-samples`
    lr_decay_samples: int | None = None

    # number of samples for the annealing phase in the wsd schedule
    lr_wsd_decay_samples: int | None = None

    # number of iterations for the annealing phase in the wsd schedule
    lr_wsd_decay_iters: int | None = None

    # fraction of lr-warmup-(iters/samples) to use for warmup (as a float)
    lr_warmup_fraction: float | None = None

    # number of iterations to linearly warmup learning rate over.
    lr_warmup_iters: int = 0

    # number of samples to linearly warmup learning rate over.
    lr_warmup_samples: int = 0

    # Initial value for learning rate warmup. The scheduler starts warmup from this value.
    lr_warmup_init: float = 0.0

    # Old lr warmup argument, do not use. Use one of the--lr-warmup-* arguments above
    warmup: int | None = None

    # Minimum value for learning rate. The schedulerclip values below this threshold.
    min_lr: float = 0.0

    # Reset the values of the scheduler (learning rate,warmup iterations, minimum learning
    # rate, maximum number of iterations, and decay style from input arguments and ignore
    # values from checkpoints. Notethat all the above values will be reset.
    override_opt_param_scheduler: bool = False

    # Use checkpoint to set the values of the scheduler (learning rate, warmup iterations,
    # minimum learning rate, maximum number of iterations, and decay style from checkpoint and
    # ignore input arguments.
    use_checkpoint_opt_param_scheduler: bool = False

    # Separate learning rate for the input and output layer
    decoupled_lr: float | None = None

    # Minimum value for learning rate for the input and output layer. The schedulerclip values
    # below this threshold
    decoupled_min_lr: float | None = None

    # Output directory to save checkpoints to.
    save: str | None = None

    # Number of iterations between persistent checkpoint saves.
    save_interval: int | None = None

    # Do not save current optimizer.
    no_save_optim: bool | None = None

    # Do not save current rng state.
    no_save_rng: bool | None = None

    # Directory containing a model checkpoint.
    load: str | None = None

    # Do not load optimizer when loading checkpoint.
    no_load_optim: bool | None = None

    # Do not load rng state when loading checkpoint.
    no_load_rng: bool | None = None

    # Number of iterations between non-persistent saves.
    non_persistent_save_interval: int | None = None

    # Type of non-persistent model checkpoints. "global" - Saved as a standard checkpoint
    # (e.g., on Lustre) with old checkpoints being removed. "local" - [TBD] Each rank saves a
    # portion of the checkpoint locally (e.g., on SSD/ramdisk). "in_memory" - [TBD] A special
    # kind of local checkpoint that avoids serialization. None - No non-persistent
    # checkpointing (default option).
    non_persistent_ckpt_type: Literal["global", "local", "in_memory", None] = None

    # Directory containing global non-persistent model checkpoints.
    non_persistent_global_ckpt_dir: str | None = None

    # Directory containing local non-persistent model checkpoints.
    non_persistent_local_ckpt_dir: str | None = None

    # Algorithm for local non-persistent checkpointing.
    non_persistent_local_ckpt_algo: Literal["fully_parallel", "atomic"] = "fully_parallel"

    # Load model for finetuning. Do not load optimizer or rng state from checkpoint and set
    # iteration to 0. Assumed when loading a release checkpoint.
    finetune: bool = False

    # Directory containing a pretrained model checkpoint for finetuning.
    pretrained_checkpoint: str | None = None

    # Checkpoint step to load model from.
    ckpt_step: int | None = None

    # Do not perform initialization when building model, can reduce startup time when
    # definitely loading from a checkpoint
    perform_initialization: bool = True

    # Override model-related command-line arguments with arguments from checkpoint
    use_checkpoint_args: bool = False

    # Copy model parallelism command-line arguments from checkpoint
    use_mp_args_from_checkpoint_args: bool = False

    # If set, do not use tokenizer model path from checkpoint
    use_tokenizer_model_from_checkpoint_args: bool = True

    # If '--load' is set, but checkpoint is not found (e.g., path typo), then exit instead of
    # random initialization.
    exit_on_missing_checkpoint: bool = False

    # Deprecated: see --ckpt-format.
    use_dist_ckpt_deprecated: bool = False

    # Determine if the checkpoint format is in legacy or distributed format. If False, expects
    # distributed checkpoint iff args.ckpt_format != "torch". Might slow down loading a bit
    # (double rank0 ckpt load).
    auto_detect_ckpt_format: bool = False

    # Deprecated: see --ckpt-format.
    dist_ckpt_format_deprecated: Any | None = None

    # Checkpoint format to use.
    ckpt_format: Literal["torch", "torch_dist", "zarr"] = "torch_dist"

    # Checkpoint format for conversion.
    ckpt_convert_format: Literal["torch", "torch_dist", "zarr"] | None = None

    # Save directory for converted checkpoint.
    ckpt_convert_save: Any | None = None

    # When loading a checkpoint, update the legacy format for the distributed optimizer, which
    # previously used a merged param/grad buffer and a different bucket mapping. The legacy
    # format was deprecated on Feb 13, 2024.
    ckpt_convert_update_legacy_dist_opt_format: bool = False

    # Deprecated: see --no-ckpt-fully-parallel-save.
    ckpt_fully_parallel_save_deprecated: bool = False

    # Disable applying full save parallelization across DP for distributed checkpoints.
    # Depending on ckpt format might decrease the number of files in the checkpoint. Makes
    # DistributedOptimizer checkpoint non-reshardable.
    ckpt_fully_parallel_save: bool = True

    # Apply async checkpointing save. Currently works only with`torch_dist` distributed
    # checkpoint format.
    async_save: bool | None = None

    # Apply full load parallelization across DP for distributed checkpoints.
    ckpt_fully_parallel_load: bool = False

    # If the model and optimizer state dict structure isconstant throughout a *single training
    # job*, it allows fordifferent checkpointing performance optimizations.
    ckpt_assume_constant_structure: bool = False

    # Determine handling of key mismatch during checkpoint load. Check StrictHandling docs for
    # flags meaning. NOTE: This flag controls only distributed checkpoint load from storage,
    # not loading state dict into the model.
    dist_ckpt_strictness: Literal[
        "assume_ok_unexpected",
        "log_unexpected",
        "log_all",
        "raise_unexpected",
        "raise_all",
        "return_unexpected",
        "return_all",
        "ignore_all",
    ] = "assume_ok_unexpected"

    # Run model in fp16 mode.
    fp16: bool = False

    # Run model in bfloat16 mode.
    bf16: bool = False

    # Static loss scaling, positive power of 2 values can improve fp16 convergence. If None,
    # dynamicloss scaling is used.
    loss_scale: float | None = None

    # Initial loss-scale for dynamic loss scaling.
    initial_loss_scale: float = 4294967296

    # Minimum loss scale for dynamic loss scaling.
    min_loss_scale: float = 1.0

    # Window over which to raise/lower dynamic scale.
    loss_scale_window: float = 1000

    # hysteresis for dynamic loss scaling
    hysteresis: int = 2

    # Move residual connections to fp32.
    fp32_residual_connection: bool = False

    # Scale Q * K^T by 1 / layer-number. Useful for fp16 training. Also sets
    # `attention_softmax_in_fp32` to True.
    apply_query_key_layer_scaling: bool = False

    # Run attention masking and softmax in fp32.
    attention_softmax_in_fp32: bool = False

    # Gradient accumulation and all-reduce in fp32.
    accumulate_allreduce_grads_in_fp32: bool = False

    # Move the cross entropy unreduced loss calculationfor lm head to fp16.
    fp16_lm_cross_entropy: bool = False

    # Degree of tensor model parallelism.
    tensor_model_parallel_size: int = 1

    # Degree of tensor model parallelism for the encoder.
    encoder_tensor_model_parallel_size: int = 0

    # Degree of pipeline model parallelism.
    pipeline_model_parallel_size: int = 1

    # Degree of pipeline model parallelism in the encoder. This is independent of the amount
    # of pipeline in the decoder.
    encoder_pipeline_model_parallel_size: int = 0

    # Rank where encoder and decoder should be split. Deprecated; use --encoder-pipeline-
    # model-parallel-size instead.
    pipeline_model_parallel_split_rank: int | None = None

    # The number of transformer layers on the first pipeline stage of the decoder. Default
    # None is even split of transformer layers across all pipeline stages
    decoder_first_pipeline_num_layers: int | None = None

    # The number of transformer layers on the last pipeline stage of the decoder. Default None
    # is even split of transformer layers across all pipeline stages
    decoder_last_pipeline_num_layers: int | None = None

    # Old model parallel argument, do not use. Use --tensor-model-parallel-size instead.
    model_parallel_size: int | None = None

    # Number of layers per virtual pipeline stage
    num_layers_per_virtual_pipeline_stage: int | None = None

    # Number of contiguous microbatches per virtual pipeline stage
    microbatch_group_size_per_vp_stage: int | None = None

    # overlap pipeline parallel communication with forward and backward chunks in 1F1B
    overlap_p2p_comm: bool = True

    # if set, overlap pipeline parallel communication in warmup and flush
    overlap_p2p_comm_warmup_flush: bool = False

    # Which backend to use for distributed training.
    distributed_backend: Literal["nccl", "gloo"] = "nccl"

    # Timeout minutes for torch.distributed.
    distributed_timeout_minutes: int = 10

    # If set, overlap DDP grad reduce.
    overlap_grad_reduce: bool = False

    # If set, defers the vocabulary projection linear layer weightgradient compute to pipeline
    # flush.
    defer_embedding_wgrad_compute: bool = False

    # Number of micro-batches for whichweight gradient computation of vocabulary projection is
    # deferred, defaults to 0 whichmeans all the micro-batches are deferred. Invalid if
    # `defer-embedding-wgrad-compute`is not set
    wgrad_deferral_limit: int = 0

    # If not set, all PP stages will launch gradient reduces simultaneously. Otherwise, each
    # PP stage will independently launch as needed.
    align_grad_reduce: bool = True

    # Bucket size for data-parallel communication
    ddp_bucket_size: int | None = None

    # If set, average directly in data-parallel communication collective.
    ddp_average_in_collective: bool = False

    # If set, overlap param all-gather in distributed optimizer.
    overlap_param_gather: bool = False

    # If set, overlap param all-gather of first bucket with optimizer step.
    overlap_param_gather_with_optimizer_step: bool = False

    # If not set, all PP stages will launch param all-gathers simultaneously. Otherwise, each
    # PP stage will independently launch as needed.
    align_param_gather: bool = True

    # If not set, use scatter/gather to optimize communication of tensors in pipeline.
    scatter_gather_tensors_in_pipeline: bool = True

    # If set, use custom-built ring exchange for p2p communications. Note that this option
    # will require a custom built image that support ring-exchange p2p.
    use_ring_exchange_p2p: bool = False

    # local rank passed from distributed launcher.
    local_rank: int = 0

    # If set to True, initialize_megatron() skips DDP initialization and returns function to
    # complete it instead.Also turns on --use-cpu-initialization flag. This is for external
    # DDP manager.
    lazy_mpu_init: bool | None = None

    # If set, *input* embedding layer is placed on its own pipeline stage, without any
    # transformer layers. (For T5, this flag currently only affects the encoder embedding.)
    standalone_embedding_stage: bool = False

    # Use distributed optimizer.
    use_distributed_optimizer: bool = False

    # Number of Distributed Optimizer copies across Data Parallel domain.
    num_distributed_optimizer_instances: int = 1

    # Use the torch FSDP2 implementation. FSDP2 is not currently working with Pipeline
    # Parallel.It is still not in a stable release stage, and may therefore contain bugs or
    # other potential issues.
    use_torch_fsdp2: bool = False

    # Degree of context parallelism.
    context_parallel_size: int = 1

    # Inter-gpu communication type for context parallelism: p2p, a2a, allgather or a2a+p2p. If
    # a single string is provided, all layers will share the same communication type. Users
    # can also specify separated types for each layer like --cp-comm-type p2p p2p a2a a2a
    # a2a+p2p a2a+p2p
    cp_comm_type: list[str] = field(default_factory=lambda: ["p2p"])

    # Degrees of the hierarchical context parallelism. Users should provide a list to specify
    # the sizes for different levels. --hierarchical-context-parallel-sizes 2 4 indicates
    # every two adjacent gpus forms the first level of cp groups and the cp ranks with the
    # same odevity forms the second level of cp groups.
    hierarchical_context_parallel_sizes: list[int] | None = None

    # Path to the yaml file with NCCL communicator configurations. The number of min/max
    # thread groups and thread group cluster size of each communicator can be configured by
    # setting `min_ctas`, `max_ctas`, and `cga_cluster_size`.
    nccl_communicator_config_path: str | None = None

    # If set, distributed ranks initialize order is changed from tp-dp-pp to tp-pp-dp. Make
    # sure EP and CP aren't used with this option enabled
    use_tp_pp_dp_mapping: bool = False

    # Number of iterations to run for evaluationvalidation/test for.
    eval_iters: int = 100

    # Interval between running evaluation on validation set.
    eval_interval: int = 1000

    # Run all real-time test alongside the experiment.
    test_mode: bool = False

    # If set, bypass the training loop, optionally do evaluation for validation/test, and
    # exit.
    skip_train: bool = False

    # The weight and prefix list for a set of train, validation, and testdatasets which split
    # according to --split. The accepted formats are: (1) a single prefix, (2) a list of
    # weight prefix pairs e.g. weight1 prefix1 weight2 prefix2, (3) a list of prefixes e.g.
    # prefix1 prefix2. For (3), weights are inferred from the lengths of the contributing
    # datasets. This argument is exclusive to the other independent --*-data-path arguments.
    data_path: list[str] | None = None

    # Renormalize the blend weights to account for the mid-level dataset oversampling done to
    # ensure fulfillment of the requested number of samples. Use this option if prompted.
    # Defaults to False for backward comparability in the data sample order.
    renormalize_blend_weights: bool = False

    # Comma-separated list of proportions for training, validation, and test split. For
    # example the split `90,5,5` will use 90%% of data for training, 5%% for validation and
    # 5%% for test.
    split: str | None = None

    # The weight and prefix list for an independent train dataset. Follows the same pattern
    # rules as --data-path.
    train_data_path: list[str] | None = None

    # The weight and prefix list for an independent validation dataset. Follows the same
    # pattern rules as --data-path.
    valid_data_path: list[str] | None = None

    # The weight and prefix list for an independent test dataset. Follows the same pattern
    # rules as --data-path.
    test_data_path: list[str] | None = None

    # Path to data-args. Instead of feeding `--data-path` with weighted dataset, we pass in a
    # file path from which we read that argument. This is useful when the list of data is too
    # big.
    data_args_path: str | None = None

    # Path to per-split-data-args. Instead of feeding `--(train|valid|test)-data-path` with
    # weighted dataset, we pass in a file path from which we read those arguments. This is
    # useful when the list of data is too big. Format is a json file with `train`, `valid,
    # `test` keys
    per_split_data_args_path: str | None = None

    # Path to a directory to hold cached index files.
    data_cache_path: Any | None = None

    # Disable mmap-ing of .bin files.
    mmap_bin_files: bool = True

    # Skip data loading and validation and opt for artificial generation of mock data when an
    # implementation is available.
    mock_data: bool = False

    # Maximum sequence length to process.
    seq_length: int | None = None

    # Maximum encoder sequence length to process.This should be exclusive of --seq-length
    encoder_seq_length: int | None = None

    # Maximum decoder sequence length to process.
    decoder_seq_length: int | None = None

    # Maximum sequence length for the biencoder model for retriever
    retriever_seq_length: int = 256

    # sample rate for training data. Supposed to be 0  < sample_rate < 1
    sample_rate: float = 1.0

    # Probability of replacing a token with mask.
    mask_prob: float = 0.15

    # Probability of producing a short sequence.
    short_seq_prob: float = 0.1

    # Dataloader number of workers.
    num_workers: int = 2

    # Reset posistion ids after end-of-document token.
    reset_position_ids: bool = False

    # Reset self attention maske after end-of-document token.
    reset_attention_mask: bool = False

    # Mask loss for the end of document tokens.
    eod_mask_loss: bool = False

    # If set, do not create attention_masks in dataloader.
    create_attention_mask_in_dataloader: bool = True

    # Number of parallel threads per rank for dataset builder
    num_dataset_builder_threads: int = 1

    # Path to cache index files when using s3 dataloader
    s3_cache_path: str | None = None

    # Size of vocab before EOD or padding.
    vocab_size: int | None = None

    # Path to the vocab file.
    vocab_file: str | None = None

    # Path to the BPE merge file.
    merge_file: str | None = None

    # Number of additional vocabulary tokens. They are used for span masking in the T5 model
    vocab_extra_ids: int = 0

    # What type of tokenizer to use.
    tokenizer_type: (
        Literal[
            "BertWordPieceLowerCase",
            "BertWordPieceCase",
            "GPT2BPETokenizer",
            "SentencePieceTokenizer",
            "GPTSentencePieceTokenizer",
            "HuggingFaceTokenizer",
            "Llama2Tokenizer",
            "TikTokenizer",
            "MultimodalTokenizer",
            "NullTokenizer",
            "DeepSeekV2Tokenizer",
            "DeepSeekV3Tokenizer",
        ]
        | None
    ) = None

    # Sentencepiece tokenizer model.
    tokenizer_model: str | None = None

    # Which tiktoken pattern to use. Options: [v1, v2]
    tiktoken_pattern: str | None = None

    # Number of special tokens in tiktoken tokenizer
    tiktoken_num_special_tokens: int = 1000

    # List of tiktoken special tokens, needs to have ["<unk>", "<s>", "</s>"]
    tiktoken_special_tokens: list[str] | None = None

    # Enable autoresume on adlr cluster.
    adlr_autoresume: bool = False

    # Intervals over which check for autoresumetermination signal
    adlr_autoresume_interval: int = 1000

    # Size of block embeddings to be used in ICT and REALM (paper default: 128)
    ict_head_size: int | None = None

    # Size of projection head used in biencoder (paper default: 128)
    biencoder_projection_dim: int = 0

    # Whether to share the parameters of the query and context models or not
    biencoder_shared_query_context_model: bool = False

    # Directory containing an ICTBertModel checkpoint
    ict_load: str | None = None

    # Directory containing an BertModel checkpoint (needed to start ICT and REALM)
    bert_load: str | None = None

    # Path to titles dataset used for ICT
    titles_data_path: str | None = None

    # Probability of keeping query in block for ICT dataset
    query_in_block_prob: float = 0.1

    # Whether to use one sentence documents in ICT
    use_one_sent_docs: bool = False

    # Path to Wikipedia Evidence frm DPR paper
    evidence_data_path: str | None = None

    # Which top-k accuracies to report (e.g. '1 5 20')
    retriever_report_topk_accuracies: list[int] = field(default_factory=lambda: [])

    # Whether to scale retriever scores by inverse square root of hidden size
    retriever_score_scaling: bool = False

    # Where to save/load BlockData to/from
    block_data_path: str | None = None

    # Where to save/load Open-Retrieval Embedding data to/from
    embedding_path: str | None = None

    # How large of batches to use when doing indexing jobs
    indexer_batch_size: int = 128

    # After how many batches should the indexer report progress
    indexer_log_interval: int = 1000

    # num of classes in vision classificaiton task
    num_classes: int = 1000

    # Image height for vision classification task
    img_h: int = 224

    # Image height for vision classification task
    img_w: int = 224

    # Number of channels in input image data
    num_channels: int = 3

    # patch dimension
    patch_dim: int = 16

    # training with fraction of classes.
    classes_fraction: float = 1.0

    # training with fraction of data per class.
    data_per_class_fraction: float = 1.0

    # Disable data sharding.
    data_sharding: bool = True

    # learning rate multiplier for head during finetuning
    head_lr_mult: float = 1.0

    # flag to indicate vision pretraining
    vision_pretraining: bool = False

    # pretraining objectives
    vision_pretraining_type: Literal["classify", "inpaint", "dino"] = "classify"

    # backbone types types
    vision_backbone_type: Literal["vit", "mit", "swin"] = "vit"

    # pretraining objectives
    swin_backbone_type: Literal["tiny", "base", "h3"] = "tiny"

    # mask types
    mask_type: Literal["random", "row"] = "random"

    # mask size scaling parameter
    mask_factor: float = 1.0

    # iterations per epoch
    iter_per_epoch: int = 1250

    # Image size for vision classification task
    dino_local_img_size: int = 96

    # Number of local crops
    dino_local_crops_number: int = 10

    # Hidden dimension size in dino head
    dino_head_hidden_size: int = 2048

    # Bottle neck dimension in dino head
    dino_bottleneck_size: int = 256

    # Freezing last layer weights
    dino_freeze_last_layer: float = 1

    # Disable Norm in last layer.
    dino_norm_last_layer: bool = False

    # warump teacher temperature
    dino_warmup_teacher_temp: float = 0.04

    # teacher temperature
    dino_teacher_temp: float = 0.07

    # warmup teacher temperaure epochs
    dino_warmup_teacher_temp_epochs: int = 30

    # Whether to layer normalize the q and k attention embeddings.
    qk_layernorm: bool = False

    # Degree of expert model parallelism.
    expert_model_parallel_size: int = 1

    # Degree of expert model parallelism. Default is None, which will be set to the value of
    # --tensor-model-paralle-size.
    expert_tensor_parallel_size: int | None = None

    # Number of Experts in MoE (None means no MoE)
    num_experts: int | None = None

    # Frequency between MoE layers and Dense layers. Accepts either: - An integer N:
    # Represents a 1:N ratio, meaning one expert layer for every N-1 dense layers - A string
    # containing a Python list expression that defines a custom pattern, e.g.:
    # "([1]*3+[0]*1)*3" evaluates to [1,1,1,0,1,1,1,0,1,1,1,0] where 1 indicates an expert
    # layer and 0 indicates a dense layer. Examples: "([0]+[1]*23)": 1 dense layer followed by
    # 23 experts layers, "([1]*3+[0]*2)*2": Three expert layers followed by two dense layers,
    # repeated twice.
    moe_layer_freq: Any = 1

    # The hidden size of each expert's feed-forward network (ffn). If not specified, defaults
    # to the ffn_hidden_size.
    moe_ffn_hidden_size: int | None = None

    # Shared expert total ffn hidden size. It should be equal to "num_shared_experts *
    # ffn_size_of_each_shared_expert" if there are multiple shared experts. None means no
    # shared expert.
    moe_shared_expert_intermediate_size: int | None = None

    # Enable overlapping between shared expert computations and dispatcher communications.
    # Without this, the shared epxerts execute after the routed experts. Only effective when
    # moe-shared-expert-intermediate-size is set.
    moe_shared_expert_overlap: bool = False

    # When there are multiple experts per rank, launch multiple local GEMM kernels in multiple
    # streams to improve the utilization and performance with GroupedLinear in
    # TransformerEngine.
    moe_grouped_gemm: bool = False

    # Determines the load balancing strategy for the router. "aux_loss" corresponds to the
    # load balancing loss used in GShard and SwitchTransformer; "seq_aux_loss" corresponds to
    # the load balancing loss used in DeepSeekV2, which computes the loss for each individual
    # sample; "sinkhorn" corresponds to the balancing algorithm used in S-BASE, and "none"
    # implies no load balancing. The default is "aux_loss".
    moe_router_load_balancing_type: Literal["aux_loss", "seq_aux_loss", "sinkhorn", "none"] = (
        "aux_loss"
    )

    # Score function for MoE TopK routing. Can be "softmax" or "sigmoid".
    moe_router_score_function: Literal["softmax", "sigmoid"] = "softmax"

    # Number of experts to route to for each token. The default is 2.
    moe_router_topk: int = 2

    # Enable pre-softmax routing for MoE, which means softmax is before the top-k selection.
    # By default, softmax is done after top-k.
    moe_router_pre_softmax: bool = False

    # Number of groups to divide experts into for group-limited routing. When using group-
    # limited routing: 1) Experts are divided into equal-sized groups, 2) For each token, a
    # subset of groups are selected based on routing scores (sum of top-2 expert scores within
    # each group), 3) From these selected groups, moe_router_topk experts are chosen.Two
    # common use cases: 1) Device-limited routing: Set equal to expert parallel size (EP) to
    # limit each token to experts on a subset of devices (See DeepSeek-V2:
    # https://arxiv.org/pdf/2405.04434) 2) Node-limited routing: Set equal to number of nodes
    # in EP group to limit each token to experts on a subset of nodes (See DeepSeek-V3:
    # https://arxiv.org/pdf/2412.19437)
    moe_router_num_groups: int | None = None

    # Number of selected groups for group-limited routing.
    moe_router_group_topk: int | None = None

    # Scaling factor for routing score in top-k selection, only works when --moe-router-pre-
    # softmax enabled. Defaults to None, which means no scaling.
    moe_router_topk_scaling_factor: float | None = None

    # TopK routing with dynamic expert bias in the aux-loss-free load balancing strategy. The
    # routing decision is based on the sum of the routing scores and the expert bias. See
    # https://arxiv.org/abs/2408.15664 for details.
    moe_router_enable_expert_bias: bool = False

    # Expert bias update rate in the aux-loss-free load balancing strategy. The expert bias is
    # updated based on the number of assigned tokens to each expert in a global batch, where
    # the bias is increased for the experts with less assigned tokens and decreased for the
    # experts with more assigned tokens. The default value 1e-3 is same as that used in
    # DeepSeekV3.
    moe_router_bias_update_rate: float = 0.001

    # Use legacy GroupedMLP rather than TEGroupedMLP. Note: The legacy one will be deprecated
    # soon.
    moe_use_legacy_grouped_gemm: bool = False

    # Scaling coefficient for the aux loss: a starting value of 1e-2 is recommended.
    moe_aux_loss_coeff: float = 0.0

    # Scaling coefficient for the z-loss: a starting value of 1e-3 is recommended.
    moe_z_loss_coeff: float | None = None

    # Add noise to the input tensor by applying jitter with a specified epsilon value.
    moe_input_jitter_eps: float | None = None

    # The type of token dispatcher to use. The default is 'allgather'. Options are
    # 'allgather', 'alltoall' and 'alltoall_seq'. We recommend using 'alltoall' when applying
    # expert parallelism. For more information, please refer to the documentation in
    # core/moe/README.
    moe_token_dispatcher_type: Literal["allgather", "alltoall", "alltoall_seq"] = "allgather"

    # Enable per-layer logging for MoE, currently supports auxiliary loss and z loss.
    moe_per_layer_logging: bool = False

    # The capacity factor for each expert, None means no token will be dropped.
    moe_expert_capacity_factor: float | None = None

    # Pads the input for each expert to match the expert capacity length, effective only after
    # the --moe-expert-capacity-factor is set.
    moe_pad_expert_input_to_capacity: bool = False

    # The policy to drop tokens. Can be either "probs" or "position". If "probs", the tokens
    # with the lowest probabilities will be dropped. If "position", tokens at the end of each
    # batch will be dropped.
    moe_token_drop_policy: Literal["probs", "position"] = "probs"

    # Enable checkpointing for moe_layer, should be used when memory is not sufficient.
    moe_layer_recompute: bool = False

    # Deprecated. Use --expert-tensor-parallel-size instead.
    moe_extended_tp: bool = False

    # Load a checkpoint of a dense model, convert it into an MoE model, and save the converted
    # model to the path specified by --save. Upcycling is implemented on the top of
    # distributed checkpointing, so it supports parallel modes different from the dense model.
    moe_use_upcycling: bool = False

    # Fuse token rearrangement ops during token dispatching.
    moe_permute_fusion: bool = False

    # Rank of Query tensor's low rank representation.
    q_lora_rank: int | None = None

    # Rank of Key and Value tensors' low rank representation.
    kv_lora_rank: int = 32

    # Dimension of the head in the QK projection. q_head_dim = qk_head_dim +
    # qk_pos_emb_head_dim
    qk_head_dim: int = 128

    # Dimension of the position embedding in the QK projection.
    qk_pos_emb_head_dim: int = 64

    # Dimension of the head in the V projection.
    v_head_dim: int = 128

    # Rotary scaling factor for the rotary embeddings.
    rotary_scaling_factor: float = 1.0

    # If set, calculate and log parameters norm.
    log_params_norm: bool = False

    # If set, calculate and log the number of zeros in gradient.
    log_num_zeros_in_grad: bool = False

    # If set, calculate and log throughput per GPU.
    log_throughput: bool = False

    # If set, log progress (in terms of number of processed tokens and number of floating-
    # point operations) to progress.txt file in checkpoint directory.
    log_progress: bool = False

    # Granularity level to measure and report timing.    0: report only iteration time and
    # make sure timing       does not introduce extra overhead.   1: report timing for
    # operations that are executed       very limited times (basically once) during       each
    # iteration (such as gradient all-reduce)    2: report timing for operations that migh be
    # executed numerous times during each iteration. Note that setting the level to 1 or 2
    # might cause increase in iteration time.
    timing_log_level: Literal[0, 1, 2] = 0

    # If not set, use barrier with level 1 time measurements. Note that this is up to the user
    # to make sure calling barrier with their timers will not result in hangs. This can happen
    # if for example the user adds a level 1 timer that is not called by all ranks.
    barrier_with_L1_time: bool = True

    # Options for logging timing:  max: report the max timing across all ranks  minmax: report
    # min and max timings across all ranks  all: report timings of all ranks.
    timing_log_option: Literal["max", "minmax", "all"] = "minmax"

    # Report to tensorboard interval.
    tensorboard_log_interval: int = 1

    # Size of the tensorboard queue for pending events and summaries before one of the ‘add’
    # calls forces a flush to disk.
    tensorboard_queue_size: int = 1000

    # If set, write timers to tensorboard.
    log_timers_to_tensorboard: bool = False

    # If set, write batch-size to tensorboard.
    log_batch_size_to_tensorboard: bool = False

    # Disable loss-scale logging to tensorboard.
    log_loss_scale_to_tensorboard: bool = True

    # If set, write validation perplexity to tensorboard.
    log_validation_ppl_to_tensorboard: bool = False

    # Enable memory logging to tensorboard.
    log_memory_to_tensorboard: bool = False

    # Enable world size logging to tensorboard.
    log_world_size_to_tensorboard: bool = False

    # The wandb project name. Ignore wandb by default.
    wandb_project: str = ""

    # The wandb experiment name.
    wandb_exp_name: str = ""

    # Path to save the wandb results locally.
    wandb_save_dir: str = ""

    # Set default logging level
    logging_level: int | None = None

    # If set, tracks and logs straggler per GPU.
    log_straggler: bool = False

    # If set, StragglerDetector is disabled on startup.
    disable_straggler_on_startup: bool = False

    # Port number to toggle StragglerDetector on/off at runtime
    straggler_ctrlr_port: int = 65535

    # Number of ranks to report with high/low estimated throughput
    straggler_minmax_count: int = 1

    # If (batch-size * sequence-length) is smaller than this thresholdthen batches will not be
    # split up for pipelining.Requires setting --pipeline-model-parallel-size > 1.Setting this
    # to -1 indicates that batch pipelining is not used.
    inference_batch_times_seqlen_threshold: int = -1

    # Maximum number of tokens during inferencetokens here is # in prompt + # to
    # generateAllows us to throw an error before OOM crashes server
    max_tokens_to_oom: int = 12000

    # Output Bert embeddings (via mean pooling) from model, rather than its binary head output
    # or entire hidden batch.
    output_bert_embeddings: bool = False

    # Select either Megatron or Huggingface as the Bert embedder.
    bert_embedder_type: Literal["megatron", "huggingface"] = "megatron"

    # Whether to use the flash decoding kernel.
    flash_decode: bool = False

    # Maximum sequence length allocated for prefill during inference.
    inference_max_seq_length: int = 2560

    # Which fp8 format scheme to use for FP8 tensors in the forward and backward pass
    fp8: Literal["e4m3", "hybrid"] | None = None

    # Scaling margin for fp8
    fp8_margin: int = 0

    # DEPRECATED. This flag is ignored. Scaling update interval for fp8
    fp8_interval: int = 1

    # Number of steps for which amax history is recorded per tensor
    fp8_amax_history_len: int = 1

    # Algorithm for computing amax from history
    fp8_amax_compute_algo: Literal["most_recent", "max"] = "most_recent"

    # Execute wgrad in higher precision even for FP8 runs
    fp8_wgrad: bool = True

    # Which Transformer implementation to use.
    transformer_impl: Literal["local", "transformer_engine"] = "transformer_engine"

    # Keep the compute param in fp8 (do not use any other intermediate dtype) and perform the
    # param all-gather in fp8.
    fp8_param_gather: bool = False

    # Keep the fp8 weight transpose cache in memory to avoid recomputing it  This will use
    # more memory
    keep_fp8_weight_transpose_cache: bool = False

    # Retro project directory, which contains the preprocessed data for pretraining. This
    # directory is built during preprocessing (see tools/retro/README.md), and contains
    # subdirectories for the chunk database and pretraining neighbors.
    retro_project_dir: Any | None = None

    # Add a retriever to the transformer, for use in pretraining a Retro model.
    retro_add_retriever: bool = False

    # Set number of training iterations for cyclic Retro training.
    retro_cyclic_train_iters: int | None = None

    # Number of layers to use for the retrieval encoder.
    retro_encoder_layers: int = 2

    # Hidden dropout for retrieval encoder.
    retro_encoder_hidden_dropout: float = 0.1

    # Attention dropout for retrieval encoder.
    retro_encoder_attention_dropout: float = 0.1

    # Number of neighbors to retrieve during pretraining.
    retro_num_neighbors: int = 2

    # Number of chunks to retrieve from the retrieval database.
    retro_num_retrieved_chunks: int = 2

    # Gated cross attention.
    retro_attention_gate: float = 1

    # Skip verifying that len(GPT dataset) == len(saved neighbors).
    retro_verify_neighbor_count: bool = True

    # Specify the <module_location function_name> pair that returns a spec to customize a
    # model, transformer block, or transformer layer, depending on the use case.To use local
    # spec specify local as the argument.For more details, see the model class,
    # `transformer_block.py`, or `transformer_layer.py`
    spec: list[str] | None = None

    # Ratio of attention layers to total layers, in the range [0.0, 1.0].
    hybrid_attention_ratio: float = 0.0

    # Ratio of mlp layers to total layers, in the range [0.0, 1.0].
    hybrid_mlp_ratio: float = 0.0

    # Force a specific hybrid layer pattern. The valueshould be a string of characters chosen
    # fromcore.ssm.mamba_hybrid_layer_allocation.Symbols.If a value greater than 0.0 is
    # supplied to any of the hybrid ratio arguments, then the number of each typeof layer in
    # the override pattern must match number inthe overidden pattern
    hybrid_override_pattern: str | None = None

    # Config file to add additional arguments
    yaml_cfg: str | None = None

    # Enforce token dispatch balancing to MoE routing
    moe_router_force_load_balancing: bool = False

    # If set, disable using one_logger to track E2E metricsNote that one_logger is an internal
    # tool and not available externally. For installation, please go to
    # https://confluence.nvidia.com/display/MLWFO/Package+Repositoriesfor more details
    enable_one_logger: bool = True

    # The one-logger project name. Will ignore if --no-one-logger is set
    one_logger_project: str = "megatron-lm"

    # The one-logger run name displayed. Will ignore if --no-one-logger is set
    one_logger_run_name: str | None = None

    # If set, forces one_logger to use async mode.
    one_logger_async: bool = False

    # Jobs belonging to same training run, suppose to have the same name. It will be used to
    # track progress of a training done over multiple different jobs
    app_tag_run_name: str | None = None

    # The version of the training of which current job is part of. It will be used to track
    # the changes in the application side which might change the performance baseline
    app_tag_run_version: str = "0.0.0"

    # If set, Fault Tolerance package is enabled. Note: This feature is for Nvidia internal
    # use only.
    enable_ft_package: bool = False

    # If set, will dump all configs to --config-logger-dir
    config_logger_dir: str = ""

    # Rate at which to inject unexpected results, e.g. 1000 means once every 1000 result
    # validations
    error_injection_rate: int = 0

    # Type of error to inject.
    error_injection_type: Literal["correct_result", "transient_error", "persistent_error"] = (
        "transient_error"
    )

    te_fallback_layernorm_linear: bool = False

    # Use re-run engine to validate results (default) or to emit stats on variability of
    # computations due to non-deterministic algorithms.
    rerun_mode: Literal["disabled", "validate_results", "report_stats"] = "disabled"

    aux: dict[str, Any] = field(default_factory=dict)


__all__ = ["MegatronConfig"]
