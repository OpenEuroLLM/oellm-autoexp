# @package _global_
defaults:
  - ../base
  - /backend: megatron_torchrun_localaddr
  - /container: jupiter
  - /slurm: jupiter
  - /backend/megatron: [base,qwen3_moe_30BA3B]
  - /sweep: none
  - _self_

backend:
  megatron:
    train_iters: 100
    global_batch_size: 16
    micro_batch_size: 4
    expert_model_parallel_size: 4
    eval_interval: 1000
    seq_length: 4096
    lr: 0.001
    log_interval: 1
    data_path:
     - /e/project1/projectnucleus/franke5/data/datasets/cerebras-SlimPajama-627B/train/merged
    data_cache_path: /e/project1/projectnucleus/poeppel1/data/
  env:
    TOKENIZERS_PARALLELISM: "false"
    TRANSFORMERS_CACHE: ${oc.env:HF_HOME}/transformers
    HUGGINGFACE_HUB_CACHE: ${oc.env:HF_HOME}/hub
    HF_HOME: ${oc.env:HF_HOME}
slurm:
  sbatch:
    nodes: 2

job:
  name: megatron_moe_qwen30BA3B_jupiter_speed_n${slurm.sbatch.nodes}


sweep:
  store_sweep_json: true
  groups:
    - type: "product"
      params:
        slurm.sbatch.nodes: [2, 4, 8, 16, 32, 64, 128]
        backend.megatron.global_batch_size: "\\${oc.eval:'\\${slurm.sbatch.nodes} * 4}"
