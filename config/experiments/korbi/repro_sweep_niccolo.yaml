# @package _global_
defaults:
  - /backend: megatron_torchrun
  - /container: lumi
  - /slurm: lumi
  - /monitoring: megatron_basic
  - /project: default
  - /sweep: none
  - /backend/megatron: base
  - _self_


project:
  base_output_dir: "dense_300M_sweep"
  name: dense_300M_01_lr{backend.megatron.lr}_gbsz{backend.megatron.global_batch_size}_beta2{backend.megatron.adam_beta2}
  resume: true

backend:
  megatron:
    wandb_exp_name: "\\${project.name}"
    wandb_project: oellm-train

    data_path: [
      /scratch/project_462000963/preprocessed/gpt-neox-20b/nemotron-cc/1.0/high-actual,
      /scratch/project_462000963/preprocessed/gpt-neox-20b/nemotron-cc/1.0/high-synthetic-distill,
      /scratch/project_462000963/preprocessed/gpt-neox-20b/nemotron-cc/1.0/high-synthetic-diverse_qa_pairs,
      /scratch/project_462000963/preprocessed/gpt-neox-20b/nemotron-cc/1.0/high-synthetic-extract_knowledge,
      /scratch/project_462000963/preprocessed/gpt-neox-20b/nemotron-cc/1.0/high-synthetic-knowledge_list,
      /scratch/project_462000963/preprocessed/gpt-neox-20b/nemotron-cc/1.0/high-synthetic-wrap_medium
    ]
    seq_length: 4096
    max_position_embeddings: 4096
    tokenizer_model: EleutherAI/gpt-neox-20b
    tokenizer_type: HuggingFaceTokenizer
    num_workers: 8
    num-dataset-builder-threads: 6 # lumi-specific
    split: '99,1,0'

    # MODEL: 300M
    num_layers: 20
    hidden_size: 896
    ffn_hidden_size: 3584
    num_attention_heads: 14
    num_query_groups: ${.num_attention_heads} # no gqa when NUM_QUERY_GROUPS==NUM_ATTENTION_HEADS TODO: check
    group_query_attentions: False
    init_method_std: 0.02
    position_embedding_type: "rope"
    rotary_base: 10000
    rotary_percent: 1.0
    attention_dropout: 0.0
    hidden_dropout: 0.0
    normalization: RMSNorm
    norm_epsilon: 1.e-5 # rmsnorm epsilon
    qk_layernorm: True # potentially enable qk-norm
    bf16: True
    swiglu: True
    untie_embeddings_and_output_weights: False
    use_flash_attn: True
    attention_softmax_in_fp32: False # OpenSci style
    disable_bias_linear: False

    # TRAINING, PARALLELISM
    recompute_activations: False
    use_torch_fsdp2: False # it's either this or the next 4 args
    tensor_model_parallel_size: 1
    pipeline_model_parallel_size: 1
    context_parallel_size: 1
    sequence_parallel: True

    distributed_backend: nccl
    distributed_timeout_minutes: 10

    # DISTRIBUTED OPTIM
    use_distributed_optimizer: True

    # OPTIMIZER
    optimizer: adam
    adam_beta1: 0.9
    adam_beta2: 0.95 # changed from 0.99 to 0.95 according to Porian
    adam_eps: 1.e-8
    clip_grad: 1.0
    weight_decay: 0.1

    # SCHEDULER
    min_lr: 0.0
    lr_decay_style: WSD
    lr_wsd_decay_style: linear
    lr_warmup_iters: 1000 # For now we fix it to 1k

    # PROFILE
    profile: False
    use_pytorch_profiler: False
    profile_ranks: [0]
    profile_step_start: 5
    profile_step_end: 12

    # EVALS
    eval_interval: 1000 # default
    eval_iters: 100 # default

    # LOGGING
    log_interval: 10
    log_progress: True
    log_throughput: True
    tensorboard_queue_size: 5

    # Set CHECKPOINT_FORMAT
    ckpt_format: "torch" # "torch_dist" is bugged (on both lumi and leo) TODO: fix
    save_interval: 2000

    # LUMI-specific configs
    te-fallback-layernorm-linear: True
    no-bias-swiglu-fusion: True
    no-bias-dropout-fusion: True
    no-gradient-accumulation-fusion: True

    # machine + model size specific
    global_batch_size: 16 # will be overridden!
    micro_batch_size: 8
    train_iters: "${oc.eval:\\(${.aux.tokens}+${.seq_length}*${.global_batch_size}-1\\)//\\(${.seq_length}*${.global_batch_size}\\)}"
    # base
    lr_wsd_decay_iters: 0
    aux:
      tokens: 50_000_000_000


slurm:
  sbatch:
    nodes: ${oc.eval:${backend.megatron.global_batch_size}//\(${backend.megatron.micro_batch_size}*${slurm.sbatch.gpus_per_node}\)}


# add monitoring part that triggers cooldowns
monitoring:
  log_events:
    # Monitor checkpoint saves to trigger cooldown
    - name: checkpoint_saved_cooldown
      pattern: 'successfully saved checkpoint from iteration\s+(?P<iteration>\d+) to (?P<path>\S+)'
      pattern_type: regex
      metadata:
        kind: checkpoint
        trigger: cooldown_check
      extract_groups:
        checkpoint_iteration: iteration
        checkpoint_path: path
      actions:
        # Log the checkpoint save
        - class_name: LogAction
          message: "checkpoint saved at iteration {checkpoint_iteration}"

        # Cooldown action - triggers new training run with modified config
        - class_name: EventAction
          mode: queue
          aux:
            decay_fraction: 0.2
            train_iters_list: [6_000_000_000, 12_000_000_000, 30_000_000_000, 50_000_000_000]
            start_iters: "${oc.eval:\\[int\\(x*\\(1-${.decay_fraction}\\)\\)\\ for\\ x\\ in\\ ${.train_iters_list}\\]}"
            start_iters_round: "${oc.eval:\\[int\\(\\(s//${backend.megatron.save_interval}\\)*${backend.megatron.save_interval}\\)\\ for\\ s\\ in\\ ${.start_iters}\\]}"

          conditions:
            # Only execute when we hit the cooldown start iteration
            - class_name: MetadataCondition
              key: checkpoint_iteration
              within: ${...aux.start_iters_round}
          action:
            class_name: RunAutoexpAction
            script: scripts/run_autoexp_container.py
            # Load unresolved config to enable flexible path management
            config_path: "{output_dir}/provenance/unresolved_config.yaml"
            overrides:
              # Absolute path for load (pinned to original checkpoint)
              - backend.megatron.load={checkpoint_path}

              - "project.base_output_dir={project.base_output_dir}_decay{checkpoint_iteration}"

              # Update project name to avoid conflicts
              - project.name={project.name}_decay{checkpoint_iteration}

              # Cooldown-specific training parameters
              - "++backend.megatron.aux.decay_fraction=${...aux.decay_fraction}"
              - "++backend.megatron.aux.tokens=\\${oc.eval:int\\({checkpoint_iteration}/\\(1.-${...aux.decay_fraction}\\)\\)*${backend.megatron.global_batch_size}}"
              - "++backend.megatron.lr_wsd_decay_iters=\\${oc.eval:int\\(\\${backend.megatron.train_iters}*\\${backend.megatron.aux.decay_fraction}\\)}"
              - "++backend.megatron.override_opt_param_scheduler=true"

              # Use simplified monitoring to avoid recursive cooldowns
              - monitoring=megatron_basic



sweep:
  grids:
  - backend.megatron.lr: [2.5e-4, 5.e-4, 1.e-3]
    backend.megatron.global_batch_size: [64, 128]

  - # 9 points
    backend.megatron.lr: [5.e-4, 1.e-3, 2.e-3]
    backend.megatron.global_batch_size: [256, 512, 1024]
