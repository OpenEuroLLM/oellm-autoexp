# @package _global_
defaults:
  - /backend: megatron
  - /container: lumi
  - /slurm: lumi
  - /job: default
  - /sweep: none
  - /backend/megatron: base
  - _self_


index: 0
stage: "all"

job:
  base_output_dir: /scratch/project_462000963/users/galicato/AutoExp/output/
  name: "06B_100BT_propella_ita_Latn_edu"
  log_path_current: /scratch/project_462000963/users/galicato/AutoExp/logs/latest.log
  log_path: /scratch/project_462000963/users/galicato/AutoExp/logs/%x-%j.log

backend:
  megatron:
    wandb_exp_name: 06B_100BT_propella_ita_Latn_edu
    wandb_project: openeurollm-project

    log_throughput: true
    log_progress: true
    log_interval: 1

    # MODEL: 600M
    num_layers: 22
    hidden_size: 1024
    ffn_hidden_size: 3840
    num_attention_heads: 16
    num_query_groups: ${.num_attention_heads}
    init_method_std: 0.02
    position_embedding_type: rope
    rotary_base: 10000
    rotary_percent: 1.0   # Need to check this
    attention_dropout: 0.0
    hidden_dropout: 0.0
    normalization: RMSNorm
    norm_epsilon: 1.e-5
    qk_layernorm: True
    bf16: True
    swiglu: True
    untie_embeddings_and_output_weights: False
    use_flash_attn: True
    attention_softmax_in_fp32: True
    add_bias_linear: false
    add_qkv_bias: false
    apply_rope_fusion: false
    masked_softmax_fusion: false
    gradient_accumulation_fusion: false
    bias_dropout_fusion: false
    async_tensor_model_parallel_allreduce: False

    # DATA
    data_path:
      - "/scratch/project_462000963/preprocessed/gpt-oss/openeurollm/propella-annotations/ita_Latn/educational_value/ita_Latn_edu"
    seq_length: 4096
    max_position_embeddings: 4096
    num_workers: 2
    data_cache_path: /flash/project_462000963/users/galicato/megatron_cache

    # TOKENIZER
    tokenizer_model: openai/gpt-oss-120b
    tokenizer_type: HuggingFaceTokenizer
    make_vocab_size_divisible_by: 128

    # TRAINING, PARALLELISM
    recompute_activations: False
    use_torch_fsdp2: False
    tensor_model_parallel_size: 1
    pipeline_model_parallel_size: 1
    context_parallel_size: 1
    sequence_parallel: True

    # COMM OPTIONS
    overlap_grad_reduce: True
    distributed_backend: nccl
    distributed_timeout_minutes: 30

    # DISTRIBUTED OPTIM
    use_distributed_optimizer: True

    # OPTIMIZER
    optimizer: adam
    adam_beta1: 0.9
    adam_beta2: 0.95
    adam_eps: 1.e-8
    clip_grad: 1.0
    weight_decay: 0.05

    # SCHEDULER
    min_lr: 0.0
    lr_decay_style: WSD
    lr_wsd_decay_style: linear

    # PROFILE
    profile: False
    use_pytorch_profiler: False
    profile_ranks: [0]
    profile_step_start: 5
    profile_step_end: 7

    # EVALS
    eval_interval: 5000
    eval_iters: 100

    # LOGGING
    tensorboard_queue_size: 5

    # Base training configuration (for stable phase)
    micro_batch_size: 4
    global_batch_size: 1024  
    lr: 1e-3  

    # training schedule. Round numbers to avoid floating point issues
    train_iters: ${oc.eval:'-(-100_000_000_000 // (${.global_batch_size} * ${.seq_length}))'}
    lr_warmup_iters: ${oc.eval:'min(${.train_iters} // 10, 25000)'}
    lr_wsd_decay_iters: ${oc.eval:'${.train_iters} * 0.2'}
    lr_decay_iters: ${.train_iters}

    # CHECKPOINT FORMAT
    ckpt_format: "torch"
    save_interval: 2000
    
    # These paths can be set by ${project.base_output_dir} but for tests I decided to define them as absolute path
    save: /scratch/project_462000963/users/galicato/mt_chpts/OELLM/ita_Latn/propella_edu 
    load: /scratch/project_462000963/users/galicato/mt_chpts/OELLM/ita_Latn/propella_edu 
    tensorboard_dir: /scratch/project_462000963/users/galicato/mt_chpts/OELLM/ita_Latn/propella_edu/tensorboard # e.g ${project.base_output_dir}/tensorboard

slurm:
  sbatch:
    nodes: 16
    cpus-per-task: 7
    ntasks-per-node: 8
    gpus-per-node: 8
    time: "01:00:00"
    mem: 480G
    partition: dev-g
    exclusive: True
  env:
    CUDA_DEVICE_MAX_CONNECTIONS: 1
    CC: gcc-12
    CXX: g++-12
