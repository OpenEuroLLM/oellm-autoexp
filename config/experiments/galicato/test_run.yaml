# @package _global_

defaults:
  - base
  - /backend: megatron_rocm
  - /container: lumi
  - /slurm: lumi
  - /monitoring: megatron_basic
  - /project: default
  - /sweep: none
  - /backend/megatron_rocm: base
  - _self_

#job:
#  base_output_dir: "test_job_galicato"
#  name: "06B_100BT_ita_Latn_propella_edu"
project:
  name: 06B_100BT_propella_ita_Latn_edu
  resume: true

slurm:
  sbatch:
    partition: standard-g
    nodes: 16
    time: 48:00:00
backend:
  megatron_rocm:
    log_throughput: true
    log_interval: 1
    log_progress: true

    # DATA
    data_path:
      - "/scratch/project_462000963/preprocessed/gpt-oss/openeurollm/propella-annotations/ita_Latn/educational_value/ita_Latn_edu"
    seq_length: 4096
    max_position_embeddings: 4096
    num_workers: 2
    data_cache_path: ${oc.env:USER_FLASH}/.cache

    # TOKENIZER
    tokenizer_model: openai/gpt-oss-120b
    tokenizer_type: HuggingFaceTokenizer

    # MODEL: 600M
    num_layers: 22
    hidden_size: 1024
    ffn_hidden_size: 3840
    num_attention_heads: 16
    num_query_groups: ${.num_attention_heads}
    init_method_std: 0.02
    position_embedding_type: "rope"
    rotary_base: 10000
    rotary_percent: 1.0   # Need to check this
    attention_dropout: 0.0
    hidden_dropout: 0.0
    normalization: RMSNorm
    norm_epsilon: 1.e-5
    qk_layernorm: True
    bf16: True
    swiglu: True
    untie_embeddings_and_output_weights: False
    use_flash_attn: True
    attention_softmax_in_fp32: True

    # TRAINING, PARALLELISM
    recompute_activations: False
    use_torch_fsdp2: False
    tensor_model_parallel_size: 1
    pipeline_model_parallel_size: 1
    context_parallel_size: 1
    sequence_parallel: True

    # COMM OPTIONS
    overlap_grad_reduce: True
    distributed_timeout_minutes: 120

    distributed_backend: nccl
    distributed_timeout_minutes: 30

    # DISTRIBUTED OPTIM
    use_distributed_optimizer: True

    # OPTIMIZER
    #optimizer: adam
    adam_beta1: 0.9
    adam_beta2: 0.95
    adam_eps: 1.e-8
    clip_grad: 1.0
    weight_decay: 0.05

    # SCHEDULER
    min_lr: 0.0
    lr_decay_style: WSD
    lr_wsd_decay_style: linear

    # PROFILE
    profile: False
    use_pytorch_profiler: False
    profile_ranks: [0]
    profile_step_start: 5
    profile_step_end: 7

    # EVALS
    eval_interval: 5000
    eval_iters: 100

    # LOGGING
    log_interval: 1
    log_progress: True
    log_throughput: True
    tensorboard_queue_size: 5

    # Base training configuration (for stable phase)
    micro_batch_size: 4
    global_batch_size: 1024  # Will be overridden by sweep
    lr: 1e-3  # Will be overridden by sweep

    # training schedule. Round numbers to avoid floating point issues
    train_iters: ${oc.eval:'-(-100_000_000_000 // (${.global_batch_size} * ${.seq_length}))'}
    lr_warmup_iters: ${oc.eval:'min(${.train_iters} // 10, 25000)'}
    lr_wsd_decay_iters: ${oc.eval:'${.train_iters} * 0.2'}
    lr_decay_iters: ${.train_iters}

    # CHECKPOINT FORMAT
    ckpt_format: "torch"
    save_interval: 2000
    
    save: ${project.base_output_dir}
    load: ${project.base_output_dir}
    tensorboard_dir: ${project.base_output_dir}/tensorboard
