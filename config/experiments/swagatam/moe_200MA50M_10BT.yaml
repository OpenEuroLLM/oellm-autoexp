# @package _global_
defaults:
  - /backend: megatron_torchrun
  - /container: leonardo
  - /slurm: leonardo
  - /job: default
  - /sweep: none
  - /backend/megatron: base
  - _self_

index: 0
stage: ""

job:
  name: "moe_200MA50M_10BT_sched${backend.megatron.lr_decay_style}_lr${backend.megatron.lr}_gbsz${backend.megatron.global_batch_size}_beta2${backend.megatron.adam_beta2}"
  base_output_dir: "results/moe_200MA50M_10BT/${job.name}"


slurm:
  sbatch:
    nodes: 4
    time: "10:00:00"


# sweep:
#   type: product
#   groups:
#     # Ablation: learning rate and batch size
#     - type: product
#       params:
#         backend.megatron.lr: [0.001, 0.003, 0.007]
#         backend.megatron.global_batch_size: [256, 512, 1024]
    
#     # Ablation: aux loss coefficient (use 'list' for non-cartesian)
#     # - type: list
#     #   configs:
#     #     - backend.megatron.moe_aux_loss_coeff: 1e-4
#     #     - backend.megatron.moe_aux_loss_coeff: 1e-3
#     #     - backend.megatron.moe_aux_loss_coeff: 1e-2

backend:
  megatron:

    # Follows the 50M dense configuration but replaces the FFN layers by MoE layers
    num_layers: 12
    hidden_size: 384
    ffn_hidden_size: 1536  # unused for MoE, see below
    num_attention_heads: 6
    num_query_groups: ${.num_attention_heads}
    init_method_std: 0.02
    position_embedding_type: "rope"
    rotary_base: 10000
    rotary_percent: 1.0
    attention_dropout: 0.0
    hidden_dropout: 0.0
    normalization: RMSNorm
    norm_epsilon: 1.e-5
    qk_layernorm: True
    bf16: True
    swiglu: True
    untie_embeddings_and_output_weights: False
    use_flash_attn: True
    attention_softmax_in_fp32: False
    add_bias_linear: True

    # TRAINING, PARALLELISM
    recompute_activations: False
    use_torch_fsdp2: False
    tensor_model_parallel_size: 1
    pipeline_model_parallel_size: 1
    context_parallel_size: 1
    sequence_parallel: True
    expert_model_parallel_size: 1
    distributed_backend: nccl

    # DATA
    seq_length: 4096
    max_position_embeddings: 4096
    num_workers: 8
    split: '99,1,0'
    legacy_tokenizer: True
    data_path: [
      /leonardo_work/OELLM_prod2026/preprocessed/gpt-neox-20b/nemotron-cc/1.0/high-actual,
      /leonardo_work/OELLM_prod2026/preprocessed/gpt-neox-20b/nemotron-cc/1.0/high-synthetic-distill,
      /leonardo_work/OELLM_prod2026/preprocessed/gpt-neox-20b/nemotron-cc/1.0/high-synthetic-diverse_qa_pairs,
      /leonardo_work/OELLM_prod2026/preprocessed/gpt-neox-20b/nemotron-cc/1.0/high-synthetic-extract_knowledge,
      /leonardo_work/OELLM_prod2026/preprocessed/gpt-neox-20b/nemotron-cc/1.0/high-synthetic-knowledge_list,
      /leonardo_work/OELLM_prod2026/preprocessed/gpt-neox-20b/nemotron-cc/1.0/high-synthetic-wrap_medium
    ]

    # DISTRIBUTED OPTIM
    use_distributed_optimizer: True

    # OPTIMIZER
    optimizer: adam
    adam_beta1: 0.9
    adam_beta2: 0.95
    adam_eps: 1.e-8
    clip_grad: 1.0
    weight_decay: 0.1

    # SCHEDULER
    min_lr: 1.e-05
    lr_decay_style: WSD
    lr_wsd_decay_style: linear

    # PROFILE
    profile: False
    use_pytorch_profiler: False
    profile_ranks: [0]
    profile_step_start: 5
    profile_step_end: 12

    # EVALS
    eval_interval: 100 # 160?
    eval_iters: 100

    # Mixture of Experts (MoE) specific parameters
    moe_ffn_hidden_size: 768
    num_experts: 16
    moe_router_topk: 2
    moe_router_dtype: "fp32"
    moe_grouped_gemm: True
    moe_token_dispatcher_type: "allgather"
    moe_router_load_balancing_type: "aux_loss"
    moe_aux_loss_coeff: 1e-3
    # moe_z_loss_coeff: 1e-3 # if want to add z-loss

    # Base training configuration (for stable phase)
    micro_batch_size: 16
    global_batch_size: 512
    lr: 0.007

    # COMM OPTIONS
    distributed_timeout_minutes: 120
    overlap_param_gather: False
    overlap_grad_reduce: True
    gradient_accumulation_fusion: False
    overlap_moe_expert_parallel_comm: False
    bias_swiglu_fusion: False
    bias_dropout_fusion: False

    ckpt_format: torch_dist

    log_throughput: true
    log_interval: 1
    log_progress: true
    save: "${job.base_output_dir}/"
    load: "${job.base_output_dir}/"
    save_interval: 10000  # TODO: reduce this to save checkpoints
    data_cache_path: ${oc.env:OELLM_CACHE_DIR,".cache"}
    vocab_file: /leonardo_work/OELLM_prod2026/models/EleutherAI/gpt-neox-20b/vocab.json
    merge_file: /leonardo_work/OELLM_prod2026/models/EleutherAI/gpt-neox-20b/merges.txt

    train_iters: ${oc.eval:'-(-10_000_000_000 // (${.global_batch_size} * ${.seq_length}))'}
    lr_wsd_decay_iters: ${oc.eval:'${.train_iters} * 0.2'}
    lr_decay_iters: ${.train_iters}
    lr_warmup_iters: ${oc.eval:'min(${.train_iters} // 10, 25000)'}

    # wandb_project: "${job.name}"
    wandb_exp_name: "${job.name}"
    wandb_project: moe-runs
    wandb_entity: openeurollm-project
    wandb_save_dir: "${job.base_output_dir}"
    tensorboard_dir: "${job.base_output_dir}/tensorboard"

  torchrun_args:
    rdzv_backend: c10d