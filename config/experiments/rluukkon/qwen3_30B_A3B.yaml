# @package _global_
defaults:
  - base
  - /backend: megatron
  - /container: lumi
  - /slurm: lumi
  - /monitoring: megatron_basic
  - /project: default
  - /sweep: none
  - /backend/megatron: base
  - _self_

project:
  name: moe-exploration
  resume: true
  
slurm:
  sbatch:
    partition: dev-g
    nodes: 16
    time: 0-00:30:00
backend:
  megatron:
    log_throughput: true
    log_interval: 1
    log_progress: true

    # MODEL
    num_layers: 48
    num_attention_heads: 32
    num_query_groups: 4 # no gqa when NUM_QUERY_GROUPS==NUM_ATTENTION_HEADS TODO: check
    group_query_attention: True 
    hidden_size: 2048
    ffn_hidden_size: 6144 # not used in the current MoE 
    moe_ffn_hidden_size: 768 
    kv_channels: 128
    num_experts: 128
    moe_router_topk: 8
    moe_router_dtype: "fp32"
    moe_grouped_gemm: True 
    moe_aux_loss_coeff: 0.01
    init_method_std: 0.02
    position_embedding_type: "rope"
    rotary_base: 1000000
    rotary_percent: 1.0
    attention_dropout: 0.0
    hidden_dropout: 0.0
    normalization: RMSNorm
    norm_epsilon: 1.e-6 # rmsnorm epsilon
    qk_layernorm: true # potentially enable qk-norm
    bf16: True
    swiglu: True
    untie_embeddings_and_output_weights: True
    use_flash_attn: True
    attention_softmax_in_fp32: True

    # Data configuration
    seq_length: 4096
    max_position_embeddings: ${.seq_length}
    num_workers: 2
    split: '99,1,0'
    data_path:
      - ${oc.env:DATA_DIR}/gpt-neox-20b/nemotron-cc/1.0/high-actual
      - ${oc.env:DATA_DIR}/gpt-neox-20b/nemotron-cc/1.0/high-synthetic-distill
      - ${oc.env:DATA_DIR}/gpt-neox-20b/nemotron-cc/1.0/high-synthetic-diverse_qa_pairs
      - ${oc.env:DATA_DIR}/gpt-neox-20b/nemotron-cc/1.0/high-synthetic-extract_knowledge
      - ${oc.env:DATA_DIR}/gpt-neox-20b/nemotron-cc/1.0/high-synthetic-knowledge_list
      - ${oc.env:DATA_DIR}/gpt-neox-20b/nemotron-cc/1.0/high-synthetic-wrap_medium
    data_cache_path: ${oc.env:WORK}/.cache
    # Tokenizer configuration
    tokenizer_model: EleutherAI/gpt-neox-20b
    tokenizer_type: HuggingFaceTokenizer

    micro_batch_size: 1
    global_batch_size: 2048
    
    # MEMORY OPTIMIZATIONS
    recompute_activations: False
    use_torch_fsdp2: False # it's either this or the next 4 args
    tensor_model_parallel_size: 1
    pipeline_model_parallel_size: 4
    expert_model_parallel_size: 4
    context_parallel_size: 2
    sequence_parallel: True
    use_distributed_optimizer: True
    moe_token_dispatcher_type: "allgather"
    
    # COMM OPTIONS
    distributed_timeout_minutes: 120
    overlap_param_gather: True
    overlap_grad_reduce: True
    gradient_accumulation_fusion: False
    overlap_moe_expert_parallel_comm: True
    disable_bias_linear: true
    bias_swiglu_fusion: False
    add_bias_linear: False
    bias_dropout_fusion: False

    # OPTIMIZER
    adam_beta1: 0.9
    adam_beta2: 0.95
    adam_eps: 1e-8
    lr: 3e-4
    min_lr: 0.0
    clip_grad: 1.0
    weight_decay: 0.05
    lr_decay_style: WSD
    lr_wsd_decay_style: linear
    
    # training schedule. Round numbers to avoid floating point issues
    train_iters: ${oc.eval:'-(-4_000_000_000 // (${.global_batch_size} * ${.seq_length}))'}
    lr_warmup_iters: ${oc.eval:'min(${.train_iters} // 10, 25000)'}
    lr_wsd_decay_iters: ${oc.eval:'${.train_iters} * 0.2'}
    lr_decay_iters: ${.train_iters}
    ckpt_format: torch_dist
    save_interval: 2
    exit_interval: 3
    save: ${project.base_output_dir}
    load: ${project.base_output_dir}
    tensorboard_dir: ${project.base_output_dir}/tensorboard
    wandb: false 
    wandb_save_dir: "${project.base_output_dir}/wandb"
    wandb_project: "${project.name}"
    wandb_exp_name: "qwen3_30B_A3B_$SLURM_JOB_ID"



