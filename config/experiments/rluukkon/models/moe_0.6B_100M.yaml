# @package backend.megatron

# MODEL
num_layers: 7
hidden_size: 768
ffn_hidden_size: 3584 
kv_channels: 64
swiglu: True

# Attention
num_attention_heads: 16
num_query_groups: 16 # no gqa when NUM_QUERY_GROUPS==NUM_ATTENTION_HEADS TODO: check
group_query_attention: False
attention_softmax_in_fp32: True
use_flash_attn: True

# Position Embedding
add_position_embedding: False
position_embedding_type: "rope"
rotary_base: 1000000
rotary_percent: 1.0
untie_embeddings_and_output_weights: False

# Normalization
normalization: RMSNorm
norm_epsilon: 1.e-6 # rmsnorm epsilon
qk_layernorm: true # enable qk-norm in attention
bf16: True

# Disable dropout
attention_dropout: 0.0
hidden_dropout: 0.0
add_bias_linear: False

# Mixture of Experts (MoE) specific parameters
moe_ffn_hidden_size: 512
num_experts: 64
moe_router_topk: 8
moe_router_dtype: "fp32"
moe_grouped_gemm: True 
moe_aux_loss_coeff: 0.001
moe_token_dispatcher_type: "allgather"

init_method_std: 0.02


# OPTIMIZER
adam_beta1: 0.9
adam_beta2: 0.95
adam_eps: 1e-8
lr: 3e-4 # from Flame-Moe
min_lr: 0.0
clip_grad: 1.0
weight_decay: 0.05
lr_decay_style: WSD
lr_wsd_decay_style: linear


# MEMORY OPTIMIZATIONS
recompute_activations: False
use_torch_fsdp2: False # it's either this or the next 4 args
tensor_model_parallel_size: 1
pipeline_model_parallel_size: 1
expert_model_parallel_size: 1
context_parallel_size: 1
sequence_parallel: True
use_distributed_optimizer: True

# COMM OPTIONS
distributed_timeout_minutes: 120
overlap_param_gather: False
overlap_grad_reduce: True
gradient_accumulation_fusion: False
overlap_moe_expert_parallel_comm: True
bias_swiglu_fusion: False
bias_dropout_fusion: False

# DEBUG
# moe_router_force_load_balancing: True
# mock_data: True


ckpt_format: torch_dist

save_interval: 8000