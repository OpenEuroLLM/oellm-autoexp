# @package _global_
defaults:
  - base
  - /backend: megatron
  - /container: lumi
  - /slurm: lumi
  - /monitoring: megatron_basic
  - /project: default
  - /sweep: none

  - /experiments/rluukkon/models: moe_2B_400M
  - /experiments/rluukkon/data:   moe_ablation_data

  - _self_

project:
  name: moe-exploration
  resume: true
  
slurm:
  sbatch:
    partition: dev-g
    nodes: 4
    time: 0-00:10:00

backend:
  megatron:
    log_throughput: true
    log_interval: 1
    log_progress: true
   
    # MODEL OVERRIDES
    moe_layer_freq: "[0]*1+[1]*${oc.eval:'${.num_layers} - 1'}" # first layer dense, rest moe 

    # training schedule. Round numbers to avoid floating point issues
    # set num_tokens to the formula
    train_iters: ${oc.eval:'-(-30_000_000_000 // (${.global_batch_size} * ${.seq_length}))'}
    lr_warmup_iters: ${oc.eval:'min(${.train_iters} // 10, 25000)'}
    lr_wsd_decay_iters: ${oc.eval:'${.train_iters} * 0.2'}
    lr_decay_iters: ${.train_iters}
    ckpt_format: torch_dist
    save_interval: 1000
    # exit_interval: 10000
    save: ${project.base_output_dir}
    load: ${project.base_output_dir}
    tensorboard_dir: ${project.base_output_dir}/tensorboard
    wandb: false 
    wandb_save_dir: "${project.base_output_dir}/wandb"
    wandb_project: "${project.name}"
    wandb_exp_name: "2B_A300M_dense_sparse_lr${.lr}_gbsz${.global_batch_size}_$SLURM_JOB_ID"



