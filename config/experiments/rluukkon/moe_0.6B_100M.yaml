# @package _global_
defaults:
  - base
  - /backend: megatron_torchrun
  - /container: lumi
  - /slurm: lumi
  - /monitoring: megatron_basic
  - /project: default
  - /sweep: none
  - /experiments/rluukkon/models: moe_0.6B_100M
  - /experiments/rluukkon/data:   moe_ablation_data
  - _self_

project:
  name: moe-exploration
  resume: true
  
slurm:
  sbatch:
    partition: dev-g
    nodes: 1
    time: 0-00:45:00

backend:
  megatron:
    log_throughput: true
    log_interval: 10
    log_progress: true


    # training schedule. Round numbers to avoid floating point issues
    # set num_tokens to the formula
    train_iters: ${oc.eval:'-(-30_000_000_000 // (${.global_batch_size} * ${.seq_length}))'}
    lr_warmup_iters: ${oc.eval:'min(${.train_iters} // 10, 25000)'}
    lr_wsd_decay_iters: ${oc.eval:'${.train_iters} * 0.2'}
    lr_decay_iters: ${.train_iters}
    # exit_interval: 10000
    save: ${project.base_output_dir}
    load: ${project.base_output_dir}
    tensorboard_dir: ${project.base_output_dir}/tensorboard
    wandb: false 
    wandb_save_dir: "${project.base_output_dir}/wandb"
    wandb_project: "${project.name}"
    wandb_exp_name: "qwen3_2B_A400M_lr${.lr}_gbsz${.global_batch_size}_$SLURM_JOB_ID"



