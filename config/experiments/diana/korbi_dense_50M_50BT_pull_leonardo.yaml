# @package _global_

# 50M dense model training with pull-based multi-stage cooldown
# Translation of dense_50M_50BT.yaml to new composable sweep system

defaults:
  - /backend: megatron_torchrun
  - /container: leonardo
  - /slurm: leonardo
  - /monitoring: megatron_basic
  - /project: default
  - /sweep: none
  - /backend/megatron: base
  - _self_

index: 0
stage: ""

project:
  base_output_dir: "dense_50M_sweep"
  name: "dense_50M_01_lr\\${backend.megatron.lr}_gbsz\\${backend.megatron.global_batch_size}_beta2\\${backend.megatron.adam_beta2}_\\${stage}"
  resume: true

container:
  image: /leonardo_work/OELLM_prod2026/container_images/pytorch-25.08.sif
  bind: 
    - "/leonardo_work/OELLM_prod2026/"
    - "/leonardo_scratch"
    - "/leonardo"

backend:
  megatron:
    # wandb_exp_name: "\\${project.name}"
    # wandb_project: oellm-train

    # DATA
    data_path: [
      /leonardo_work/OELLM_prod2026/datasets/Nemotron-cc-2024-HQ-real-synth-mix/GPT-NeoX/merged_0,
      /leonardo_work/OELLM_prod2026/datasets/Nemotron-cc-2024-HQ-real-synth-mix/GPT-NeoX/merged_1,
      /leonardo_work/OELLM_prod2026/datasets/Nemotron-cc-2024-HQ-real-synth-mix/GPT-NeoX/merged_2,
      /leonardo_work/OELLM_prod2026/datasets/Nemotron-cc-2024-HQ-real-synth-mix/GPT-NeoX/merged_3,
      /leonardo_work/OELLM_prod2026/datasets/Nemotron-cc-2024-HQ-real-synth-mix/GPT-NeoX/merged_4,
      /leonardo_work/OELLM_prod2026/datasets/Nemotron-cc-2024-HQ-real-synth-mix/GPT-NeoX/merged_5  
    ]
    data_cache_path: /leonardo_scratch/large/userexternal/donutu00/MEGATRON_CACHEDIR
    seq_length: 4096
    max_position_embeddings: 4096
    vocab_file: /leonardo_work/OELLM_prod2026/models/EleutherAI/gpt-neox-20b/vocab.json
    merge_file: /leonardo_work/OELLM_prod2026/models/EleutherAI/gpt-neox-20b/merges.txt
    num_workers: 8
    split: '99,1,0'

    # MODEL: 50M
    num_layers: 12
    hidden_size: 384
    ffn_hidden_size: 1536
    num_attention_heads: 6
    num_query_groups: ${.num_attention_heads}
    init_method_std: 0.02
    position_embedding_type: "rope"
    rotary_base: 10000
    rotary_percent: 1.0
    attention_dropout: 0.0
    hidden_dropout: 0.0
    normalization: RMSNorm
    norm_epsilon: 1.e-5
    qk_layernorm: True
    bf16: True
    swiglu: True
    untie_embeddings_and_output_weights: False
    use_flash_attn: True
    attention_softmax_in_fp32: False

    # TRAINING, PARALLELISM
    recompute_activations: False
    use_torch_fsdp2: False
    tensor_model_parallel_size: 1
    pipeline_model_parallel_size: 1
    context_parallel_size: 1
    sequence_parallel: True

    distributed_backend: nccl
    distributed_timeout_minutes: 10

    # DISTRIBUTED OPTIM
    use_distributed_optimizer: True

    # OPTIMIZER
    optimizer: adam
    adam_beta1: 0.9
    adam_beta2: 0.99
    adam_eps: 1.e-8
    clip_grad: 1.0
    weight_decay: 0.1

    # SCHEDULER
    min_lr: 1.e-05
    lr_decay_style: WSD
    lr_wsd_decay_style: linear
    lr_warmup_iters: 2000

    # PROFILE
    profile: False
    use_pytorch_profiler: False
    profile_ranks: [0]
    profile_step_start: 5
    profile_step_end: 12

    # EVALS
    eval_interval: 3200
    eval_iters: 100

    # LOGGING
    log_interval: 10
    log_progress: True
    log_throughput: True
    tensorboard_queue_size: 5

    # CHECKPOINT FORMAT
    ckpt_format: "torch"
    save_interval: 8000

    # Base training configuration (for stable phase)
    micro_batch_size: 8
    global_batch_size: 64  # Will be overridden by sweep
    lr: 1e-4  # Will be overridden by sweep

    # Auxiliary computation variables
    # Note: Computed fields (train_iters, lr_wsd_decay_iters, start_iter, start_iter_round)
    # are defined in the sweep defaults alongside the tokens they depend on,
    # so they get resolved together with stage-specific values
    aux:
      tokens: 50_000_000_000  # Default: 50B tokens
      decay_fraction: 0.0  # Default: no decay
      start_iter: 0  # only used in decay
      start_iter_round: 0 # only used in decay

slurm:
  sbatch:
    nodes: "${oc.eval:${backend.megatron.global_batch_size}//\\(${backend.megatron.micro_batch_size}*${slurm.sbatch.gpus_per_node}\\)}"


# COMPOSABLE SWEEP with PULL-BASED MULTI-STAGE
sweep:
  # Outer level: product (combines hyperparameters × stages)
  type: product

  groups:
    - type: list
      configs:
        - project.name: "dense_50M_01_lr\\${backend.megatron.lr}_gbsz\\${backend.megatron.global_batch_size}_beta2\\${backend.megatron.adam_beta2}_\\${stage}"
          # backend.megatron.wandb_exp_name: "\\${project.name}"
          project.base_output_dir: "${project.base_output_dir}/\\${project.name}"
    
# GROUP 1: Hyperparameter configurations (list of two product grids)
    - type: list
      configs:
        # Grid 1: 6 configurations (3 lr × 2 batch sizes)
        - type: product
          params:
            backend.megatron.lr: [2.5e-4, 5.e-4, 1.e-3]
            backend.megatron.global_batch_size: [64, 128]

        # Grid 2: 9 configurations (3 lr × 3 batch sizes)
        - type: product
          params:
            backend.megatron.lr: [5.e-4, 1.e-3, 2.e-3]
            backend.megatron.global_batch_size: [256, 512, 1024]

    # GROUP 2: Multi-stage training (stable + cooldown/decay)
    # Uses a nested structure with defaults for decay stages
    - type: list
      configs:
        # STABLE PHASE: Train for 50B tokens with no decay
        - stage: stable

          backend.megatron.aux.tokens: 50_000_000_000
          # Computed fields using ESCAPED OmegaConf interpolations (\\$ prevents early resolution)
          # These will be unescaped during sweep expansion and resolved with stage-specific values
          backend.megatron.train_iters: "\\${oc.eval:\\(\\${backend.megatron.aux.tokens}+\\${backend.megatron.seq_length}*\\${backend.megatron.global_batch_size}-1\\)//\\(\\${backend.megatron.seq_length}*\\${backend.megatron.global_batch_size}\\)}"
          backend.megatron.lr_wsd_decay_iters: 0  # No decay in stable phase
          backend.megatron.aux.start_iter: "\\${oc.eval:int\\(\\(\\(\\${backend.megatron.aux.tokens}+\\${backend.megatron.seq_length}*\\${backend.megatron.global_batch_size}-1\\)//\\(\\${backend.megatron.seq_length}*\\${backend.megatron.global_batch_size}\\)\\)*\\(1-\\${backend.megatron.aux.decay_fraction}\\)\\)}"
          backend.megatron.aux.start_iter_round: "\\${oc.eval:int\\(\\${backend.megatron.aux.start_iter}//\\${backend.megatron.save_interval}\\)*\\${backend.megatron.save_interval}}"

        # NESTED GROUP: Cooldown/decay stages with shared defaults
        # All decay stages share: 20% decay, checkpoint loading, start/cancel conditions
        - type: list
          defaults:
            # Common decay configuration
            backend.megatron.aux.decay_fraction: 0.2

            # Computed fields using ESCAPED OmegaConf interpolations (\\$ prevents early resolution)
            # These will be unescaped during sweep expansion and resolved with stage-specific values (6B, 12B, 30B, 50B)
            # Each computation is self-contained (doesn't reference other computed values) to avoid ordering issues
            backend.megatron.train_iters: "\\${oc.eval:\\(\\${backend.megatron.aux.tokens}+\\${backend.megatron.seq_length}*\\${backend.megatron.global_batch_size}-1\\)//\\(\\${backend.megatron.seq_length}*\\${backend.megatron.global_batch_size}\\)}"
            backend.megatron.lr_wsd_decay_iters: "\\${oc.eval:int\\(\\(\\(\\${backend.megatron.aux.tokens}+\\${backend.megatron.seq_length}*\\${backend.megatron.global_batch_size}-1\\)//\\(\\${backend.megatron.seq_length}*\\${backend.megatron.global_batch_size}\\)\\)*\\${backend.megatron.aux.decay_fraction}\\)}"
            backend.megatron.aux.start_iter: "\\${oc.eval:int\\(\\(\\(\\${backend.megatron.aux.tokens}+\\${backend.megatron.seq_length}*\\${backend.megatron.global_batch_size}-1\\)//\\(\\${backend.megatron.seq_length}*\\${backend.megatron.global_batch_size}\\)\\)*\\(1-\\${backend.megatron.aux.decay_fraction}\\)\\)}"
            backend.megatron.aux.start_iter_round: "\\${oc.eval:int\\(\\${backend.megatron.aux.start_iter}//\\${backend.megatron.save_interval}\\)*\\${backend.megatron.save_interval}}"

            # Checkpoint loading configuration
            backend.megatron.load: "\\${sibling.stable.project.base_output_dir}/checkpoints/"
            backend.megatron.ckpt_step: "\\${backend.megatron.aux.start_iter_round}"
            backend.megatron.override_opt_param_scheduler: true

            # Start conditions: wait for stable to reach checkpoint (new async approach)
            job.start_conditions:
              - class_name: FileExistsCondition
                path: "\\${sibling.stable.project.base_output_dir}/checkpoints/iter_\\${backend.megatron.aux.start_iter_round}/latest_checkpointed_iteration.txt"
                blocking: true
                timeout_seconds: 7200

            # Cancel conditions: cancel if stable job fails
            job.cancel_conditions:
              - class_name: SlurmStateCondition
                job_name: "\\${sibling.stable.project.name}"
                state: FAILED
              - class_name: LogPatternCondition
                log_path: "\\${sibling.stable.project.log_path_current}"
                pattern: "FATAL ERROR|OutOfMemoryError|Traceback"
          configs:
            # Each decay stage only needs to specify its unique stage and tokens
            - stage: decay6B
              backend.megatron.aux.tokens: 6_000_000_000
            - stage: decay12B
              backend.megatron.aux.tokens: 12_000_000_000
            - stage: decay30B
              backend.megatron.aux.tokens: 30_000_000_000
            - stage: decay50B
              backend.megatron.aux.tokens: 50_000_000_000


# Summary of what this creates:
# - GROUP 1: 15 hyperparameter configurations
#   - Grid 1: 6 configs (3 lr × 2 batch sizes)
#   - Grid 2: 9 configs (3 lr × 3 batch sizes)
#   - Union (list mode): 6 + 9 = 15 configs
# - GROUP 2: 5 stages per config
#   - Stable: 1 config
#   - Nested group with defaults: 4 decay stages
#     - Common config in 'defaults' (20% decay, checkpoint loading, conditions)
#     - Each stage overrides only stage and tokens
#   - Total: 1 + 4 = 5 stages
# - Outer product: 15 hyperparameter configs × 5 stages = 75 total jobs
#
# Benefits of using group-level defaults:
# 1. DRY (Don't Repeat Yourself) - common config defined once
# 2. Easy to modify - change decay_fraction, start_conditions, etc. in one place
# 3. Clear structure - defaults vs. stage-specific parameters
# 4. No exotic YAML features (vs. anchors) - more maintainable
#
# OmegaConf interpolation strategy:
# 1. Base config (backend.megatron.aux) defines interpolations for start_iter, start_iter_round
# 2. Defaults override primitive values (decay_fraction, tokens)
# 3. Interpolations in base config automatically pick up overridden values
# 4. This ensures correct evaluation order: tokens → train_iters → start_iter → start_iter_round
#
# Pull-based advantages over push-based:
# 1. All jobs planned upfront - full visibility
# 2. No dynamic job creation during monitoring
# 3. Cooldown jobs wait for checkpoints (start_conditions with FileExistsCondition)
# 4. Automatic cancellation if stable job fails (cancel_conditions)
# 5. Uses stable log symlink for reliable monitoring
# 6. Unified condition system (same conditions for start gating and failure detection)
