# @package _global_

# 50M dense model training with pull-based multi-stage cooldown
# Translation of dense_50M_50BT.yaml to new composable sweep system

defaults:
  - /backend: megatron_torchrun
  - /container: leonardo
  - /slurm: leonardo
  - /job: default
  - /sweep: none
  - /backend/megatron: base
  - _self_

index: 0
stage: ""

job:
  name: "dense_50M_01_lr${backend.megatron.lr}_gbsz${backend.megatron.global_batch_size}_beta2${backend.megatron.adam_beta2}_${stage}"
  base_output_dir: "dense_50M_50BT/${job.name}"
  # resume: true

backend:
  megatron:
    wandb_exp_name: "${job.name}"
    wandb_project: sanity-checks
    wandb_entity: openeurollm-project
    wandb_save_dir: "${job.base_output_dir}"
    tensorboard_dir: "${job.base_output_dir}/tensorboard"

    # DATA
    data_path: [
      /leonardo_work/OELLM_prod2026/preprocessed/gpt-neox-20b/nemotron-cc/1.0/high-actual,
      /leonardo_work/OELLM_prod2026/preprocessed/gpt-neox-20b/nemotron-cc/1.0/high-synthetic-distill,
      /leonardo_work/OELLM_prod2026/preprocessed/gpt-neox-20b/nemotron-cc/1.0/high-synthetic-diverse_qa_pairs,
      /leonardo_work/OELLM_prod2026/preprocessed/gpt-neox-20b/nemotron-cc/1.0/high-synthetic-extract_knowledge,
      /leonardo_work/OELLM_prod2026/preprocessed/gpt-neox-20b/nemotron-cc/1.0/high-synthetic-knowledge_list,
      /leonardo_work/OELLM_prod2026/preprocessed/gpt-neox-20b/nemotron-cc/1.0/high-synthetic-wrap_medium
    ]
    data_cache_path: /leonardo_scratch/large/userexternal/donutu00/MEGATRON_CACHEDIR
    seq_length: 4096
    max_position_embeddings: 4096
    vocab_file: /leonardo_work/OELLM_prod2026/models/EleutherAI/gpt-neox-20b/vocab.json
    merge_file: /leonardo_work/OELLM_prod2026/models/EleutherAI/gpt-neox-20b/merges.txt
    num_workers: 8
    split: '99,1,0'
    legacy_tokenizer: True

    # MODEL: 50M
    num_layers: 12
    hidden_size: 384
    ffn_hidden_size: 1536
    num_attention_heads: 6
    num_query_groups: ${.num_attention_heads}
    init_method_std: 0.02
    position_embedding_type: "rope"
    rotary_base: 10000
    rotary_percent: 1.0
    attention_dropout: 0.0
    hidden_dropout: 0.0
    normalization: RMSNorm
    norm_epsilon: 1.e-5
    qk_layernorm: True
    bf16: True
    swiglu: True
    untie_embeddings_and_output_weights: False
    use_flash_attn: True
    attention_softmax_in_fp32: False

    # TRAINING, PARALLELISM
    recompute_activations: False
    use_torch_fsdp2: False
    tensor_model_parallel_size: 1
    pipeline_model_parallel_size: 1
    context_parallel_size: 1
    sequence_parallel: True

    distributed_backend: nccl
    distributed_timeout_minutes: 10

    # DISTRIBUTED OPTIM
    use_distributed_optimizer: True

    # OPTIMIZER
    optimizer: adam
    adam_beta1: 0.9
    adam_beta2: 0.99
    adam_eps: 1.e-8
    clip_grad: 1.0
    weight_decay: 0.1

    # SCHEDULER
    min_lr: 1.e-05
    lr_decay_style: WSD
    lr_wsd_decay_style: linear
    lr_warmup_iters: 2000

    # PROFILE
    profile: False
    use_pytorch_profiler: False
    profile_ranks: [0]
    profile_step_start: 5
    profile_step_end: 12

    # EVALS
    eval_interval: 800
    eval_iters: 100

    # LOGGING
    log_interval: 10
    log_progress: True
    log_throughput: True
    tensorboard_queue_size: 5
    log_timers_to_tensorboard: True

    # CHECKPOINT FORMAT
    ckpt_format: "torch_dist"
    save_interval: 2000
    save: ${job.base_output_dir}/checkpoints

    # Base training configuration (for stable phase)
    micro_batch_size: 16
    global_batch_size: 256  # Will be overridden by sweep
    lr: 1.e-3  # Will be overridden by sweep

    # Auxiliary computation variables
    # Note: Computed fields (train_iters, lr_wsd_decay_iters, start_iter, start_iter_round)
    # are defined in the sweep defaults alongside the tokens they depend on,
    # so they get resolved together with stage-specific values
    aux:
      tokens: 50_000_000_000  # Default: 50B tokens
      decay_fraction: 0.0  # Default: no decay
      start_iter: 0  # only used in decay
      start_iter_round: 0 # only used in decay

slurm:
  sbatch:
    nodes: "${oc.eval:'int(${backend.megatron.global_batch_size}//(${backend.megatron.micro_batch_size}*${slurm.sbatch.gpus_per_node}))'}"
    # qos: boost_qos_dbg

# COMPOSABLE SWEEP with PULL-BASED MULTI-STAGE
sweep:
  type: product

  groups:
    - type: list
      configs:
        - job.name: "dense_50M_01_lr\\${backend.megatron.lr}_gbsz\\${backend.megatron.global_batch_size}_beta2\\${backend.megatron.adam_beta2}_\\${stage}"
          backend.megatron.wandb_exp_name: "\\${job.name}"
    
    # Multi-stage training (stable + cooldown/decay)
    # Uses a nested structure with defaults for decay stages
    - type: list
      configs:
        # STABLE PHASE: Train for 50B tokens with no decay
        - stage: stable

          backend.megatron.aux.tokens: 50_000_000_000
          # Computed fields using ESCAPED OmegaConf interpolations (\\$ prevents early resolution)
          # These will be unescaped during sweep expansion and resolved with stage-specific values
          backend.megatron.train_iters: "\\${oc.eval:'(\\${backend.megatron.aux.tokens}+\\${backend.megatron.seq_length}*\\${backend.megatron.global_batch_size}-1)//(\\${backend.megatron.seq_length}*\\${backend.megatron.global_batch_size})'}"
          backend.megatron.lr_wsd_decay_iters: 0  # No decay in stable phase
          backend.megatron.aux.start_iter: "\\${oc.eval:'int(((\\${backend.megatron.aux.tokens}+\\${backend.megatron.seq_length}*\\${backend.megatron.global_batch_size}-1)//(\\${backend.megatron.seq_length}*\\${backend.megatron.global_batch_size}))*(1-\\${backend.megatron.aux.decay_fraction}))'}"
          backend.megatron.aux.start_iter_round: "\\${oc.eval:'int(\\${backend.megatron.aux.start_iter}//\\${backend.megatron.save_interval})*\\${backend.megatron.save_interval}'}"

        # NESTED GROUP: Cooldown/decay stages with shared defaults
        # All decay stages share: 20% decay, checkpoint loading, start/cancel conditions
        - type: list
          defaults:
            # Common decay configuration
            backend.megatron.aux.decay_fraction: 0.2

            # Computed fields using ESCAPED OmegaConf interpolations (\\$ prevents early resolution)
            # These will be unescaped during sweep expansion and resolved with stage-specific values (6B, 12B, 30B, 50B)
            # Each computation is self-contained (doesn't reference other computed values) to avoid ordering issues
            backend.megatron.train_iters: "\\${oc.eval:'(\\${backend.megatron.aux.tokens}+\\${backend.megatron.seq_length}*\\${backend.megatron.global_batch_size}-1)//(\\${backend.megatron.seq_length}*\\${backend.megatron.global_batch_size})'}"
            backend.megatron.lr_wsd_decay_iters: "\\${oc.eval:'int(((\\${backend.megatron.aux.tokens}+\\${backend.megatron.seq_length}*\\${backend.megatron.global_batch_size}-1)//(\\${backend.megatron.seq_length}*\\${backend.megatron.global_batch_size}))*\\${backend.megatron.aux.decay_fraction})'}"
            backend.megatron.aux.start_iter: "\\${oc.eval:'int(((\\${backend.megatron.aux.tokens}+\\${backend.megatron.seq_length}*\\${backend.megatron.global_batch_size}-1)//(\\${backend.megatron.seq_length}*\\${backend.megatron.global_batch_size}))*(1-\\${backend.megatron.aux.decay_fraction}))'}"
            backend.megatron.aux.start_iter_round: "\\${oc.eval:'int(\\${backend.megatron.aux.start_iter}//\\${backend.megatron.save_interval})*\\${backend.megatron.save_interval}'}"

            # Checkpoint loading configuration
            backend.megatron.load: "\\${sibling.stable.job.base_output_dir}/checkpoints"
            backend.megatron.ckpt_step: "\\${backend.megatron.aux.start_iter_round}"
            backend.megatron.override_opt_param_scheduler: true

            # Start conditions: wait for stable to reach checkpoint (new async approach)
            job.start_condition:
              class_name: FileExistsCondition
              # path: "\\${sibling.stable.job.base_output_dir}/checkpoints/iter_\\${backend.megatron.aux.start_iter_round}"
              path: "\\${sibling.stable.job.base_output_dir}/checkpoints/iter_\\${oc.eval:'str(\\${backend.megatron.aux.start_iter_round}).zfill(7)'}"

            # Cancel conditions: cancel if stable job fails
            job.cancel_condition:
              class_name: LogPatternCondition
              log_path: "\\${sibling.stable.job.log_path_current}"
              pattern: "FATAL ERROR|OutOfMemoryError|Traceback"
          configs:
            # Each decay stage only needs to specify its unique stage and tokens
            - stage: decay50B
              backend.megatron.aux.tokens: 50_000_000_000
