accumulate_allreduce_grads_in_fp32: false  # Gradient accumulation and all-reduce in fp32.
adam_beta1: 0.9  # First coefficient for computing running averages of gradient and its square
adam_beta2: 0.999  # Second coefficient for computing running averages of gradient and its square
adam_eps: 1.0e-08  # Term added to the denominator to improvenumerical stability
add_bias_linear: true  # Disable bias in the linear layers
add_position_embedding: true  # Disable position embedding. Deprecated: use --position-embedding-type
add_qkv_bias: false  # Enable bias only in the QKV linear layers
adlr_autoresume: false  # Enable autoresume on adlr cluster.
adlr_autoresume_interval: 1000  # Intervals over which check for autoresumetermination signal
align_grad_reduce: true  # If not set, all PP stages will launch gradient reduces simultaneously. Otherwise, each PP stage will independently launch as needed.
align_param_gather: true  # If not set, all PP stages will launch param all-gathers simultaneously. Otherwise, each PP stage will independently launch as needed.
app_tag_run_name: null  # Jobs belonging to same training run, suppose to have the same name. It will be used to track progress of a training done over multiple different jobs
app_tag_run_version: 0.0.0  # The version of the training of which current job is part of. It will be used to track the changes in the application side which might change the performance baseline
apply_layernorm_1p: false  # Adjust LayerNorm weights such that they are centered around zero. This improves numerical stability.
apply_query_key_layer_scaling: false  # Scale Q * K^T by 1 / layer-number. Useful for fp16 training. Also sets `attention_softmax_in_fp32` to True.
apply_residual_connection_post_layernorm: false  # If set, use original BERT residula connection ordering.
apply_rope_fusion: true  # Disable rope fusion, the fusion is available only when using megatron-core.
async_save: null  # Apply async checkpointing save. Currently works only with`torch_dist` distributed checkpoint format.
async_tensor_model_parallel_allreduce: true  # DEPRECATED. This flag is ignored.
attention_backend: auto  # choices: [1, 2, 3, 4, 5] | Attention backend to use (flash,fused,unfused,local,auto). Defaults to auto
attention_dropout: 0.1  # Post attention dropout probability.
attention_sink_k: 0  # k attention sink tokens.
attention_softmax_in_fp32: false  # Run attention masking and softmax in fp32.
auto_detect_ckpt_format: false  # Determine if the checkpoint format is in legacy or distributed format. If False, expects distributed checkpoint iff args.ckpt_format != "torch". Might slow down loading a bit (double rank0 ckpt load).
barrier_with_L1_time: true  # If not set, use barrier with level 1 time measurements. Note that this is up to the user to make sure calling barrier with their timers will not result in hangs. This can happen if for example the user adds a level 1 timer that is not called by all ranks.
batch_size: null  # Old batch size parameter, do not use. Use --micro-batch-size instead
bert_binary_head: true  # Disable BERT binary head.
bert_embedder_type: megatron  # choices: ['megatron', 'huggingface'] | Select either Megatron or Huggingface as the Bert embedder.
bert_load: null  # Directory containing an BertModel checkpoint (needed to start ICT and REALM)
bf16: false  # Run model in bfloat16 mode.
bias_dropout_fusion: true  # Disable bias and dropout fusion.
bias_gelu_fusion: true  # Disable bias and gelu fusion.
bias_swiglu_fusion: true  # Disable bias and swiglu fusion, the fusion is available only when using megatron-core.
biencoder_projection_dim: 0  # Size of projection head used in biencoder (paper default: 128)
biencoder_shared_query_context_model: false  # Whether to share the parameters of the query and context models or not
block_data_path: null  # Where to save/load BlockData to/from
calculate_per_token_loss: false  # Scale cross entropy loss by the number of non-padded tokens in the global batch, versus the default behavior of assuming all tokens are non-padded.
check_for_nan_in_loss_and_grad: true  # Check for NaNs in loss and grad
check_for_spiky_loss: false  # Check for spiky loss
check_weight_hash_across_dp_replicas_interval: null  # Interval to check weight hashes are same across DP replicas. If not specified, weight hashes not checked.
checkpoint_activations: false  # Checkpoint activation to allow for training with larger models, sequences, and batch sizes.
ckpt_assume_constant_structure: false  # If the model and optimizer state dict structure isconstant throughout a *single training job*, it allows fordifferent checkpointing performance optimizations.
ckpt_convert_format: null  # choices: ['torch', 'torch_dist', 'zarr'] | Checkpoint format for conversion.
ckpt_convert_save: null  # Save directory for converted checkpoint.
ckpt_convert_update_legacy_dist_opt_format: false  # When loading a checkpoint, update the legacy format for the distributed optimizer, which previously used a merged param/grad buffer and a different bucket mapping. The legacy format was deprecated on Feb 13, 2024.
ckpt_format: torch_dist  # choices: ['torch', 'torch_dist', 'zarr'] | Checkpoint format to use.
ckpt_fully_parallel_load: false  # Apply full load parallelization across DP for distributed checkpoints.
ckpt_fully_parallel_save: true  # Disable applying full save parallelization across DP for distributed checkpoints. Depending on ckpt format might decrease the number of files in the checkpoint. Makes DistributedOptimizer checkpoint non-reshardable.
ckpt_fully_parallel_save_deprecated: false  # Deprecated: see --no-ckpt-fully-parallel-save.
ckpt_step: null  # Checkpoint step to load model from.
classes_fraction: 1.0  # training with fraction of classes.
clip_grad: 1.0  # Gradient clipping based on global L2 norm.
clone_scatter_output_in_embedding: true  # If not set, clone the output of the scatter in embedding layer to GC original tensor.
config_logger_dir: ''  # If set, will dump all configs to --config-logger-dir
context_parallel_size: 1  # Degree of context parallelism.
cp_comm_type:  # Inter-gpu communication type for context parallelism: p2p, a2a, allgather or a2a+p2p. If a single string is provided, all layers will share the same communication type. Users can also specify separated types for each layer like --cp-comm-type p2p p2p a2a a2a a2a+p2p a2a+p2p
- p2p
create_attention_mask_in_dataloader: true  # If set, do not create attention_masks in dataloader.
cross_entropy_loss_fusion: false  # Enabled fusion of cross entropy loss calculation.
data_args_path: null  # Path to data-args. Instead of feeding `--data-path` with weighted dataset, we pass in a file path from which we read that argument. This is useful when the list of data is too big.
data_cache_path: null  # Path to a directory to hold cached index files.
data_parallel_random_init: false  # Enable random initialization of params across data parallel ranks
data_path: null  # The weight and prefix list for a set of train, validation, and testdatasets which split according to --split. The accepted formats are: (1) a single prefix, (2) a list of weight prefix pairs e.g. weight1 prefix1 weight2 prefix2, (3) a list of prefixes e.g. prefix1 prefix2. For (3), weights are inferred from the lengths of the contributing datasets. This argument is exclusive to the other independent --*-data-path arguments.
data_per_class_fraction: 1.0  # training with fraction of data per class.
data_sharding: true  # Disable data sharding.
dataloader_type: null  # choices: ['single', 'cyclic', 'external'] | Single pass vs multiple pass data loader
ddp_average_in_collective: false  # If set, average directly in data-parallel communication collective.
ddp_bucket_size: null  # Bucket size for data-parallel communication
decoder_first_pipeline_num_layers: null  # The number of transformer layers on the first pipeline stage of the decoder. Default None is even split of transformer layers across all pipeline stages
decoder_last_pipeline_num_layers: null  # The number of transformer layers on the last pipeline stage of the decoder. Default None is even split of transformer layers across all pipeline stages
decoder_num_layers: null  # Number of decoder transformer layers.
decoder_seq_length: null  # Maximum decoder sequence length to process.
decoupled_lr: null  # Separate learning rate for the input and output layer
decoupled_min_lr: null  # Minimum value for learning rate for the input and output layer. The schedulerclip values below this threshold
decrease_batch_size_if_needed: false  # If set, decrease batch size if microbatch_size * dp_sizedoes not divide batch_size. Useful for KSO (Keep Soldiering On)to continue making progress if number of healthy GPUs (andcorresponding dp_size) does not support current batch_size.Old batch_size will be restored if training is re-started withdp_size that divides batch_size // microbatch_size.
defer_embedding_wgrad_compute: false  # If set, defers the vocabulary projection linear layer weightgradient compute to pipeline flush.
deprecated_use_mcore_models: false  # DEPRECATED. Use the implementation from megatron core.Now ignored and mcore models are the default, use --use-legacy-models to not use core models.
deterministic_mode: false  # Choose code that has deterministic execution. This usually means slower execution, but is good for debugging and testing.
dino_bottleneck_size: 256  # Bottle neck dimension in dino head
dino_freeze_last_layer: 1  # Freezing last layer weights
dino_head_hidden_size: 2048  # Hidden dimension size in dino head
dino_local_crops_number: 10  # Number of local crops
dino_local_img_size: 96  # Image size for vision classification task
dino_norm_last_layer: false  # Disable Norm in last layer.
dino_teacher_temp: 0.07  # teacher temperature
dino_warmup_teacher_temp: 0.04  # warump teacher temperature
dino_warmup_teacher_temp_epochs: 30  # warmup teacher temperaure epochs
disable_straggler_on_startup: false  # If set, StragglerDetector is disabled on startup.
disable_te_fused_rope: false  # Disable fused rope from transformer-engine: use --disable_te_fused_rope
dist_ckpt_format_deprecated: null  # Deprecated: see --ckpt-format.
dist_ckpt_strictness: assume_ok_unexpected  # choices: ['assume_ok_unexpected', 'log_unexpected', 'log_all', 'raise_unexpected', 'raise_all', 'return_unexpected', 'return_all', 'ignore_all'] | Determine handling of key mismatch during checkpoint load. Check StrictHandling docs for flags meaning. NOTE: This flag controls only distributed checkpoint load from storage, not loading state dict into the model.
distribute_saved_activations: false  # If set, distribute recomputed activations across model parallel group.
distributed_backend: nccl  # choices: ['nccl', 'gloo'] | Which backend to use for distributed training.
distributed_timeout_minutes: 10  # Timeout minutes for torch.distributed.
embedding_path: null  # Where to save/load Open-Retrieval Embedding data to/from
empty_unused_memory_level: 0  # choices: [0, 1, 2] | Call torch.cuda.empty_cache() each iteration (training and eval), to reduce fragmentation.0=off, 1=moderate, 2=aggressive.
enable_ft_package: false  # If set, Fault Tolerance package is enabled. Note: This feature is for Nvidia internal use only.
enable_one_logger: true  # If set, disable using one_logger to track E2E metricsNote that one_logger is an internal tool and not available externally. For installation, please go to https://confluence.nvidia.com/display/MLWFO/Package+Repositoriesfor more details
encoder_num_layers: null  # Number of encoder transformer layers.
encoder_pipeline_model_parallel_size: 0  # Degree of pipeline model parallelism in the encoder. This is independent of the amount of pipeline in the decoder.
encoder_seq_length: null  # Maximum encoder sequence length to process.This should be exclusive of --seq-length
encoder_tensor_model_parallel_size: 0  # Degree of tensor model parallelism for the encoder.
end_weight_decay: null  # End of run weight decay coefficient for L2 regularization.
eod_mask_loss: false  # Mask loss for the end of document tokens.
error_injection_rate: 0  # Rate at which to inject unexpected results, e.g. 1000 means once every 1000 result validations
error_injection_type: transient_error  # choices: ['correct_result', 'transient_error', 'persistent_error'] | Type of error to inject.
eval_interval: 1000  # Interval between running evaluation on validation set.
eval_iters: 100  # Number of iterations to run for evaluationvalidation/test for.
evidence_data_path: null  # Path to Wikipedia Evidence frm DPR paper
exit_duration_in_mins: null  # Exit the program after this many minutes.
exit_interval: null  # Exit the program after the iteration is divisible by this value.
exit_on_missing_checkpoint: false  # If '--load' is set, but checkpoint is not found (e.g., path typo), then exit instead of random initialization.
exit_signal_handler: false  # Dynamically save the checkpoint and shutdown the training if SIGTERM is received
expert_model_parallel_size: 1  # Degree of expert model parallelism.
expert_tensor_parallel_size: null  # Degree of expert model parallelism. Default is None, which will be set to the value of --tensor-model-paralle-size.
ffn_hidden_size: null  # Transformer Feed-Forward Network hidden size. This is set to 4*hidden-size if not provided
finetune: false  # Load model for finetuning. Do not load optimizer or rng state from checkpoint and set iteration to 0. Assumed when loading a release checkpoint.
flash_decode: false  # Whether to use the flash decoding kernel.
fp16: false  # Run model in fp16 mode.
fp16_lm_cross_entropy: false  # Move the cross entropy unreduced loss calculationfor lm head to fp16.
fp32_residual_connection: false  # Move residual connections to fp32.
fp8: null  # choices: ['e4m3', 'hybrid'] | Which fp8 format scheme to use for FP8 tensors in the forward and backward pass
fp8_amax_compute_algo: most_recent  # choices: ['most_recent', 'max'] | Algorithm for computing amax from history
fp8_amax_history_len: 1  # Number of steps for which amax history is recorded per tensor
fp8_interval: 1  # DEPRECATED. This flag is ignored. Scaling update interval for fp8
fp8_margin: 0  # Scaling margin for fp8
fp8_param_gather: false  # Keep the compute param in fp8 (do not use any other intermediate dtype) and perform the param all-gather in fp8.
fp8_wgrad: true  # Execute wgrad in higher precision even for FP8 runs
fused_padded_mla_attention: false  # Pad value head dim to the size of qk head dim, and run ck fused attention.
global_batch_size: null  # Training batch size. If set, it should be a multiple of micro-batch-size times data-parallel-size. If this value is None, then use micro-batch-size * data-parallel-size as the global batch size. This choice will result in 1 for number of micro-batches.
gradient_accumulation_fusion: true  # Disable fusing gradient accumulation to weight gradient computation of linear layers
group_query_attention: false  # Use group-query attention.
head_lr_mult: 1.0  # learning rate multiplier for head during finetuning
hidden_dropout: 0.1  # Dropout probability for hidden state transformer.
hidden_size: null  # Tansformer hidden size.
hierarchical_context_parallel_sizes: null  # Degrees of the hierarchical context parallelism. Users should provide a list to specify the sizes for different levels. --hierarchical-context-parallel-sizes 2 4 indicates every two adjacent gpus forms the first level of cp groups and the cp ranks with the same odevity forms the second level of cp groups.
hybrid_attention_ratio: 0.0  # Ratio of attention layers to total layers, in the range [0.0, 1.0].
hybrid_mlp_ratio: 0.0  # Ratio of mlp layers to total layers, in the range [0.0, 1.0].
hybrid_override_pattern: null  # Force a specific hybrid layer pattern. The valueshould be a string of characters chosen fromcore.ssm.mamba_hybrid_layer_allocation.Symbols.If a value greater than 0.0 is supplied to any of the hybrid ratio arguments, then the number of each typeof layer in the override pattern must match number inthe overidden pattern
hysteresis: 2  # hysteresis for dynamic loss scaling
ict_head_size: null  # Size of block embeddings to be used in ICT and REALM (paper default: 128)
ict_load: null  # Directory containing an ICTBertModel checkpoint
img_h: 224  # Image height for vision classification task
img_w: 224  # Image height for vision classification task
indexer_batch_size: 128  # How large of batches to use when doing indexing jobs
indexer_log_interval: 1000  # After how many batches should the indexer report progress
inference_batch_times_seqlen_threshold: -1  # If (batch-size * sequence-length) is smaller than this thresholdthen batches will not be split up for pipelining.Requires setting --pipeline-model-parallel-size > 1.Setting this to -1 indicates that batch pipelining is not used.
inference_max_seq_length: 2560  # Maximum sequence length allocated for prefill during inference.
init_method_std: 0.02  # Standard deviation of the zero mean normal distribution used for weight initialization.
init_method_xavier_uniform: false  # Enable Xavier uniform parameter initialization
initial_loss_scale: 4294967296  # Initial loss-scale for dynamic loss scaling.
iter_per_epoch: 1250  # iterations per epoch
keep_fp8_weight_transpose_cache: false  # Keep the fp8 weight transpose cache in memory to avoid recomputing it  This will use more memory
kv_channels: null  # Projection weights dimension in multi-head attention. This is set to    args.hidden_size // args.num_attention_heads if not provided.
kv_lora_rank: 32  # Rank of Key and Value tensors' low rank representation.
lazy_mpu_init: null  # If set to True, initialize_megatron() skips DDP initialization and returns function to complete it instead.Also turns on --use-cpu-initialization flag. This is for external DDP manager.
load: null  # Directory containing a model checkpoint.
local_rank: 0  # local rank passed from distributed launcher.
log_batch_size_to_tensorboard: false  # If set, write batch-size to tensorboard.
log_interval: 100  # Report loss and timing interval.
log_loss_scale_to_tensorboard: true  # Disable loss-scale logging to tensorboard.
log_memory_to_tensorboard: false  # Enable memory logging to tensorboard.
log_num_zeros_in_grad: false  # If set, calculate and log the number of zeros in gradient.
log_params_norm: false  # If set, calculate and log parameters norm.
log_progress: false  # If set, log progress (in terms of number of processed tokens and number of floating-point operations) to progress.txt file in checkpoint directory.
log_straggler: false  # If set, tracks and logs straggler per GPU.
log_throughput: false  # If set, calculate and log throughput per GPU.
log_timers_to_tensorboard: false  # If set, write timers to tensorboard.
log_validation_ppl_to_tensorboard: false  # If set, write validation perplexity to tensorboard.
log_world_size_to_tensorboard: false  # Enable world size logging to tensorboard.
logging_level: null  # Set default logging level
loss_scale: null  # Static loss scaling, positive power of 2 values can improve fp16 convergence. If None, dynamicloss scaling is used.
loss_scale_window: 1000  # Window over which to raise/lower dynamic scale.
lr: null  # Initial learning rate. Depending on decay style and initial warmup, the learning rate at each iteration would be different.
lr_decay_iters: null  # number of iterations to decay learning rate over, If None defaults to `--train-iters`
lr_decay_samples: null  # number of samples to decay learning rate over, If None defaults to `--train-samples`
lr_decay_style: linear  # choices: ['constant', 'linear', 'cosine', 'inverse-square-root', 'WSD'] | Learning rate decay function.
lr_warmup_fraction: null  # fraction of lr-warmup-(iters/samples) to use for warmup (as a float)
lr_warmup_init: 0.0  # Initial value for learning rate warmup. The scheduler starts warmup from this value.
lr_warmup_iters: 0  # number of iterations to linearly warmup learning rate over.
lr_warmup_samples: 0  # number of samples to linearly warmup learning rate over.
lr_wsd_decay_iters: null  # number of iterations for the annealing phase in the wsd schedule
lr_wsd_decay_samples: null  # number of samples for the annealing phase in the wsd schedule
lr_wsd_decay_style: exponential  # choices: ['exponential', 'linear', 'cosine'] | Decay style for the annealing phase of WSD
make_vocab_size_divisible_by: 128  # Pad the vocab size to be divisible by this value.This is added for computational efficieny reasons.
manual_gc: false  # Disable the threshold-based default garbage collector and trigger the garbage collection manually. Manual garbage collection helps to align the timing of the collection across ranks which mitigates the impact of CPU-associated jitters. When the manual gc is enabled, garbage collection is performed only at the start and the end of the validation routine by default.
manual_gc_eval: true  # When using manual garbage collection, disable garbage collection at the start and the end of each evaluation run.
manual_gc_interval: 0  # Training step interval to trigger manual garbage collection. When the value is set to 0, garbage collection is not triggered between training steps.
mask_factor: 1.0  # mask size scaling parameter
mask_prob: 0.15  # Probability of replacing a token with mask.
mask_type: random  # choices: ['random', 'row'] | mask types
masked_softmax_fusion: true  # Disable fusion of query_key_value scaling, masking, and softmax.
max_position_embeddings: null  # Maximum number of position embeddings to use. This is the size of position embedding.
max_tokens_to_oom: 12000  # Maximum number of tokens during inferencetokens here is # in prompt + # to generateAllows us to throw an error before OOM crashes server
memory_snapshot_path: snapshot.pickle  # Specifies where to dump the memory history pickle.
merge_file: null  # Path to the BPE merge file.
micro_batch_size: null  # Batch size per model instance (local batch size). Global batch size is local batch size times data parallel size times number of micro batches.
microbatch_group_size_per_vp_stage: null  # Number of contiguous microbatches per virtual pipeline stage
min_loss_scale: 1.0  # Minimum loss scale for dynamic loss scaling.
min_lr: 0.0  # Minimum value for learning rate. The schedulerclip values below this threshold.
mmap_bin_files: true  # Disable mmap-ing of .bin files.
mock_data: false  # Skip data loading and validation and opt for artificial generation of mock data when an implementation is available.
model_parallel_size: null  # Old model parallel argument, do not use. Use --tensor-model-parallel-size instead.
moe_aux_loss_coeff: 0.0  # Scaling coefficient for the aux loss: a starting value of 1e-2 is recommended.
moe_expert_capacity_factor: null  # The capacity factor for each expert, None means no token will be dropped.
moe_extended_tp: false  # Deprecated. Use --expert-tensor-parallel-size instead.
moe_ffn_hidden_size: null  # The hidden size of each expert's feed-forward network (ffn). If not specified, defaults to the ffn_hidden_size.
moe_grouped_gemm: false  # When there are multiple experts per rank, launch multiple local GEMM kernels in multiple streams to improve the utilization and performance with GroupedLinear in TransformerEngine.
moe_input_jitter_eps: null  # Add noise to the input tensor by applying jitter with a specified epsilon value.
moe_layer_freq: 1  # Frequency between MoE layers and Dense layers. Accepts either: - An integer N: Represents a 1:N ratio, meaning one expert layer for every N-1 dense layers - A string containing a Python list expression that defines a custom pattern, e.g.: "([1]*3+[0]*1)*3" evaluates to [1,1,1,0,1,1,1,0,1,1,1,0] where 1 indicates an expert layer and 0 indicates a dense layer. Examples: "([0]+[1]*23)": 1 dense layer followed by 23 experts layers, "([1]*3+[0]*2)*2": Three expert layers followed by two dense layers, repeated twice.
moe_layer_recompute: false  # Enable checkpointing for moe_layer, should be used when memory is not sufficient.
moe_pad_expert_input_to_capacity: false  # Pads the input for each expert to match the expert capacity length, effective only after the --moe-expert-capacity-factor is set.
moe_per_layer_logging: false  # Enable per-layer logging for MoE, currently supports auxiliary loss and z loss.
moe_permute_fusion: false  # Fuse token rearrangement ops during token dispatching.
moe_router_bias_update_rate: 0.001  # Expert bias update rate in the aux-loss-free load balancing strategy. The expert bias is updated based on the number of assigned tokens to each expert in a global batch, where the bias is increased for the experts with less assigned tokens and decreased for the experts with more assigned tokens. The default value 1e-3 is same as that used in DeepSeekV3.
moe_router_enable_expert_bias: false  # TopK routing with dynamic expert bias in the aux-loss-free load balancing strategy. The routing decision is based on the sum of the routing scores and the expert bias. See https://arxiv.org/abs/2408.15664 for details.
moe_router_force_load_balancing: false  # Enforce token dispatch balancing to MoE routing
moe_router_group_topk: null  # Number of selected groups for group-limited routing.
moe_router_load_balancing_type: aux_loss  # choices: ['aux_loss', 'seq_aux_loss', 'sinkhorn', 'none'] | Determines the load balancing strategy for the router. "aux_loss" corresponds to the load balancing loss used in GShard and SwitchTransformer; "seq_aux_loss" corresponds to the load balancing loss used in DeepSeekV2, which computes the loss for each individual sample; "sinkhorn" corresponds to the balancing algorithm used in S-BASE, and "none" implies no load balancing. The default is "aux_loss".
moe_router_num_groups: null  # Number of groups to divide experts into for group-limited routing. When using group-limited routing: 1) Experts are divided into equal-sized groups, 2) For each token, a subset of groups are selected based on routing scores (sum of top-2 expert scores within each group), 3) From these selected groups, moe_router_topk experts are chosen.Two common use cases: 1) Device-limited routing: Set equal to expert parallel size (EP) to limit each token to experts on a subset of devices (See DeepSeek-V2: https://arxiv.org/pdf/2405.04434) 2) Node-limited routing: Set equal to number of nodes in EP group to limit each token to experts on a subset of nodes (See DeepSeek-V3: https://arxiv.org/pdf/2412.19437)
moe_router_pre_softmax: false  # Enable pre-softmax routing for MoE, which means softmax is before the top-k selection. By default, softmax is done after top-k.
moe_router_score_function: softmax  # choices: ['softmax', 'sigmoid'] | Score function for MoE TopK routing. Can be "softmax" or "sigmoid".
moe_router_topk: 2  # Number of experts to route to for each token. The default is 2.
moe_router_topk_scaling_factor: null  # Scaling factor for routing score in top-k selection, only works when --moe-router-pre-softmax enabled. Defaults to None, which means no scaling.
moe_shared_expert_intermediate_size: null  # Shared expert total ffn hidden size. It should be equal to "num_shared_experts * ffn_size_of_each_shared_expert" if there are multiple shared experts. None means no shared expert.
moe_shared_expert_overlap: false  # Enable overlapping between shared expert computations and dispatcher communications. Without this, the shared epxerts execute after the routed experts. Only effective when moe-shared-expert-intermediate-size is set.
moe_token_dispatcher_type: allgather  # choices: ['allgather', 'alltoall', 'alltoall_seq'] | The type of token dispatcher to use. The default is 'allgather'. Options are 'allgather', 'alltoall' and 'alltoall_seq'. We recommend using 'alltoall' when applying expert parallelism. For more information, please refer to the documentation in core/moe/README.
moe_token_drop_policy: probs  # choices: ['probs', 'position'] | The policy to drop tokens. Can be either "probs" or "position". If "probs", the tokens with the lowest probabilities will be dropped. If "position", tokens at the end of each batch will be dropped.
moe_use_legacy_grouped_gemm: false  # Use legacy GroupedMLP rather than TEGroupedMLP. Note: The legacy one will be deprecated soon.
moe_use_upcycling: false  # Load a checkpoint of a dense model, convert it into an MoE model, and save the converted model to the path specified by --save. Upcycling is implemented on the top of distributed checkpointing, so it supports parallel modes different from the dense model.
moe_z_loss_coeff: null  # Scaling coefficient for the z-loss: a starting value of 1e-3 is recommended.
multi_latent_attention: false  # Use multi-latent attention for model.
nccl_communicator_config_path: null  # Path to the yaml file with NCCL communicator configurations. The number of min/max thread groups and thread group cluster size of each communicator can be configured by setting `min_ctas`, `max_ctas`, and `cga_cluster_size`.
no_load_optim: null  # Do not load optimizer when loading checkpoint.
no_load_rng: null  # Do not load rng state when loading checkpoint.
no_persist_layer_norm: false  # Disable using persistent fused layer norm kernel. This kernel supports only a set of hidden sizes. Please check persist_ln_hidden_sizes if your hidden size is supported.
no_save_optim: null  # Do not save current optimizer.
no_save_rng: null  # Do not save current rng state.
non_persistent_ckpt_type: null  # choices: ['global', 'local', 'in_memory', None] | Type of non-persistent model checkpoints. "global" - Saved as a standard checkpoint (e.g., on Lustre) with old checkpoints being removed. "local" - [TBD] Each rank saves a portion of the checkpoint locally (e.g., on SSD/ramdisk). "in_memory" - [TBD] A special kind of local checkpoint that avoids serialization. None - No non-persistent checkpointing (default option).
non_persistent_global_ckpt_dir: null  # Directory containing global non-persistent model checkpoints.
non_persistent_local_ckpt_algo: fully_parallel  # choices: ['fully_parallel', 'atomic'] | Algorithm for local non-persistent checkpointing.
non_persistent_local_ckpt_dir: null  # Directory containing local non-persistent model checkpoints.
non_persistent_save_interval: null  # Number of iterations between non-persistent saves.
norm_epsilon: 1.0e-05  # Epsilon for layer norm and RMS norm.
normalization: LayerNorm  # choices: ['LayerNorm', 'RMSNorm'] | Which normalization technique to use.
num_attention_heads: null  # Number of transformer attention heads.
num_channels: 3  # Number of channels in input image data
num_classes: 1000  # num of classes in vision classificaiton task
num_dataset_builder_threads: 1  # Number of parallel threads per rank for dataset builder
num_distributed_optimizer_instances: 1  # Number of Distributed Optimizer copies across Data Parallel domain.
num_experts: null  # Number of Experts in MoE (None means no MoE)
num_layers: null  # Number of transformer layers.
num_layers_per_virtual_pipeline_stage: null  # Number of layers per virtual pipeline stage
num_query_groups: 1
num_workers: 2  # Dataloader number of workers.
one_logger_async: false  # If set, forces one_logger to use async mode.
one_logger_project: megatron-lm  # The one-logger project name. Will ignore if --no-one-logger is set
one_logger_run_name: null  # The one-logger run name displayed. Will ignore if --no-one-logger is set
onnx_safe: null  # Use workarounds for known problems with Torch ONNX exporter
openai_gelu: false  # Use OpenAIs GeLU implementation. This optionshould not be used unless for backward compatibilityreasons.
optimizer: adam  # choices: ['adam', 'sgd'] | Optimizer function
output_bert_embeddings: false  # Output Bert embeddings (via mean pooling) from model, rather than its binary head output or entire hidden batch.
overlap_grad_reduce: false  # If set, overlap DDP grad reduce.
overlap_p2p_comm: true  # overlap pipeline parallel communication with forward and backward chunks in 1F1B
overlap_p2p_comm_warmup_flush: false  # if set, overlap pipeline parallel communication in warmup and flush
overlap_param_gather: false  # If set, overlap param all-gather in distributed optimizer.
overlap_param_gather_with_optimizer_step: false  # If set, overlap param all-gather of first bucket with optimizer step.
override_opt_param_scheduler: false  # Reset the values of the scheduler (learning rate,warmup iterations, minimum learning rate, maximum number of iterations, and decay style from input arguments and ignore values from checkpoints. Notethat all the above values will be reset.
patch_dim: 16  # patch dimension
per_split_data_args_path: null  # Path to per-split-data-args. Instead of feeding `--(train|valid|test)-data-path` with weighted dataset, we pass in a file path from which we read those arguments. This is useful when the list of data is too big. Format is a json file with `train`, `valid, `test` keys
perform_initialization: true  # Do not perform initialization when building model, can reduce startup time when definitely loading from a checkpoint
pipeline_model_parallel_size: 1  # Degree of pipeline model parallelism.
pipeline_model_parallel_split_rank: null  # Rank where encoder and decoder should be split. Deprecated; use --encoder-pipeline-model-parallel-size instead.
position_embedding_type: learned_absolute  # choices: ['learned_absolute', 'rope', 'none'] | Position embedding type.
pretrained_checkpoint: null  # Directory containing a pretrained model checkpoint for finetuning.
profile: false  # Enable nsys profiling. When using this option, nsys options should be specified in commandline. An example nsys commandline is `nsys profile -s none -t nvtx,cuda -o <path/to/output_file> --force-overwrite true --capture-range=cudaProfilerApi --capture-range-end=stop`.
profile_ranks:  # Global ranks to profile.
- 0
profile_step_end: 12  # Global step to stop profiling.
profile_step_start: 10  # Global step to start profiling.
q_lora_rank: null  # Rank of Query tensor's low rank representation.
qk_head_dim: 128  # Dimension of the head in the QK projection. q_head_dim = qk_head_dim + qk_pos_emb_head_dim
qk_layernorm: false  # Whether to layer normalize the q and k attention embeddings.
qk_pos_emb_head_dim: 64  # Dimension of the position embedding in the QK projection.
query_in_block_prob: 0.1  # Probability of keeping query in block for ICT dataset
rampup_batch_size: null  # Batch size ramp up with the following values:  --rampup-batch-size <start batch size>                       <batch size incerement>                       <ramp-up samples> For example:   --rampup-batch-size 16 8 300000 \    --global-batch-size 1024will start with global batch size 16 and over  (1024 - 16) / 8 = 126 intervals will increasethe batch size linearly to 1024. In each intervalwe will use approximately 300000 / 126 = 2380 samples.
recompute_activations: false  # recompute activation to allow for training with larger models, sequences, and batch sizes.
recompute_granularity: null  # choices: ['full', 'selective'] | Checkpoint activations to allow for training with larger models, sequences, and batch sizes. It is supported at two granularities 1) full: whole transformer layer is recomputed, 2) selective: core attention part of the transformer layer is recomputed.
recompute_method: null  # choices: ['uniform', 'block'] | 1) uniform: uniformly divide the total number of Transformer layers and recompute the input activation of each divided chunk at specified granularity, 2) recompute the input activations of only a set number of individual Transformer layers per pipeline stage and do the rest without any recomputing at specified granularitydefault) do not apply activations recompute to any layers
recompute_num_layers: null  # 1) uniform: the number of Transformer layers in each uniformly divided recompute unit, 2) block: the number of individual Transformer layers to recompute within each pipeline stage.
record_memory_history: false  # Record memory history in last rank.
renormalize_blend_weights: false  # Renormalize the blend weights to account for the mid-level dataset oversampling done to ensure fulfillment of the requested number of samples. Use this option if prompted. Defaults to False for backward comparability in the data sample order.
rerun_mode: disabled  # choices: ['disabled', 'validate_results', 'report_stats'] | Use re-run engine to validate results (default) or to emit stats on variability of computations due to non-deterministic algorithms.
reset_attention_mask: false  # Reset self attention maske after end-of-document token.
reset_position_ids: false  # Reset posistion ids after end-of-document token.
retriever_report_topk_accuracies: []  # Which top-k accuracies to report (e.g. '1 5 20')
retriever_score_scaling: false  # Whether to scale retriever scores by inverse square root of hidden size
retriever_seq_length: 256  # Maximum sequence length for the biencoder model for retriever
retro_add_retriever: false  # Add a retriever to the transformer, for use in pretraining a Retro model.
retro_attention_gate: 1  # Gated cross attention.
retro_cyclic_train_iters: null  # Set number of training iterations for cyclic Retro training.
retro_encoder_attention_dropout: 0.1  # Attention dropout for retrieval encoder.
retro_encoder_hidden_dropout: 0.1  # Hidden dropout for retrieval encoder.
retro_encoder_layers: 2  # Number of layers to use for the retrieval encoder.
retro_num_neighbors: 2  # Number of neighbors to retrieve during pretraining.
retro_num_retrieved_chunks: 2  # Number of chunks to retrieve from the retrieval database.
retro_project_dir: null  # Retro project directory, which contains the preprocessed data for pretraining. This directory is built during preprocessing (see tools/retro/README.md), and contains subdirectories for the chunk database and pretraining neighbors.
retro_verify_neighbor_count: true  # Skip verifying that len(GPT dataset) == len(saved neighbors).
rotary_base: 10000  # Base to use for rotary positional embeddings, default 10000
rotary_interleaved: false  # Use interleaved rotary embedding.
rotary_percent: 1.0  # Percent of rotary dimension to use, default 100%%
rotary_scaling_factor: 1.0  # Rotary scaling factor for the rotary embeddings.
rotary_seq_len_interpolation_factor: null  # Sequence length interpolation factor for rotary embeddings.
s3_cache_path: null  # Path to cache index files when using s3 dataloader
sample_rate: 1.0  # sample rate for training data. Supposed to be 0  < sample_rate < 1
save: null  # Output directory to save checkpoints to.
save_interval: null  # Number of iterations between persistent checkpoint saves.
scatter_gather_tensors_in_pipeline: true  # If not set, use scatter/gather to optimize communication of tensors in pipeline.
seed: 1234  # Random seed used for python, numpy, pytorch, and cuda.
seq_length: null  # Maximum sequence length to process.
sequence_parallel: false  # Enable sequence parallel optimization.
sgd_momentum: 0.9  # Momentum factor for sgd
short_seq_prob: 0.1  # Probability of producing a short sequence.
skip_train: false  # If set, bypass the training loop, optionally do evaluation for validation/test, and exit.
spec: null  # Specify the <module_location function_name> pair that returns a spec to customize a model, transformer block, or transformer layer, depending on the use case.To use local spec specify local as the argument.For more details, see the model class, `transformer_block.py`, or `transformer_layer.py`
split: null  # Comma-separated list of proportions for training, validation, and test split. For example the split `90,5,5` will use 90%% of data for training, 5%% for validation and 5%% for test.
squared_relu: false  # Use squared relu activation instead of default gelu
standalone_embedding_stage: false  # If set, *input* embedding layer is placed on its own pipeline stage, without any transformer layers. (For T5, this flag currently only affects the encoder embedding.)
start_weight_decay: null  # Initial weight decay coefficient for L2 regularization.
straggler_ctrlr_port: 65535  # Port number to toggle StragglerDetector on/off at runtime
straggler_minmax_count: 1  # Number of ranks to report with high/low estimated throughput
swiglu: false  # Use gated linear units and SiLU activation instead of default gelu
swin_backbone_type: tiny  # choices: ['tiny', 'base', 'h3'] | pretraining objectives
tensor_model_parallel_size: 1  # Degree of tensor model parallelism.
tensorboard_dir: null  # Write TensorBoard logs to this directory.
tensorboard_log_interval: 1  # Report to tensorboard interval.
tensorboard_queue_size: 1000  # Size of the tensorboard queue for pending events and summaries before one of the ‘add’ calls forces a flush to disk.
test_data_path: null  # The weight and prefix list for an independent test dataset. Follows the same pattern rules as --data-path.
test_mode: false  # Run all real-time test alongside the experiment.
tiktoken_num_special_tokens: 1000  # Number of special tokens in tiktoken tokenizer
tiktoken_pattern: null  # Which tiktoken pattern to use. Options: [v1, v2]
tiktoken_special_tokens: null  # List of tiktoken special tokens, needs to have ["<unk>", "<s>", "</s>"]
timing_log_level: 0  # choices: [0, 1, 2] | Granularity level to measure and report timing.    0: report only iteration time and make sure timing       does not introduce extra overhead.   1: report timing for operations that are executed       very limited times (basically once) during       each iteration (such as gradient all-reduce)    2: report timing for operations that migh be       executed numerous times during each iteration. Note that setting the level to 1 or 2 might cause increase in iteration time.
timing_log_option: minmax  # choices: ['max', 'minmax', 'all'] | Options for logging timing:  max: report the max timing across all ranks  minmax: report min and max timings across all ranks  all: report timings of all ranks.
titles_data_path: null  # Path to titles dataset used for ICT
tokenizer_model: null  # Sentencepiece tokenizer model.
tokenizer_type: null  # choices: ['BertWordPieceLowerCase', 'BertWordPieceCase', 'GPT2BPETokenizer', 'SentencePieceTokenizer', 'GPTSentencePieceTokenizer', 'HuggingFaceTokenizer', 'Llama2Tokenizer', 'TikTokenizer', 'MultimodalTokenizer', 'NullTokenizer', 'DeepSeekV2Tokenizer', 'DeepSeekV3Tokenizer'] | What type of tokenizer to use.
tp_comm_bootstrap_backend: nccl  # choices: ['nccl', 'mpi', 'gloo'] | Set the bootstrapping backend of Tensor parallel communications.
tp_comm_bulk_dgrad: true  # Disables the All-Gather overlap with bprop activation gradient GEMM.
tp_comm_bulk_wgrad: true  # Disables the Reduce-Scatter overlap with bprop weight gradient GEMM.
tp_comm_overlap: false  # Enables the  overlap of Tensor parallel communication and GEMM kernels.
tp_comm_overlap_ag: true  # Disables the All-Gather overlap with GEMM by pipelining the GEMM and All-Gather.
tp_comm_overlap_cfg: null  # Config file when tp_comm_overlap is enabled.
tp_comm_overlap_rs: true  # Disables the Reduce-Scatter overlap with GEMM by pipelining the GEMM and Reduce-Scatter.
tp_comm_overlap_rs_dgrad: false  # Enables the Reduce-Scatter overlap with dgrad GEMM.
tp_comm_split_ag: true  # Disables the All-Gather overlap with fprop GEMM.
tp_comm_split_rs: true  # Disables the Reduce-Scatter overlap with fprop GEMM.
train_data_path: null  # The weight and prefix list for an independent train dataset. Follows the same pattern rules as --data-path.
train_iters: null  # Total number of iterations to train over all training runs. Note that either train-iters or train-samples should be provided.
train_samples: null  # Total number of samples to train over all training runs. Note that either train-iters or train-samples should be provided.
train_sync_interval: null  # Training CPU-GPU synchronization interval, to ensure that CPU is not running too far ahead of GPU.
transformer_impl: transformer_engine  # choices: ['local', 'transformer_engine'] | Which Transformer implementation to use.
untie_embeddings_and_output_weights: false  # Untie embeddings and output weights.
use_checkpoint_args: false  # Override model-related command-line arguments with arguments from checkpoint
use_checkpoint_opt_param_scheduler: false  # Use checkpoint to set the values of the scheduler (learning rate, warmup iterations, minimum learning rate, maximum number of iterations, and decay style from checkpoint and ignore input arguments.
use_cpu_initialization: null  # If set, initialize weights on the CPU. This eliminates init differences based on tensor parallelism.
use_dist_ckpt_deprecated: false  # Deprecated: see --ckpt-format.
use_distributed_optimizer: false  # Use distributed optimizer.
use_flash_attn: false  # use FlashAttention implementation of attention. https://arxiv.org/abs/2205.14135
use_legacy_models: false  # Use the legacy Megatron models, not Megatron-Core models.
use_mp_args_from_checkpoint_args: false  # Copy model parallelism command-line arguments from checkpoint
use_one_sent_docs: false  # Whether to use one sentence documents in ICT
use_pytorch_profiler: false  # Use the built-in pytorch profiler. Useful if you wish to view profiles in tensorboard.
use_ring_exchange_p2p: false  # If set, use custom-built ring exchange for p2p communications. Note that this option will require a custom built image that support ring-exchange p2p.
use_rope_scaling: false  # Apply rope scaling as used in llama3.1
use_rotary_position_embeddings: false  # Use rotary positional embeddings or not. Deprecated: use --position-embedding-type
use_tokenizer_model_from_checkpoint_args: true  # If set, do not use tokenizer model path from checkpoint
use_torch_fsdp2: false  # Use the torch FSDP2 implementation. FSDP2 is not currently working with Pipeline Parallel.It is still not in a stable release stage, and may therefore contain bugs or other potential issues.
use_tp_pp_dp_mapping: false  # If set, distributed ranks initialize order is changed from tp-dp-pp to tp-pp-dp. Make sure EP and CP aren't used with this option enabled
use_z_loss: false  # use z-loss for supplement loss (Google PaLM method)
v_head_dim: 128  # Dimension of the head in the V projection.
valid_data_path: null  # The weight and prefix list for an independent validation dataset. Follows the same pattern rules as --data-path.
vision_backbone_type: vit  # choices: ['vit', 'mit', 'swin'] | backbone types types
vision_pretraining: false  # flag to indicate vision pretraining
vision_pretraining_type: classify  # choices: ['classify', 'inpaint', 'dino'] | pretraining objectives
vocab_extra_ids: 0  # Number of additional vocabulary tokens. They are used for span masking in the T5 model
vocab_file: null  # Path to the vocab file.
vocab_size: null  # Size of vocab before EOD or padding.
wandb_exp_name: ''  # The wandb experiment name.
wandb_project: ''  # The wandb project name. Ignore wandb by default.
wandb_save_dir: ''  # Path to save the wandb results locally.
warmup: null  # Old lr warmup argument, do not use. Use one of the--lr-warmup-* arguments above
weight_decay: 0.01  # Weight decay coefficient for L2 regularization.
weight_decay_incr_style: constant  # choices: ['constant', 'linear', 'cosine'] | Weight decay increment function.
wgrad_deferral_limit: 0  # Number of micro-batches for whichweight gradient computation of vocabulary projection is deferred, defaults to 0 whichmeans all the micro-batches are deferred. Invalid if `defer-embedding-wgrad-compute`is not set
window_size: null  # sliding window size
yaml_cfg: null  # Config file to add additional arguments
