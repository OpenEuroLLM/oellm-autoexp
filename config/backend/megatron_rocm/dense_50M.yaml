defaults:
  - base
  - _self_

# DATA
seq_length: 4096
max_position_embeddings: 4096
num_workers: 7
num_dataset_builder_threads: 6
split: '99,1,0'
data_path:
  - ${oc.env:DATA_DIR}/gpt-neox-20b/nemotron-cc/1.0/high-actual
  - ${oc.env:DATA_DIR}/gpt-neox-20b/nemotron-cc/1.0/high-synthetic-distill
  - ${oc.env:DATA_DIR}/gpt-neox-20b/nemotron-cc/1.0/high-synthetic-diverse_qa_pairs
  - ${oc.env:DATA_DIR}/gpt-neox-20b/nemotron-cc/1.0/high-synthetic-extract_knowledge
  - ${oc.env:DATA_DIR}/gpt-neox-20b/nemotron-cc/1.0/high-synthetic-knowledge_list
  - ${oc.env:DATA_DIR}/gpt-neox-20b/nemotron-cc/1.0/high-synthetic-wrap_medium
data_cache_path: /flash/project_462000963/cache/

# Tokenizer configuration
tokenizer_model: EleutherAI/gpt-neox-20b
tokenizer_type: HuggingFaceTokenizer

micro_batch_size: 16
global_batch_size: 512

# MODEL
num_layers: 12
hidden_size: 384
ffn_hidden_size: 1536
num_attention_heads: 6
init_method_std: 0.02
position_embedding_type: "rope"
rotary_base: 10000
rotary_percent: 1.0
num_query_groups: 6
attention_dropout: 0.0
hidden_dropout: 0.0
normalization: RMSNorm
norm_epsilon: 1.e-5 # rmsnorm epsilon
qk_layernorm: True # potentially enable qk-norm
bf16: True
swiglu: True
untie_embeddings_and_output_weights: False
use_flash_attn: True
attention_softmax_in_fp32: False # OpenSci style
te_fallback_layernorm_linear: True

# TRAINING, PARALLELISM
group_query_attention: False
recompute_activations: False
use_torch_fsdp2: False # it's either this or the next 4 args
tensor_model_parallel_size: 1
pipeline_model_parallel_size: 1
context_parallel_size: 1
sequence_parallel: False

train_iters: 10
lr_wsd_decay_iters: 10
save_interval: 1000000000
save: ${project.base_output_dir}
load: ${project.base_output_dir}
tensorboard_dir: ${project.base_output_dir}/tensorboard
add_bias_linear: true

# SCHEDULER
min_lr: 0.0
lr_decay_style: WSD
lr_wsd_decay_style: linear
lr_warmup_iters: 1000 # For now we fix it to 1k

#OPTIMIZER
lr: 0.008
optimizer: adam
adam_beta1: 0.9
adam_beta2: 0.99
adam_eps: 1.e-8
clip_grad: 1.0
weight_decay: 0.1

#FUSIONS
bias_dropout_fusion: false  # Disable bias and dropout fusion.
bias_swiglu_fusion: false  # Disable bias and swiglu fusion, the fusion is available only when using megatron-core.
gradient_accumulation_fusion: false  # Disable fusing gradient accumulation to weight gradient computation of linear layers
