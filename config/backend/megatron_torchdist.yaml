defaults:
  - megatron: base
  - _self_

class_name: MegatronBackend

python_cmd: ${container.python}
full_schema_validation: false
launcher_script: "./submodules/Megatron-LM/pretrain_gpt.py"
dist_cmd: "${.python_cmd} ${oc.join:'',${oc.mapcondtmpl:'\\-\\-.*','\\\n  %',' %',\
 ${oc.concat:['-u', '-m', 'torch.distributed.run'],${oc.cliargs:${.torchrun_args}}}}}\\\n   ${.launcher_script}"
torchrun_args:
  nproc_per_node: ${slurm.sbatch.gpus_per_node}
  nnodes: ${slurm.sbatch.nodes}
  max_restarts: 0
  tee: 3
  rdzv_endpoint: "$MASTER_ADDR:$MASTER_PORT"
  rdzv_backend: "static"
  node_rank: "$SLURM_NODEID"
  master_addr: "$MASTER_ADDR"
  master_port: "$MASTER_PORT"
env:
  PYTHONPATH: ".:submodules/Megatron-LM"
  RUN_DIR: ${.PROJECT_DIR}/submodules/Megatron-LM
  PROJECT_DIR: ${oc.env:PROJECT_DIR,.}

# map megatron command to
full_cmd: "${.dist_cmd} ${oc.join:'',${oc.mapcondtmpl:'\\-\\-.*','\\\n   %',' %',${oc.argsmegatron:${backend.megatron}}}}"
