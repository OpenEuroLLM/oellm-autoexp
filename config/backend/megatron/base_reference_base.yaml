defaults:
  - base


accumulate_allreduce_grads_in_fp32: true
adam_beta2: 0.99
adam_eps: 1e-08
add_qkv_bias: true
align_param_gather: false
attention_backend: auto
attention_dropout: 0.0
bf16: true
bias_dropout_fusion: false
bias_gelu_fusion: false
bias_swiglu_fusion: false
ckpt_format: torch_dist
dataloader_type: single
gradient_accumulation_fusion: false
hidden_dropout: 0.0
legacy_tokenizer: true
log_interval: 100
log_progress: true
log_throughput: true
lr_decay_style: WSD
norm_epsilon: 1e-05
normalization: RMSNorm
num_dataset_builder_threads: 6
num_workers: 7
overlap_p2p_comm: false
override_opt_param_scheduler: true
position_embedding_type: rope
qk_layernorm: true
rerun_mode: disabled
split: 99,1,0
swiglu: true
tokenizer_model: EleutherAI/gpt-neox-20b
tokenizer_type: HuggingFaceTokenizer
use_distributed_optimizer: true
use_flash_attn: true
weight_decay: 0.1
