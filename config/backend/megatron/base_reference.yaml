defaults:
  - base


accumulate_allreduce_grads_in_fp32: true
adam_beta2: 0.99
adam_eps: 1e-08
add_qkv_bias: true
align_param_gather: false
attention_backend: AttnBackend.auto
attention_dropout: 0.0
bf16: true
bias_dropout_fusion: false
bias_gelu_fusion: false
bias_swiglu_fusion: false
ckpt_format: torch
dataloader_type: single
encoder_num_layers: 12
encoder_seq_length: 4096
end_weight_decay: 0.1
eval_interval: 800
expert_tensor_parallel_size: 1
ffn_hidden_size: 1536
global_batch_size: 256
gradient_accumulation_fusion: false
hidden_dropout: 0.0
hidden_size: 384
kv_channels: 64
log_interval: 10
log_progress: true
log_throughput: true
lr: 0.001
lr_decay_style: WSD
lr_warmup_iters: 2000
lr_wsd_decay_iters: 9536
lr_wsd_decay_style: linear
max_position_embeddings: 4096
micro_batch_size: 16
min_lr: 1e-05
moe_ffn_hidden_size: 1536
norm_epsilon: 1e-05
normalization: RMSNorm
num_attention_heads: 6
num_dataset_builder_threads: 6
num_layers: 12
num_query_groups: 6
num_workers: 7
overlap_p2p_comm: false
override_opt_param_scheduler: true
position_embedding_type: rope
qk_layernorm: true
rerun_mode: disabled
save_interval: 2000
seq_length: 4096
split: 99,1,0
start_weight_decay: 0.1
swiglu: true
tokenizer_model: EleutherAI/gpt-neox-20b
tokenizer_type: HuggingFaceTokenizer
train_iters: 47684
use_distributed_optimizer: true
use_flash_attn: true
weight_decay: 0.1
