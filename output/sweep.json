{
  "project": "megatron_auto_restart_example",
  "jobs": [
    {
      "name": "megatron_auto_restart_example",
      "output_dir": "output/megatron_auto_restart_example",
      "log_path": "logs/megatron_auto_restart_example-%j.out",
      "parameters": {},
      "launch": {
        "argv": [
          "python",
          "-u",
          "-m",
          "torch.distributed.run",
          "--nproc-per-node",
          "4",
          "--nnodes",
          "1",
          "--max-restarts",
          "0",
          "--tee",
          "3",
          "--rdzv-endpoint",
          "$MASTER_ADDR:$MASTER_PORT",
          "--rdzv-backend",
          "static",
          "--node-rank",
          "$SLURM_NODEID",
          "--local-addr",
          "$LOCAL_ADDR",
          "--master-addr",
          "$MASTER_ADDR",
          "--master-port",
          "$MASTER_PORT",
          "submodules/Megatron-LM/pretrain_gpt.py",
          "--position-embedding-type",
          "rope",
          "--rotary-base",
          "500000",
          "--normalization",
          "RMSNorm",
          "--norm-epsilon",
          "1e-06",
          "--swiglu",
          "--untie-embeddings-and-output-weights",
          "--attention-dropout",
          "0.0",
          "--hidden-dropout",
          "0.0",
          "--weight-decay",
          "0.05",
          "--adam-beta2",
          "0.95",
          "--recompute-activations",
          "--use-flash-attn",
          "--sequence-parallel",
          "--lr-decay-style",
          "cosine",
          "--min-lr",
          "0.0001",
          "--bf16",
          "--overlap-grad-reduce",
          "--overlap-param-gather",
          "--use-distributed-optimizer",
          "--num-workers",
          "4",
          "--data-cache-path",
          "/p/project1/projectnucleus/shared/data/megatron_cache",
          "--data-path",
          "/p/project1/projectnucleus/shared/data/cerebras-SlimPajama-627B/train/merged",
          "--hidden-size",
          "2048",
          "--load",
          "./output",
          "--lr",
          "0.001",
          "--max-position-embeddings",
          "131072",
          "--micro-batch-size",
          "8",
          "--num-attention-heads",
          "16",
          "--num-layers",
          "32",
          "--recompute-granularity",
          "full",
          "--save",
          "./output",
          "--save-interval",
          "1000000000",
          "--seq-length",
          "4096",
          "--split",
          "989,10,1",
          "--tensorboard-dir",
          "./output/tensorboard",
          "--tokenizer-model",
          "EleutherAI/gpt-neox-20b",
          "--tokenizer-type",
          "HuggingFaceTokenizer",
          "--train-iters",
          "500",
          "--vocab-size",
          "50304"
        ],
        "env": {
          "PYTHONPATH": ".:submodules/Megatron-LM",
          "PROJECT_DIR": ".",
          "RUN_DIR": "./submodules/Megatron-LM",
          "SLURM_NODEID": "$SLURM_NODEID",
          "LOCAL_ADDR": "$LOCAL_ADDR",
          "MASTER_ADDR": "$MASTER_ADDR",
          "MASTER_PORT": "$MASTER_PORT"
        }
      }
    }
  ]
}
